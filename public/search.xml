<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[XV6源代码阅读-虚拟内存管理]]></title>
    <url>%2F2019%2F06%2F09%2Fxv6-virtual-memory%2F</url>
    <content type="text"><![CDATA[Exercise1 源代码阅读 内存管理部分: kalloc.c vm.c 以及相关其他文件代码 kalloc.c：char kalloc(void)负责在需要的时候为用户空间、内核栈、页表页以及缓冲区分配物理内存，将物理地址转为虚拟地址返回，物理页大小为4k。void kfree(char v)接收一个虚拟地址，找对对应的物理地址进行释放。xv6使用空闲内存的前部分作为指针域来指向下一页空闲内存，物理内存管理是以页（4K）为单位进行分配的。物理内存空间上空闲的每一页，都有一个指针域（虚拟地址）指向下一个空闲页，最后一个空闲页为NULL ，通过这种方式，kmem只需要保存着虚拟地址空间上的freelist地址即可； 1234567891011121314151617// kalloc.c// Physical memory allocator, intended to allocate// memory for user processes, kernel stacks, page table pages,// and pipe buffers. Allocates 4096-byte pages.void freerange(void *vstart, void *vend);extern char end[]; // first address after kernel loaded from ELF filestruct run &#123; struct run *next;&#125;;struct &#123; struct spinlock lock; int use_lock; struct run *freelist;&#125; kmem; xv6让每个进程都有独立的页表结构，在切换进程时总是需要切换页表，switchkvm设置cr3寄存器的值为kpgdir首地址，kpgdir仅仅在scheduler内核线程中使用。页表和内核栈都是每个进程独有的，xv6使用结构体proc将它们统一起来，在进程切换的时候，他们也往往随着进程切换而切换，内核中模拟出了一个内核线程，它独占内核栈和内核页表kpgdir，它是所有进程调度的基础。switchuvm通过传入的proc结构负责切换相关的进程独有的数据结构，其中包括TSS相关的操作，然后将进程特有的页表载入cr3寄存器，完成设置进程相关的虚拟地址空间环境； 123456789101112131415161718192021222324252627// vm.c……// Switch h/w page table register to the kernel-only page table,// for when no process is running.voidswitchkvm(void)&#123; lcr3(v2p(kpgdir)); // switch to the kernel page table&#125;// Switch TSS and h/w page table to correspond to process p.voidswitchuvm(struct proc *p)&#123; pushcli(); cpu-&gt;gdt[SEG_TSS] = SEG16(STS_T32A, &amp;cpu-&gt;ts, sizeof(cpu-&gt;ts)-1, 0); cpu-&gt;gdt[SEG_TSS].s = 0; cpu-&gt;ts.ss0 = SEG_KDATA &lt;&lt; 3; cpu-&gt;ts.esp0 = (uint)proc-&gt;kstack + KSTACKSIZE; ltr(SEG_TSS &lt;&lt; 3); if(p-&gt;pgdir == 0) panic("switchuvm: no pgdir"); lcr3(v2p(p-&gt;pgdir)); // switch to new address space popcli();&#125; 进程的页表在使用前往往需要初始化，其中必须包含内核代码的映射，这样进程在进入内核时便不需要再次切换页表，进程使用虚拟地址空间的低地址部分，高地址部分留给内核，主要接口： pde_t * setupkvm(void)通过kalloc分配一页内存作为页目录，然后将按照kmap数据结构映射内核虚拟地址空间到物理地址空间，期间调用了工具函数mappages； int allocuvm(pde_t * pgdir, uint oldsz, uint newsz)在设置页表的同时分配虚拟地址oldsz到newsz的以页为单位的内存； int deallocuvm(pde_t * pgdir, uint oldsz, uint newsz)则将newsz到oldsz对应的虚拟地址空间内存置为空闲； int loaduvm(pde_t pgdir, char addr, struct inode * ip, uint offset, uint sz)将文件系统上的i节点内容读取载入到相应的地址上，通过allocuvm接口为用户进程分配内存和设置页表，然后调用loaduvm接口将文件系统上的程序载入到内存，便能够为exec系统调用提供接口，为用户进程的正式运行做准备； 当进程销毁需要回收内存时，调用void freevm(pde_t * pgdir)清除用户进程相关的内存环境，其首先调用将0到KERNBASE的虚拟地址空间回收，然后销毁整个进程的页表； pde_t copyuvm(pde_t pgdir, uint sz)负责复制一个新的页表并分配新的内存，新的内存布局和旧的完全一样，xv6使用这个函数作为fork()底层实现。 Exercise2 带着问题阅读 XV6初始化之后到执行main.c时，内存布局是怎样的(其中已有哪些内容)? 内核代码存在于物理地址低地址的0x100000处，页表为main.c文件中的entrypgdir数组，其中虚拟地址低4M映射物理地址低4M，虚拟地址 [KERNBASE, KERNBASE+4MB) 映射到 物理地址[0, 4MB)； 紧接着调用kinit1初始化内核末尾到物理内存4M的物理内存空间为未使用，然后调用kinit2初始化剩余内核空间到PHYSTOP为未使用。kinit1调用前使用的还是最初的页表（也就是是上面的内存布局），所以只能初始化4M，同时由于后期再构建新页表时也要使用页表转换机制来找到实际存放页表的物理内存空间，这就构成了自举问题，xv6通过在main函数最开始处释放内核末尾到4Mb的空间来分配页表，由于在最开始时多核CPU还未启动，所以没有设置锁机制。kinit2在内核构建了新页表后，能够完全访问内核的虚拟地址空间，所以在这里初始化所有物理内存，并开始了锁机制保护空闲内存链表； 然后main函数通过调用void kvmalloc(void)函数来实现内核新页表的初始化； 最后内存布局和地址空间如下：内核末尾物理地址到物理地址PHYSTOP的内存空间未使用，虚拟地址空间KERNBASE以上部分映射到物理内存低地址相应位置。 12345678910111213141516171819202122232425262728293031323334353637// kalloc.c// Initialization happens in two phases.// 1. main() calls kinit1() while still using entrypgdir to place just// the pages mapped by entrypgdir on free list.// 2. main() calls kinit2() with the rest of the physical pages// after installing a full page table that maps them on all cores.voidkinit1(void *vstart, void *vend)&#123; initlock(&amp;kmem.lock, "kmem"); kmem.use_lock = 0; freerange(vstart, vend);&#125;voidkinit2(void *vstart, void *vend)&#123; freerange(vstart, vend); kmem.use_lock = 1;&#125;// kmap.c……// This table defines the kernel's mappings, which are present in// every process's page table.static struct kmap &#123; void *virt; uint phys_start; uint phys_end; int perm;&#125; kmap[] = &#123; &#123; (void*)KERNBASE, 0, EXTMEM, PTE_W&#125;, // I/O space &#123; (void*)KERNLINK, V2P(KERNLINK), V2P(data), 0&#125;, // kern text+rodata &#123; (void*)data, V2P(data), PHYSTOP, PTE_W&#125;, // kern data+memory &#123; (void*)DEVSPACE, DEVSPACE, 0, PTE_W&#125;, // more devices&#125;;…… XV6 的动态内存管理是如何完成的? 有一个kmem(链表)，用于管理可分配的物理内存页。(vend=0x00400000，也就是可分配的内存页最大为4Mb)详见“Exercise 1 源代码阅读”部分，已经作出完整解答。 XV6的虚拟内存是如何初始化的? 画出XV6的虚拟内存布局图，请说出每一部分对应的内容是什么。见memlayout.h和vm.c的kmap上的注释? main函数通过调用void kinit1(void vstart, void vend), void kinit2(void vstart, void vend), void kvmalloc(void)函数来实现内核新页表的初始化。虚拟地址与物理地址的转换接口： 1234567891011121314151617181920212223// memlayout.h// Memory layout#define EXTMEM 0x100000 // Start of extended memory#define PHYSTOP 0xE000000 // Top physical memory#define DEVSPACE 0xFE000000 // Other devices are at high addresses// Key addresses for address space layout (see kmap in vm.c for layout)#define KERNBASE 0x80000000 // First kernel virtual address#define KERNLINK (KERNBASE+EXTMEM) // Address where kernel is linked#ifndef __ASSEMBLER__static inline uint v2p(void *a) &#123; return ((uint) (a)) - KERNBASE; &#125;static inline void *p2v(uint a) &#123; return (void *) ((a) + KERNBASE); &#125;#endif#define V2P(a) (((uint) (a)) - KERNBASE)#define P2V(a) (((void *) (a)) + KERNBASE)#define V2P_WO(x) ((x) - KERNBASE) // same as V2P, but without casts#define P2V_WO(x) ((x) + KERNBASE) // same as V2P, but without casts 内存布局： 关于XV6 的内存页式管理。发生中断时，用哪个页表? 一个内存页是多大? 页目录有多少项? 页表有多少项? 最大支持多大的内存? 画出从虚拟地址到物理地址的转换图。在XV6中，是如何将虚拟地址与物理地址映射的(调用了哪些函数实现了哪些功能)? 发生中断时，将换入cpu的进程的页表首地址存入cr3寄存器；一个内存页为4k；XV6页表采用的二级目录，一级目录有$2^{10}$条，二级目录有$2^{10} * 2^{10}$条；页表项为$2^2$Bytes，故页表有$2^{12} / 2^2 = 2^{10} = 1024$项；最大支持4G内存； 物理内存页的申请与释放，虚拟地址与物理地址如何映射等在“Exercise 1 源代码阅读”都已经详述了，在此主要说下mappages接口，虚拟地址 va与物理地址 pa映射size个字节，同时赋予该页的权限perm，如下: 123456789101112131415161718192021222324252627// vm.c……// Create PTEs for virtual addresses starting at va that refer to// physical addresses starting at pa. va and size might not// be page-aligned.static intmappages(pde_t *pgdir, void *va, uint size, uint pa, int perm)&#123; char *a, *last; pte_t *pte; a = (char*)PGROUNDDOWN((uint)va); last = (char*)PGROUNDDOWN(((uint)va) + size - 1); for(;;)&#123; if((pte = walkpgdir(pgdir, a, 1)) == 0) return -1; if(*pte &amp; PTE_P) panic("remap"); *pte = pa | perm | PTE_P; if(a == last) break; a += PGSIZE; pa += PGSIZE; &#125; return 0;&#125;…… 参考文献[1] xv6虚拟内存-博客园[2] xv6 virtual memory-hexo[3] xv6内存管理-简书[4] xv6内存管理-CSDN]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>XV6</tag>
        <tag>虚拟内存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XV6源代码阅读-中断与系统调用]]></title>
    <url>%2F2019%2F06%2F09%2Fxv6-interrupt-systemcall%2F</url>
    <content type="text"><![CDATA[Exercise1 源代码阅读 启动部分: bootasm.S bootmain.c 和xv6初始化模块:main.c bootasm.S 由16位和32位汇编混合编写成的XV6引导加载器。bootasm.S内的汇编代码会调用bootmain.c中的void bootmain(void)；main.c主函数内部初始化各模块； 当x86 PC启动时，它执行的是一个叫BIOS的程序。BIOS存放在非易失存储器中，BIOS的作用是在启动时进行硬件的准备工作，接着把控制权交给操作系统。具体来说，BIOS会把控制权交给从磁盘第0块引导扇区（用于引导的磁盘的第一个512字节的数据区）加载的代码。引导扇区中包含引导加载器——负责内核加载到内存中。BIOS 会把引导扇区加载到内存 0x7c00 处，接着（通过设置寄存器 %ip）跳转至该地址。引导加载器开始执行后，处理器处于模拟Intel 8088处理器的模式下。而接下来的工作就是把处理器设置为现代的操作模式，并从磁盘中把 xv6内核载入到内存中，然后将控制权交给内核。 123456789101112131415# Start the first CPU: switch to 32-bit protected mode, jump into C.# The BIOS loads this code from the first sector of the hard disk into# memory at physical address 0x7c00 and starts executing in real mode# with %cs=0 %ip=7c00..code16 # Assemble for 16-bit mode.globl startstart: cli # BIOS enabled interrupts; disable # Zero data segment registers DS, ES, and SS. xorw %ax,%ax # Set %ax to zero movw %ax,%ds # -&gt; Data Segment movw %ax,%es # -&gt; Extra Segment movw %ax,%ss # -&gt; Stack Segment 中断与系统调用部分: trap.c trapasm.S vectors.S &amp; vectors.pl syscall.c sysproc.c proc.c 以及相关其他文件代码 trap.c 陷入指令c语言处理接口，trapasm.S陷入指令的汇编逻辑； vector.S由vector.pl生成，中断描述符256个； proc.c 内部主要接口：static struct proc * allocproc(void)、void userinit(void)、int growproc(int n)、int fork(void)、void exit(void)、int wait(void)、void scheduler(void)、void yield(void)； syscall.c 内部定义了各种类型的系统调用函数，sysproc.c内部是与进程创建、退出等相关的系统调用函数的实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// syscall.h System call numbers……#define SYS_fork 1#define SYS_exit 2#define SYS_wait 3#define SYS_pipe 4#define SYS_read 5#define SYS_kill 6#define SYS_exec 7……// syscall.c 声明系统调用……extern int sys_chdir(void);extern int sys_close(void);extern int sys_dup(void);extern int sys_exec(void);extern int sys_exit(void);extern int sys_fork(void);extern int sys_fstat(void);extern int sys_getpid(void);extern int sys_kill(void);extern int sys_link(void);extern int sys_mkdir(void);extern int sys_mknod(void);extern int sys_open(void);……// sysproc.c 定义前面声明的系统调用接口int sys_fork(void)&#123; return fork();&#125;int sys_exit(void)&#123; exit(); return 0; // not reached&#125;int sys_wait(void)&#123; return wait();&#125;int sys_kill(void)&#123; int pid; if(argint(0, &amp;pid) &lt; 0) return -1; return kill(pid);&#125;…… Exercise2 带着问题阅读 什么是用户态和内核态，两者有何区别? 什么是中断和系统调用，两者有何区别? 计算机在运行时，是如何确定当前处于用户态还是内核态的? 当一个进程在执行用户自己的代码时处于用户运行态（用户态），此时特权级最低，为3级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在用户态。Ring3状态不能访问Ring0的地址空间，包括代码和数据；当一个进程因为系统调用陷入内核代码中执行时处于内核运行态（内核态），此时特权级最高，为0级。执行的内核代码会使用当前进程的内核栈，每个进程都有自己的内核栈。用户运行一个程序，该程序创建的进程开始时运行自己的代码，处于用户态。如果要执行文件操作、网络数据发送等操作必须通过write、send等系统调用，这些系统调用会调用内核的代码。进程会切换到Ring0，从而进入内核地址空间去执行内核代码来完成相应的操作。内核态的进程执行完后又会切换到Ring3，回到用户态。这样，用户态的程序就不能随意操作内核地址空间，具有一定的安全保护作用。这说的保护模式是指通过内存页表操作等机制，保证进程间的地址空间不会互相冲突，一个进程的操作不会修改另一个进程地址空间中的数据； 系统调用需要借助于中断机制来实现。两者都是从同一个异常处理入口开始，但是系统调用会一开始让CPU进入内核模式且使能中断，然后从系统调用表中取得相应的注册函数调用之；而中断处理则让CPU进入内核模式且disable中断。所以系统调用的真实处理（系统调用表中的注册函数执行）中可以阻塞，而中断处理的上半部不可以。所以在写驱动代码如字符设备驱动，实现读操作时是可以让其sleep的（比如没有数据时候，用户设置读模式是阻塞型的）。另一方面，如果该驱动读操作过于耗时也是不可取的，它在内核态中执行，这个时候只有中断的优先级比它高，其它的高优先级线程将不能得到及时调度执行； 用户态和内核态的特权级不同，因此可以通过特全级判断当前处于用户态还是内核态。 计算机开始运行阶段就有中断吗? XV6 的中断管理是如何初始化的? XV6 是如何实现内核态到用户态的转变的? XV6 中的硬件中断是如何开关的? 实际的计算机里，中断有哪几种? 计算机开始运行阶段就有BIOS支持的中断； 由于xv6在开始运行阶段没有初始化中断处理程序，于是xv6在bootasm.S中用cli命令禁止中断发生。xv6的终端管理初始化各部分通过main.c中的main()函数调用。picinit()和oapicinit()初始化可编程中断控制器，consoleinit()和uartinit()设置了I/O、设备端口的中断。接着，tvinit()调用trap.c中的代码初始化中断描述符表，关联vectors.S中的中断IDT表项，在调度开始前调用idtinit()设置32号时钟中断，最后在scheduler()中调用sti()开中断，完成中断管理初始化； xv6在proc.c中的userinit()函数中，通过设置第一个进程的tf(trap frame)中cs ds es ss处于DPL_USER(用户模式) 完成第一个用户态进程的设置，然后在scheduler中进行初始化该进程页表、切换上下文等操作，最终第一个进程调用trapret，而此时第一个进程构造的tf中保存的寄存器转移到CPU中，设置了 %cs 的低位，使得进程的用户代码运行在 CPL = 3 的情况下，完成内核态到用户态的转变； xv6的硬件中断由picirq.c ioapic.c timer.c中的代码对可编程中断控制器进行设置和管理，比如通过调用ioapicenable控制IOAPIC中断。处理器可以通过设置 eflags 寄存器中的 IF 位来控制自己是否想要收到中断，xv6中通过命令cli关中断，sti开中断； 中断的种类有：程序性中断：程序性质的错误等，如用户态下直接使用特权指令；外中断: 中央处理的外部装置引发，如时钟中断；I/O中断: 输入输出设备正常结束或发生错误时引发，如读取磁盘完成；硬件故障中断: 机器发生故障时引发，如电源故障；访管中断: 对操作系统提出请求时引发，如读写文件。 什么是中断描述符，中断描述符表（IDT）? 在XV6里是用什么数据结构表示的? 中断描述符表的每一项是一个中断描述符，在x86系统中，中断处理程序定义存储在IDT中。XV6的IDT有256个入口点，每个入口点中对应的处理程序不同，在出发trap时，只要找到对应编号的入口，就能得到对应的处理程序； XV6中的数据结构中中断描述符用struct gatedesc表示： 12345678910111213141516// trap.c# generated by vectors.pl - do not edit# handlers.globl alltraps.globl vector0vector0: pushl $0 pushl $0 jmp alltraps.globl vector1vector1: pushl $0 pushl $1 jmp alltraps.globl vector2…… alltraps继续保存处理器的寄存器，设置数据和CPU段，然后压入 %esp，调用trap，到此时已完成用户态到内核态的转变； 1234567891011121314151617181920212223// trapasm.S # vectors.S sends all traps here..globl alltrapsalltraps: # Build trap frame. pushl %ds pushl %es pushl %fs pushl %gs pushal # Set up data and per-cpu segments. 设置数据和CPU段 movw $(SEG_KDATA&lt;&lt;3), %ax movw %ax, %ds movw %ax, %es movw $(SEG_KCPU&lt;&lt;3), %ax movw %ax, %fs movw %ax, %gs # Call trap(tf), where tf=%esp 压入 %esp pushl %esp # 调用trap call trap addl $4, %esp trap会根据%esp指向对应的tf，首先根据trapno判断该中断是否是系统调用，之后判断硬件中断，由于除零不是以上两种，于是判断为代码错误中断，并且是发生在用户空间的。接着处理程序将该进程标记为killed，并退出，继续下一个进程的调度； 12345678910111213141516171819202122232425262728293031323334353637// trap.c//PAGEBREAK: 41void trap(struct trapframe *tf)&#123; if(tf-&gt;trapno == T_SYSCALL)&#123; // 判断该中断是否为系统调用 if(proc-&gt;killed) exit(); proc-&gt;tf = tf; syscall(); if(proc-&gt;killed) exit(); return; &#125; switch(tf-&gt;trapno)&#123; …… // PAGEBREAK: 13 // tf-&gt;trapno与其他case语句对不上,除零被视为代码错误中断,进入这里杀掉进程 default: if(proc == 0 || (tf-&gt;cs&amp;3) == 0)&#123; // In kernel, it must be our mistake. cprintf("unexpected trap %d from cpu %d eip %x (cr2=0x%x)\n", tf-&gt;trapno, cpu-&gt;id, tf-&gt;eip, rcr2()); panic("trap"); &#125; // In user space, assume process misbehaved. cprintf("pid %d %s: trap %d err %d on cpu %d " "eip 0x%x addr 0x%x--kill proc\n", proc-&gt;pid, proc-&gt;name, tf-&gt;trapno, tf-&gt;err, cpu-&gt;id, tf-&gt;eip, rcr2()); proc-&gt;killed = 1; &#125; ……&#125; 涉及到的主要数据结构：中断描述符表IDT(trap.c +12)、（vi x86.h +150）、（vi vector.S）。 123456789101112131415161718192021222324252627282930313233343536// trap.c// Interrupt descriptor table (shared by all CPUs).struct gatedesc idt[256];extern uint vectors[]; // in vectors.S: array of 256 entry pointers……// x86.h//PAGEBREAK: 36// Layout of the trap frame built on the stack by the// hardware and by trapasm.S, and passed to trap().struct trapframe &#123; // registers as pushed by pusha uint edi; uint esi; uint ebp; uint oesp; // useless &amp; ignored uint ebx; uint edx; uint ecx; uint eax; ……&#125;;// vector.S 0～255共256个vectors: .long vector0 .long vector1 .long vector2 .long vector3 .long vector4 .long vector5 .long vector6 .long vector7 .long vector8 .long vector9 …… 请以系统调用setrlimit(该系统调用的作用是设置资源使用限制)为例，叙述如何在XV6中实现一个系统调用。(提示:需要添加系统调用号，系统调用函数，用户接口等等)。 在syscall.h中添加系统调用号 #define SYS_setrlimit 22； 12345// syscall.h……#define SYS_mkdir 20#define SYS_close 21#define SYS_setrlimit 22 // add by yangyu 在syscall.c中添加对应的处理程序的调用接口 12345678// syscall.c……static int (*syscalls[])(void) = &#123;……[SYS_mkdir] sys_mkdir,[SYS_close] sys_close,[SYS_setrlimit] SYS_setrlimit, // add by yangyu&#125;; 在sysproc.c中添加系统调用函数int sys_setrlimit(void)，具体实现对于进程资源使用限制的设置； 1234567891011121314151617// syspro.c……int sys_uptime(void)&#123; uint xticks; acquire(&amp;tickslock); xticks = ticks; release(&amp;tickslock); return xticks;&#125;// 在这里面写逻辑,限制进程资源的使用int sys_setrlimit(void)&#123; // to do&#125; 在user.h中声明系统调用接口int setrlimit(int resource, const struct rlimit * rlim)； 1234567// syspro.c……// system callsint fork(void);int exit(void) __attribute__((noreturn));…… // 调用该接口陷入内核执行系统调用int setrlimit(int resource, const struct rlimit *rlim); 在usys.S添加SYSCALL(setrlimit)。 12345// usys.S……SYSCALL(sleep)SYSCALL(uptime)SYSCALL(setrlimit) // add by yangyu 参考文献[1] xv6 idt初始化[2] xv6中文文档[3] xv6 alltraps[4] xv6 trap/interrupt]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>XV6</tag>
        <tag>中断</tag>
        <tag>系统调用</tag>
        <tag>中断描述符表IDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XV6源代码阅读-进程线程]]></title>
    <url>%2F2019%2F06%2F09%2Fxv6-process-thread%2F</url>
    <content type="text"><![CDATA[Exercise1 源代码阅读 基本头文件：types.h param.h memlayout.h defs.h x86.h asm.h mmu.h elf.h types.h：仅仅是定义uint, ushort, uchar pde_t别名； 1234typedef unsigned int uint;typedef unsigned short ushort;typedef unsigned char uchar;typedef uint pde_t; parame.h 利用宏定义了进程最大数量，进程的内核栈大小，CPU的最大数量，进程能打开的文件描述符数等； 1234567891011#define NPROC 64 // maximum number of processes#define KSTACKSIZE 4096 // size of per-process kernel stack#define NCPU 8 // maximum number of CPUs#define NOFILE 16 // open files per process#define NFILE 100 // open files per system#define NBUF 10 // size of disk block cache#define NINODE 50 // maximum number of active i-nodes#define NDEV 10 // maximum major device number#define ROOTDEV 1 // device number of file system root disk#define MAXARG 32 // max exec arguments#define LOGSIZE 10 // max data sectors in on-disk log memlayout.h：利用宏定义内存空间范围，地址和偏移的转换等； defs.h：内部包含了许多前向声明，同时声明了许多全局函数，这些全局函数的实现在具体使用的模块定义； x86.h：定义了许多C代码与汇编指令交互的内敛函数，同时定义了从用户态陷入内核态的数据结构struct trapframe; asm.h：汇编宏定义； mmu.h：内存管理单元，进程地址空间详细数据结构； elf.h：声明了elf格式的目标文件有关的数据结构。 进程线程部分：vm.c proc.h proc.c swtch.S kalloc.c以及相关其他文件代码 vm.c：内存管理接口，比如页表申请，页表释放，页目录的转换，cpu与进程的绑定等； proc.h：声明了cpu、进程、进程上下文等数据结构；swtch.S：内部是上下文切换的汇编指令，保存换出进程的现场，加载换入进程的现场； kalloc.c：物理内存的申请和释放。主要接口：void kinit1(void vstart, void vend), kinit2(void vstart, void vend), char kalloc(void), void kfree(char v)。 Exercise2 带着问题阅读 什么是进程？什么是线程？操作系统的资源分配单位和调度单位分别是什么？XV6 中的进程和线程分别是什么，都实现了吗? 进程是在多道程序系统出现以后，为了描述系统内部各作业的活动规律而引进的概念。进程有3个基本状态：运行状态、就绪状态和等待状态；进程是具有独立功能的程序关于某个数据集合上的一次运行活动； 线程是轻量级的进程，线程是进程内的一个相对独立的可执行的单元，若把进程称为任务的话，那么线程则是应用中的一个子任务的执行； 操作系统的资源分配的单位是进程，处理机调度的单位是线程； xv6操作系统实现了一个基于进程 (没有实现线程) 的简单进程管理机制。XV6中进程和CPU的数据结构见proc.h。 12345678910111213141516171819202122232425262728// proc.h // 上下文切换现场struct context &#123; uint edi; uint esi; uint ebx; uint ebp; uint eip;&#125;;// 枚举进程状态enum procstate &#123; UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, ZOMBIE &#125;;// Per-process statestruct proc &#123; uint sz; // Size of process memory (bytes) pde_t* pgdir; // Page table char *kstack; // Bottom of kernel stack for this process enum procstate state; // Process state volatile int pid; // Process ID struct proc *parent; // Parent process struct trapframe *tf; // Trap frame for current syscall struct context *context; // swtch() here to run process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging)&#125;; 进程管理的数据结构是什么？在 Windows，Linux，XV6 中分别叫什么名字？其中包含哪些内容？操作系统是如何进行管理进程管理数据结构的？它们是如何初始化的？ 进程管理的数据结构是进程控制块 (PCB ); Linux下为/include/linux/sched.h内部的struct task_struct，其中包括管理进程所需的各种信息。创建一个新进程时，系统在内存中申请一个空的task_struct ，并填入所需信息。同时将指向该结构的指针填入到task[]数组中。当前处于运行状态进程的PCB用指针数组current_set[]来指出。这是因为 Linux 支持多处理机系统，系统内可能存在多个同时运行的进程，故 current_set定义成指针数组； Windows下用EPROCESS(执行体进程对象)表示 ， PCB 也称为内核进程块KPROCESS(KPROCESS即内核进程对象 )，EPOCESS和KPROCESS位于内核空间 ； XV6下在proc .h内声明，包括进程 ID ，进程状态 ，父进程，context，cpu记录了内存地址和栈指针等。XV6中通过get_raw_proc()对进程进行控制管理，get_raw_proc()方法可以创建一个新的进程并对进程进行初始化分配进程页表和内核堆栈。 XV6中进程有哪些状态? 请画出XV6 的进程状态转化图。在Linux中，进程的状态分别包括哪些? 你认为操作系统的设计者为什么会有这样的设计思路? XV6中进程各状态及转换图如下： Linux 的进程状态可分为R (TASK_RUNNING)，可执行状态，S (TASK_INTERRUPTIBLE)，可中断的睡眠状态，D(TASK_UNINTERRUPTIBLE)不可中断的睡眠状态，T(TASK_STOPPED or TASK_TRACED)暂停状态或跟踪状态，Z(TASK_DEAD - EXIT_ZOMBIE)退出状态但资源未回收，称为僵尸进程，X (TASK_DEAD - EXIT_DEAD)退出状态，进程即将被销毁。 设计这这样设计的目的，是为了cpu能够根据进程的状态进行更好的调度，提高cpu的利用率，适应io密集型和cpu密集型的进程，避免cpu的浪费。 如何启动多进程(创建子进程)? 如何调度多进程，调度算法有哪些? 操作系统为何要限制一个CPU最大支持的进程数? XV6中的最大进程数是多少？如何执行进程的切换? 什么是进程上下文? 多进程和多CPU有什么关系? 父进程可以利用fork()函数创建多个子进程。首先，为每个新建的子进程分配一个空闲的proc结构并赋予子进程唯一标识pid；其次，以一次一页的方式复制父进程地址空间（采用cow写时复制），获得子进程继承的共享资源的指针；最后将子进程加入就绪队列，对子进程返回标识符0，向父进程返回子进程pid； cpu使用规定的调度算法从就绪队列选择一个进程执行，常用调度算法：时间片轮转调度、先来先服务、短作业优先调度策略、基于优先级的可抢占调度策略、基于优先级的不可抢占式调度策略、最短剩余时间优先算法、高响应比优先算法、多级反馈队列算法等； 一是内存空间有限，如果读入的进程数量过多，势必会吃掉大量的内存空间，而cpu在调度过程中也会从栈或堆上申请空间，如果申请失败则无法继续运行。二是增加了缺页中断的可能性，会导致cpu不断的执行页面换入换出，使得大部分时间浪费在无意义的事情上； XV6的最大进程数见param.h文件中的#define NPROC 64，最大64； 进程切换是一个进程让出处理器，由另一个进程占用处理器的过程。进程的切换是由进程状态的变化引起的，而进程状态的变化又与出现的事件有关。当有事件(中断或异常)发生时，当前运行进程暂停，保存当前进程的现场，然后根据调度算法从就绪队列选一个进程换入CPU，同时加载换入进程的现场进行执行； 进程的上下文包括当前进程的程序计数 器PC和当前运行的CPU中各个寄存器的内容。当进程切换和发生中断的时候这些信息要保存下来以便于下次运行时使用； 同一时刻每个cpu上只能有一个进程被执行，且同一时刻一个进程只能被一个cpu调度，同一时刻多个cpu可以同时调度不同的进程，同一时间段内每个cpu可以调度多个进程。 内核态进程是什么? 用户态进程是什么? 它们有什么区别? 多数系统将处理器工作状态划分为内核态和用户态。前者一般指操作系统管理程序运行的状态，具有较高的特权级别，又称为特权态、系统态或管态；后者一般指用户程序运行时的状态；具有较低的特权级别，又称为普通态、目态。区分了用户态和内核态就是限定用户什么操作可以做，什么操作不能让用户直接做。如果遇到不能让用户直接做的操作，用户就必须请求操作系统做系统调用，这样操作系统就会进入内核态进行系统操作。内核态的进程就是系统进入内核态之后进行系统操作所产生的进程； 用户态进程是用户通过请求操作而产生的进程； 区别: 运行在不同的系统状态，用户态进程执行在用户态，内核态进程执行在内核态；进入方式不同，用户态进程可直接进入，内核态必须通过运行系统调用命令；返回方式不同，用户态进程直接返回，内核态进程有重新调度过程；内核态进程优先级要高于用户态进程，并且内核态进程特权级别最高，它可以执行系统级别的代码。 进程在内存中是如何布局的？ 进程的堆和栈有什么区别? 内存分为内核空间和用户空间，内核空间一般运行操作系统程序，而用户空间一般运行用户程序。主要目的是对系统程序进行包含。进程在内存中包含堆、栈、数据段、代码段。代码段 : 保存程序的执行码，在进程并发时，代码段是共享的且只读的，在存储器中只需有一个副本。数据段 : 此段又称为初始化数据段，它包含了程序中已初始化的全局变量、全局静态变量、局部静态变量。 栈 : 程序执行前静态分配的内存空间，栈的大小可在编译时指定，Linux环境下默认为 8M。栈段是存放程序执行时局部变量、函数调用信息、中断现场保留信息的空间。程序执行时，CPU堆栈段指针会在栈顶根据执行情况进行上下移动。 堆 : 程序执行时, 按照程序需要动态分配的内存空间，使用malloc、 calloc、realloc函数分配的空间都在堆上分配。 参考文献[1] xv6进程与内存管理-CSDN[2] linux进程地址空间-博客园[3] xv6进程线程-百度文库[4] 操作系统-进程线程模型课件]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>XV6</tag>
        <tag>进程</tag>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XV6源代码阅读-同步机制]]></title>
    <url>%2F2019%2F06%2F08%2Fxv6-synch%2F</url>
    <content type="text"><![CDATA[Exercise1 源代码阅读锁部分：spinlock.h/spinlock.c以及相关其他文件代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// Mutual exclusion lock.struct spinlock &#123; uint locked; // 0未被占用, 1已被占用 // For debugging: char *name; // Name of lock. struct cpu *cpu; // The cpu holding the lock. uint pcs[10]; // The call stack (an array of program counters) // that locked the lock.&#125;;// 初始化自旋锁void initlock(struct spinlock *lk, char *name)&#123; lk-&gt;name = name; lk-&gt;locked = 0; lk-&gt;cpu = 0;&#125;// Acquire the lock.// Loops (spins) until the lock is acquired.// Holding a lock for a long time may cause// other CPUs to waste time spinning to acquire it.void acquire(struct spinlock *lk)&#123; // 关中断 pushcli(); // disable interrupts to avoid deadlock. if(holding(lk)) // 判断锁的持有是否为当前cpu panic("acquire"); // The xchg is atomic. // It also serializes, so that reads after acquire are not // reordered before it. while(xchg(&amp;lk-&gt;locked, 1) != 0); // 拿不到锁开始自旋 // Record info about lock acquisition for debugging. lk-&gt;cpu = cpu; getcallerpcs(&amp;lk, lk-&gt;pcs);&#125;// Release the lock.void release(struct spinlock *lk)&#123; if(!holding(lk)) panic("release"); lk-&gt;pcs[0] = 0; lk-&gt;cpu = 0; // The xchg serializes, so that reads before release are // not reordered after it. The 1996 PentiumPro manual (Volume 3, // 7.2) says reads can be carried out speculatively and in // any order, which implies we need to serialize here. // But the 2007 Intel 64 Architecture Memory Ordering White // Paper says that Intel 64 and IA-32 will not move a load // after a store. So lock-&gt;locked = 0 would work here. // The xchg being asm volatile ensures gcc emits it after // the above assignments (and after the critical section). xchg(&amp;lk-&gt;locked, 0); popcli();&#125; Exercise2 带着问题阅读 什么是临界区? 什么是同步和互斥? 什么是竞争状态? 临界区操作时中断是否应该开启? 中断会有什么影响? XV6的锁是如何实现的，有什么操作? xchg 是什么指令，该指令有何特性? 临界区(Critical Section)：访问临界区的那段代码，多个进程/线程必须互斥进入临界区； 同步(Synchronization)：指多个进程/线程能够按照程序员期望的方式来协调执行顺序，为了实现这个目的，必须要借助于同步机制(如信号量，条件变量，管程等)； 互斥(Mutual Exclusion)：互斥的目的是保护临界区； 竞争状态：竞争是基于并发环境下的，单个进程/线程不存在竞争，在并发环境下，多个进程/线程都需要请求某资源的时候，只有竞争到该资源的进程/线程才能够执行，释放资源后，剩余进程/线程按照预定的算法策略重新竞争； 操作临界区必须关中断，对临界区的操作是原子性的； 中断影响：中断降低了并发性能，同时中断也会导致频繁的上下文切换，上下文切换会导致tlb快表失效，因此要尽可能的缩减中断处理的时间； 自旋锁(Spinlock)：xv6中利用该数据结构实现多个进程/线程同步和互斥访问临界区。当进程/线程请求锁失败时进入循环，直至锁可用并成功拿到后返回，对于单cpu系统自旋锁浪费CPU资源，不利于并发，自旋锁的优势体现在多CPU系统下，XV6支持多CPU。主要接口有void initlock(struct spinlock lk, char name)、void initlock(struct spinlock lk, char name)、void release(struct spinlock * lk)； xchg：xchg()函数使用GCC的内联汇编语句，该函数中通过xchg原子性交换spinlock.locked和newval，并返回spinlock.locked原来的值。当返回值为1时，说明其他线程占用了该锁，继续循环等待；当返回值为0时，说明其他地方没有占用该锁，同时locked本设置成1了，所以该锁被此处占用。 123456789101112// x86.h 调用方式如xchg(&amp;lk-&gt;locked, 1)static inline uint xchg(volatile uint *addr, uint newval)&#123; uint result; // The + in "+m" denotes a read-modify-write operand. asm volatile("lock; xchgl %0, %1" : "+m" (*addr), "=a" (result) : "1" (newval) : "cc"); return result;&#125; 基于XV6的spinlock, 请给出实现信号量、读写锁、信号机制的设计方案(三选二，请写出相应的伪代码)? 信号量实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556struct semaphore &#123; int value; struct spinlock lock; struct proc *queue[NPROC]; // 进程等待队列,这是一个循环队列 int end; // 队尾 int start; // 队头&#125;;// 初始化信号量void sem_init(struct semaphore *s, int value) &#123; s-&gt;value = value; initlock(&amp;s-&gt;lock, "semaphore_lock"); end = start = 0;&#125;void sem_wait(struct semaphore *s) &#123; acquire(&amp;s-&gt;lock); // 竞争锁,如果竞争不到进入自旋 s-&gt;value--; if (s-&gt;value &lt; 0) &#123; s-&gt;queue[s-&gt;end] = myproc(); // myproc()获取当前进程, 放入队尾 s-&gt;end = (s-&gt;end + 1) % NPROC; // 循环队列计算新的队尾 // 1. 释放锁(下一个sem_wait的进程才能进入acquire), // 2. 然后进入睡眠等待, 被唤醒时重新竞争锁 sleep(myproc(), &amp;s-&gt;lock); &#125; release(&amp;s-&gt;lock);&#125;void sem_signal(struct semaphore *s) &#123; acquire(&amp;s-&gt;lock); // 竞争锁 s-&gt;value++; if (s-&gt;value &lt;= 0) &#123; wakeup(s-&gt;queue[s-&gt;start]); // 唤醒循环队列头的进程 s-&gt;queue[s-&gt;start] = 0; s-&gt;start = (s-&gt;start + 1) % NPROC; // 重新计算队头 &#125; release(&amp;s-&gt;lock);&#125;// proc.h// Per-process statestruct proc &#123; uint sz; // Size of process memory (bytes) pde_t* pgdir; // Page table char *kstack; // Bottom of kernel stack for this process enum procstate state; // Process state volatile int pid; // Process ID struct proc *parent; // Parent process struct trapframe *tf; // Trap frame for current syscall struct context *context; // swtch() here to run process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging)&#125;; 参考文献[1] xv6锁-博客园[2] xv6锁-xchg[3] xv6锁-CSDN[4] xv6整体报告-百度文库]]></content>
      <categories>
        <category>同步机制</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>XV6</tag>
        <tag>自旋锁</tag>
        <tag>中断</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XV6源代码阅读-文件系统]]></title>
    <url>%2F2019%2F06%2F08%2Fxv6-filesystem%2F</url>
    <content type="text"><![CDATA[Exercise1 源代码阅读文件系统部分 buf.h fcntl.h stat.h fs.h file.h ide.c bio.c log.c fs.c file.c sysfile.c exec.c buf.h：对xv6中磁盘块数据结构进行定义，块大小为512字节。 12345678910111213// xv6中磁盘块数据结构,块大小512字节struct buf &#123; int flags; // DIRTY, VALID uint dev; uint sector; // 对应扇区 struct buf *prev; // LRU cache list struct buf *next; // 链式结构用于连接 struct buf *qnext; // disk queue uchar data[512];&#125;;#define B_BUSY 0x1 // buffer is locked by some process#define B_VALID 0x2 // buffer has been read from disk#define B_DIRTY 0x4 // buffer needs to be written to disk fcntl.h：宏定义操作权限。 1234#define O_RDONLY 0x000 // 只读#define O_WRONLY 0x001 // 只写#define O_RDWR 0x002 // 读写#define O_CREATE 0x200 // 创建 stat.h：声明文件或目录属性数据结构。 1234567891011#define T_DIR 1 // Directory#define T_FILE 2 // File#define T_DEV 3 // Devicestruct stat &#123; short type; // Type of file int dev; // File system's disk device uint ino; // Inode number short nlink; // Number of links to file uint size; // Size of file in bytes&#125;; fs.h / fs.c：声明超级块、dinode、文件和目录数据结构，以及相关的宏定义。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#define ROOTINO 1 // root i-number#define BSIZE 512 // block size// File system super blockstruct superblock &#123; uint size; // Size of file system image (blocks) uint nblocks; // Number of data blocks uint ninodes; // Number of inodes. uint nlog; // Number of log blocks&#125;;#define NDIRECT 12#define NINDIRECT (BSIZE / sizeof(uint))#define MAXFILE (NDIRECT + NINDIRECT)// 磁盘上inode节点体现形式// On-disk inode structurestruct dinode &#123; short type; // File type short major; // Major device number (T_DEV only) short minor; // Minor device number (T_DEV only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT+1]; // Data block addresses&#125;;// Inodes per block.#define IPB (BSIZE / sizeof(struct dinode))// Block containing inode i#define IBLOCK(i) ((i) / IPB + 2)// Bitmap bits per block#define BPB (BSIZE*8)// Block containing bit for block b#define BBLOCK(b, ninodes) (b/BPB + (ninodes)/IPB + 3)// Directory is a file containing a sequence of dirent structures.#define DIRSIZ 14// 文件或目录据结构，目录本身是以文件的方式存储到磁盘上的，叫做目录文件。struct dirent &#123; ushort inum; // i节点 char name[DIRSIZ]; // 文件或目录名&#125;; file.h：声明inode、file数据结构。 12345678910111213141516171819202122232425262728293031323334353637383940struct file &#123; // 分为管道文件,设备文件,普通文件 enum &#123; FD_NONE, FD_PIPE, FD_INODE &#125; type; int ref; // reference count char readable; char writable; struct pipe *pipe; struct inode *ip; // 指向inode节点 uint off;&#125;;// 在内存中inode节点体现形式// in-memory copy of an inodestruct inode &#123; uint dev; // Device number uint inum; // Inode number int ref; // Reference count int flags; // I_BUSY, I_VALID // 下面这些编程都是dinode的拷贝 // copy of disk inode short type; short major; short minor; short nlink; uint size; uint addrs[NDIRECT+1];&#125;;#define I_BUSY 0x1#define I_VALID 0x2// table mapping major device number to device functionsstruct devsw &#123; int (*read)(struct inode*, char*, int); int (*write)(struct inode*, char*, int);&#125;;extern struct devsw devsw[];#define CONSOLE 1 ide.c：磁盘IO的具体实现，xv6维护了一个进程请求磁盘操作的队列(idequeue)。当进程调用void iderw(struct buf *b)请求读写磁盘时，该请求被加入等待队列idequeue，同时进程进入睡眠状态。当一个磁盘读写操作完成时，会触发一个中断，中断处理程序ideintr()会移除队列开头的请求，唤醒队列开头请求所对应的进程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// idequeue points to the buf now being read/written to the disk.// idequeue-&gt;qnext points to the next buf to be processed.// You must hold idelock while manipulating queue.static struct spinlock idelock; // 保护 idequeuestatic struct buf *idequeue; // 磁盘读写操作的请求队列……// 等待磁盘进入空闲状态// Wait for IDE disk to become ready.static int idewait(int checkerr)&#123; …… // while(((r = inb(0x1f7)) &amp; (IDE_BSY|IDE_DRDY)) != IDE_DRDY); ……&#125;// 初始化IDE磁盘IOvoid ideinit(void)&#123; ……&#125;// 开始一个磁盘读写请求// Start the request for b. Caller must hold idelock.static void idestart(struct buf *b)&#123; ……&#125;// 当磁盘请求完成后中断处理程序会调用的函数// Interrupt handler.void ideintr(void)&#123; …… // 处理完一个磁盘IO请求后，唤醒等待在等待队列头的那个进程 wakeup(b); // 如果队列不为空，继续处理下一个磁盘IO任务 // Start disk on next buf in queue. if(idequeue != 0) idestart(idequeue); ……&#125;//PAGEBREAK! 上层文件系统调用的磁盘IO接口// Sync buf with disk. // If B_DIRTY is set, write buf to disk, clear B_DIRTY, set B_VALID.// Else if B_VALID is not set, read buf from disk, set B_VALID.void iderw(struct buf *b)&#123; …… // 竞争锁 acquire(&amp;idelock); //DOC:acquire-lock // Append b to idequeue. b-&gt;qnext = 0; for(pp=&amp;idequeue; *pp; pp=&amp;(*pp)-&gt;qnext) //DOC:insert-queue ; *pp = b; // Start disk if necessary. 开始处理一个磁盘IO任务 if(idequeue == b) idestart(b); // Wait for request to finish. 睡眠等待 while((b-&gt;flags &amp; (B_VALID|B_DIRTY)) != B_VALID)&#123; sleep(b, &amp;idelock); &#125; release(&amp;idelock); // 释放锁&#125; bio.c：Buffer Cache的具体实现。因为读写磁盘操作效率不高，根据时间与空间局部性原理，这里将最近经常访问的磁盘块缓存在内存中。主要接口有struct buf bread(uint dev, uint sector)、void bwrite(struct buf b)，bread会首先从缓存中去寻找块是否存在，如果存在直接返回，如果不存在则请求磁盘读操作，读到缓存中后再返回结果。bwrite直接将缓存中的数据写入磁盘。 log.c：该模块主要是维护文件系统的一致性。引入log模块后，对于上层文件系统的全部磁盘操作都被切分为transaction，每个transaction都会首先将数据和其对应磁盘号写入磁盘上的log区域，且只有在log区域写入成功后，才将log区域的数据写入真正存储的数据块。因此，如果在写log的时候宕机，重启后文件系统视为该log区的写入不存在，如果从log区写到真实区域的时候宕机，则可根据log区域的数据恢复。 sysfile.c：主要定义了与文件相关的系统调用。主要接口及含义如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// Allocate a file descriptor for the given file.// Takes over file reference from caller on success.static int fdalloc(struct file *f)&#123; …… // 申请一个未使用的文件句柄&#125;int sys_dup(void)&#123; …… // 调用filedup对文件句柄的引用计数+1 filedup(f); return fd;&#125;int sys_read(void)&#123; …… // 读取文件数据 return fileread(f, p, n);&#125;int sys_write(void)&#123; …… // 向文件写数据 return filewrite(f, p, n);&#125;int sys_close(void)&#123; …… // 释放文件句柄资源 fileclose(f); return 0;&#125;int sys_fstat(void)&#123; …… // 修改文件统计信息 return filestat(f, st);&#125;// Create the path new as a link to the same inode as old.int sys_link(void)&#123; …… // 为已有的inode创建一个新名字&#125;//PAGEBREAK!int sys_unlink(void)&#123; …… // 解除inode中的某个名字, 若名字全被移除, inode回被释放&#125;static struct inode* create(char *path, short type, short major, short minor)&#123; …… // &#125;int sys_mkdir(void)&#123; …… // 创建一个目录&#125;int sys_mknod(void)&#123; …… // 创建一个新文件&#125;int sys_chdir(void)&#123; …… // 切换目录&#125;int sys_pipe(void)&#123; …… // 创建一个管道文件&#125; exec.c：只有一个exec接口，实质就是传入elf格式的可执行文件，装载到内存并分配内存页，argv是一个指针数组，用于携带参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556int exec(char *path, char **argv)&#123; …… // 判断文件是否存在 if((ip = namei(path)) == 0) return -1; ilock(ip); pgdir = 0; // Check ELF header 检查elf头是否合法 if(readi(ip, (char*)&amp;elf, 0, sizeof(elf)) &lt; sizeof(elf)) goto bad; …… // Load program into memory. sz = 0; for(i=0, off=elf.phoff; i&lt;elf.phnum; i++, off+=sizeof(ph))&#123; if(readi(ip, (char*)&amp;ph, off, sizeof(ph)) != sizeof(ph)) goto bad; if(ph.type != ELF_PROG_LOAD) continue; if(ph.memsz &lt; ph.filesz) goto bad; if((sz = allocuvm(pgdir, sz, ph.vaddr + ph.memsz)) == 0) goto bad; if(loaduvm(pgdir, (char*)ph.vaddr, ip, ph.off, ph.filesz) &lt; 0) goto bad; &#125; iunlockput(ip); ip = 0; // Allocate two pages at the next page boundary. // Make the first inaccessible. Use the second as the user stack. sz = PGROUNDUP(sz); if((sz = allocuvm(pgdir, sz, sz + 2*PGSIZE)) == 0) goto bad; clearpteu(pgdir, (char*)(sz - 2*PGSIZE)); sp = sz; // Push argument strings, prepare rest of stack in ustack. for(argc = 0; argv[argc]; argc++) &#123; if(argc &gt;= MAXARG) goto bad; sp = (sp - (strlen(argv[argc]) + 1)) &amp; ~3; if(copyout(pgdir, sp, argv[argc], strlen(argv[argc]) + 1) &lt; 0) goto bad; ustack[3+argc] = sp; &#125; …… bad: if(pgdir) freevm(pgdir); if(ip) iunlockput(ip); return -1;&#125; Exercise2 带着问题阅读 了解 UNIX 文件系统的主要组成部分：超级块(superblock)，i节点(inode)，数据块(datablock)，目录块(directoryblock)，间接块(indirectionblock)。分别解释它们的作用。 boot super block dinode free bitmap blocks data blocks log blocks 第0块 第1块 superblock.ninodes块 位图管理空闲区块 superblock.nblocks块 superblock.nlog块 bootloader引导区(第0块)：用于存放引导程序，系统启动从这里开始； superblock超级块(第1块)：记录文件系统的元信息，如文件系统的总块数，数据块块数，i节点数，日志的块数； i节点(inode)：从第2块开始存放 i 节点，每一块能够存放多个 i 节点； bitmap空闲块管理区：用于存放空闲块位图，因为系统需要知道文件系统的使用情况，哪些块已经分配出去了，哪些块还未被分配； 数据块 (datablock)：数据块存储的是真真实实的文件内容； 目录块(directoryblock)：文件系统中除了文件外，还有目录，目录本身是一个文件目录(由很多FCB组成)，文件目录也需要以文件的形式存储到磁盘上，存储到磁盘上的这个文件叫做目录文件，目录文件就是存储到目录块中的； 间接块(indirectionblock)：xv6这里应该是指log日志块，这是文件系统执行磁盘IO操作的中间层，主要目的是维护文件系统的一致性。 阅读文件ide.c。这是一个简单的ide硬盘驱动程序，对其内容作大致了解。 xv6 的文件系统分6层实现，从底至顶如下： System calls File descriptors Pathnames Recursive lookup Directories Directory inodes Files Inodes and block allocator Transactions Logging Blocks Buffer cache 底层通过块缓冲Buffer cache读写IDE 硬盘，它同步了对磁盘的访问，保证同时只有一个内核进程可以修改磁盘块； 第二层Loggins向上层提供服务，该层实现了文件系统的一致性，使得更高层的接口可以将对磁盘的更新按会话打包，通过会话的方式来保证这些操作是原子操作(要么都被应用，要么都不被应用)； 第三层提供无名文件，每一个这样的文件由一个 i 节点和一连串的数据块组成； 第四层将目录实现为一种特殊的 i 节点，它的内容是一连串的目录项，每一个目录项包含一个文件名和对应的 i 节点； 第五层提供了层次路经名（如/usr/rtm/xv6/fs.c这样的），这一层通过递归的方式来查询路径对应的文件； 最后一层将许多 UNIX 的资源（如管道，设备，文件等）抽象为文件系统的接口，极大地简化了程序员的工作。 阅读文件buf.h，bio.c。了解 XV6 文件系统中buffer cache层的内容和实现。描述buffer双链表数据结构及其初始化过程。了解 buffer的状态。了解对buffer的各种操作。 数据结构bcache维护了一个由struct buf组成的双向链表，同时bcache.lock用户互斥访问； 首先系统调用binit()初始化缓存，随即调用initlock初始化bcache.lock，然后循环遍历buf数组，采用头插法逐个链接到bcache.head后； 上层文件系统读磁盘时，调用bread()，随即调用bget()检查请求的磁盘块是否在缓存中，如果命中，返回缓存命中结果。如果未命中，转到底层的iderw()函数先将此磁盘块从磁盘加载进缓存中，再返回此磁盘块； 上层文件系统写磁盘时，调用bwrite()直接将缓存中的数据写入磁盘。Buffer Cache层不会尝试执行任何延迟写入的操作，何时调用bwrite()写入磁盘是由上层的文件系统控制的； 上层文件系统可通过调用brelse()释放一块不再使用的缓冲区。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// buf.hstruct buf &#123; int flags; uint dev; uint sector; struct buf *prev; // LRU cache list struct buf *next; struct buf *qnext; // disk queue uchar data[512];&#125;;// bio.cstruct &#123; struct spinlock lock; struct buf buf[NBUF]; // Linked list of all buffers, through prev/next. // head.next is most recently used. struct buf head;&#125; bcache;void binit(void)&#123; struct buf *b; initlock(&amp;bcache.lock, "bcache"); //PAGEBREAK! 头插法,每次都是插入到bcache.head的后面 // Create linked list of buffers bcache.head.prev = &amp;bcache.head; bcache.head.next = &amp;bcache.head; for(b = bcache.buf; b &lt; bcache.buf+NBUF; b++)&#123; b-&gt;next = bcache.head.next; b-&gt;prev = &amp;bcache.head; b-&gt;dev = -1; bcache.head.next-&gt;prev = b; bcache.head.next = b; &#125;&#125;// Return a B_BUSY buf with the contents of the indicated disk sector.struct buf* bread(uint dev, uint sector)&#123; struct buf *b; // 优先查找缓存 b = bget(dev, sector); if(!(b-&gt;flags &amp; B_VALID)) iderw(b); // 命中失败时调用下一次接口真真实实读磁盘 return b;&#125;// Write b's contents to disk. Must be B_BUSY.void bwrite(struct buf *b)&#123; if((b-&gt;flags &amp; B_BUSY) == 0) panic("bwrite"); b-&gt;flags |= B_DIRTY; iderw(b); // 立即写, 未延迟写&#125; 阅读文件log.c，了解XV6文件系统中的logging和transaction机制;日志存在于磁盘末端已知的固定区域。它包含了一个起始块，紧接着一连串的数据块。起始块包含了一个扇区号的数组，每一个对应于日志中的数据块，起始块还包含了日志数据块的计数。xv6 在提交后修改日志的起始块，而不是之前，并且在将日志中的数据块都拷贝到文件系统之后将数据块计数清0。提交之后，清0之前的崩溃就会导致一个非0的计数值。 阅读文件fs.h/fs.c。了解XV6文件系统的硬盘布局。 1234567891011// On-disk inode structurestruct dinode &#123; short type; // File type short major; // Major device number (T_DEV only) short minor; // Minor device number (T_DEV only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) // NDIRECT = 12, 前12个为直接索引, // 第13个为间接索引, 可容纳128个直接索引 uint addrs[NDIRECT+1]; // Data block addresses &#125;; 阅读文件file.h/file.c。了解XV6的“文件”有哪些，以及文件，i节点，设备相关的数据结构。了解XV6对文件的基本操作有哪些。XV6最多支持多少个文件? 每个进程最多能打开多少个文件? xv6文件分为管道文件，设备文件和普通文件； XV6最多支持同时打开100个文件，也就是分配100个文件句柄； 单个进程最多能打开16个文件。123// param.h#define NOFILE 16 // open files per process#define NFILE 100 // open files per system 阅读文件sysfile.c。了解与文件系统相关的系统调用，简述各个系统调用的作用。参见源代码阅读部分，已经做出了完整解答。 参考文献[1] xv6中文文档[2] xv6文件系统博客园[3] xv6文件系统CSDN[4] xv6文件系统CSDN[5] 操作系统-文件系统课件]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>XV6</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epoll源码分析(基于linux-5.1.4)]]></title>
    <url>%2F2019%2F06%2F03%2Fepoll-source%2F</url>
    <content type="text"><![CDATA[APIepoll提供给用户进程的接口有如下四个，本文基于linux-5.1.4源码详细分析每个API具体做了啥工作，通过UML时序图理清内核内部的函数调用关系。 int epoll_create1(int size)； 创建一个epfd句柄，size为0时等价于int epoll_create(0)。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； 向epfd上添加/修改/删除fd。 int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout)； 返回所有就绪的fd。 内核数据结构先上一张UML类图，从整体进行把握，图中已经标出各个数据结构所在的文件。 下面贴出各个数据结构代码，切记，实际在过代码的时候，其实我们没有必要对每一个变量和每一行代码咬文嚼字，也不建议这样去做，我们只需要重点关注主要的数据成员和那些关键的代码行，把心思和精力投入到我们最该关注的那部分，从框架层面去把握整体，抓准各个模块的核心，各个模块之间如何耦合，如何同步，如何通信等，这才是能够让你快速进步的最优路线。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145/** Each file descriptor added to the eventpoll interface will* have an entry of this type linked to the "rbr" RB tree.* Avoid increasing the size of this struct, there can be many thousands* of these on a server and we do not want this to take another cache line.*/struct epitem &#123; union &#123; /* RB tree node links this structure to the eventpoll RB tree */ struct rb_node rbn; /* Used to free the struct epitem */ struct rcu_head rcu; &#125;; /* List header used to link this structure to the eventpoll ready list */ struct list_head rdllink; /* * Works together "struct eventpoll"-&gt;ovflist in keeping the * single linked chain of items. */ struct epitem *next; /* The file descriptor information this item refers to */ struct epoll_filefd ffd; /* Number of active wait queue attached to poll operations */ int nwait; /* List containing poll wait queues */ struct list_head pwqlist; /* The "container" of this item */ struct eventpoll *ep; /* List header used to link this item to the "struct file" items list */ struct list_head fllink; /* wakeup_source used when EPOLLWAKEUP is set */ struct wakeup_source __rcu *ws; /* The structure that describe the interested events and the source fd */ struct epoll_event event;&#125;;/** This structure is stored inside the "private_data" member of the file* structure and represents the main data structure for the eventpoll* interface.*/struct eventpoll &#123; /* * This mutex is used to ensure that files are not removed * while epoll is using them. This is held during the event * collection loop, the file cleanup path, the epoll file exit * code and the ctl operations. */ struct mutex mtx; /* Wait queue used by sys_epoll_wait() */ wait_queue_head_t wq; /* Wait queue used by file-&gt;poll() */ wait_queue_head_t poll_wait; /* List of ready file descriptors */ struct list_head rdllist; /* Lock which protects rdllist and ovflist */ rwlock_t lock; /* RB tree root used to store monitored fd structs */ struct rb_root_cached rbr; /* * This is a single linked list that chains all the "struct epitem" that * happened while transferring ready events to userspace w/out * holding -&gt;lock. */ struct epitem *ovflist; /* wakeup_source used when ep_scan_ready_list is running */ struct wakeup_source *ws; /* The user that created the eventpoll descriptor */ struct user_struct *user; struct file *file; /* used to optimize loop detection check */ int visited; struct list_head visited_list_link;#ifdef CONFIG_NET_RX_BUSY_POLL /* used to track busy poll napi_id */ unsigned int napi_id;#endif&#125;;/* eppoll_entry主要完成epitem和epitem事件发生时的callback（ep_poll_callback） * 函数之间的关联，并将上述两个数据结构包装成一个链表节点， * 挂载到目标文件file的waithead中。 * Wait structure used by the poll hooks */struct eppoll_entry &#123; /* List header used to link this structure to the "struct epitem" */ struct list_head llink; /* The "base" pointer is set to the container "struct epitem" */ struct epitem *base; /* * Wait queue item that will be linked to the target file wait * queue head. */ wait_queue_entry_t wait; /* The wait queue head that linked the "wait" wait queue item */ wait_queue_head_t *whead;&#125;;/* ep_pqueue主要完成epitem和callback函数的关联。 * 然后通过目标文件的poll函数调用callback函数ep_ptable_queue_proc。 * Poll函数一般由设备驱动提供，以网络设备为例， * 他的poll函数为sock_poll然后根据sock类型调用不同的poll函数如： * packet_poll。packet_poll在通过datagram_poll调用sock_poll_wait， * 最后在poll_wait实际调用callback函数（ep_ptable_queue_proc） * Wrapper struct used by poll queueing */struct ep_pqueue &#123; poll_table pt; struct epitem *epi;&#125;;/* Used by the ep_send_events() function as callback private data */struct ep_send_events_data &#123; int maxevents; struct epoll_event __user *events; int res;&#125;;struct fd &#123; struct file *file; unsigned int flags;&#125;; 全局调用关系再贴一张各个API从用户进程陷入到内核态并执行系统调用的详细过程，以及client发数据过来时触发ep_poll_callback回调函数的执行流程。 epoll模块初始化&amp;内存池开辟epoll是内核的一个module，内核启动时会初始化这个module。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// fs/eventpoll.cstatic int __init eventpoll_init(void)&#123; struct sysinfo si; si_meminfo(&amp;si); /* * Allows top 4% of lomem to be allocated for epoll watches (per user). */ max_user_watches = (((si.totalram - si.totalhigh) / 25) &lt;&lt; PAGE_SHIFT) / EP_ITEM_COST; BUG_ON(max_user_watches &lt; 0); /* * Initialize the structure used to perform epoll file descriptor * inclusion loops checks. */ ep_nested_calls_init(&amp;poll_loop_ncalls);#ifdef CONFIG_DEBUG_LOCK_ALLOC /* Initialize the structure used to perform safe poll wait head wake ups */ ep_nested_calls_init(&amp;poll_safewake_ncalls);#endif /* * We can have many thousands of epitems, so prevent this from * using an extra cache line on 64-bit (and smaller) CPUs */ BUILD_BUG_ON(sizeof(void *) &lt;= 8 &amp;&amp; sizeof(struct epitem) &gt; 128); // 提前开辟eventpoll_epi内存池,UML时序图的第21步alloc时直接从内存池里取, // 而不是重新调用malloc,效率得以提高 /* Allocates slab cache used to allocate "struct epitem" items */ epi_cache = kmem_cache_create("eventpoll_epi", sizeof(struct epitem),0, placehold_flag, 0); // 提前开辟eventpoll_pwq内存池,UML时序图的第28步alloc时直接从内存池里取 // 而不是重新调用malloc,效率得以提高 /* Allocates slab cache used to allocate "struct eppoll_entry" */ pwq_cache = kmem_cache_create("eventpoll_pwq", sizeof(struct eppoll_entry), 0, SLAB_PANIC|SLAB_ACCOUNT, NULL); return 0;&#125;fs_initcall(eventpoll_init); epoll_create用户空间调用epoll_create(0)或epoll_create1(int)，其实质就是在名为”eventpollfs”的文件系统里创建了一个新文件，同时为该文件申请一个fd，绑定一个inode，最后返回该文件句柄。 epoll_create/epoll_create1陷入内核123456789101112// fs/eventpoll.cSYSCALL_DEFINE1(epoll_create1, int, flags)&#123; return do_epoll_create(flags);&#125;SYSCALL_DEFINE1(epoll_create, int, size)&#123; if (size &lt;= 0) return -EINVAL; return do_epoll_create(0);&#125; do_epoll_create/ep_alloc1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/* * fs/eventpoll.c * Open an eventpoll file descriptor. */static int do_epoll_create(int flags)&#123; int error, fd; struct eventpoll *ep = NULL; struct file *file; /* Check the EPOLL_* constant for consistency. */ BUILD_BUG_ON(EPOLL_CLOEXEC != O_CLOEXEC); if (flags &amp; ~EPOLL_CLOEXEC) return -EINVAL; /* * 申请一个struct eventpoll内存空间,执行初始化后赋给ep * Create the internal data structure ("struct eventpoll"). */ error = ep_alloc(&amp;ep); if (error &lt; 0) return error; /* * 获取一个未使用的fd句柄 * Creates all the items needed to setup an eventpoll file. That is, * a file structure and a free file descriptor. */ fd = get_unused_fd_flags(O_RDWR | (flags &amp; O_CLOEXEC)); if (fd &lt; 0) &#123; error = fd; goto out_free_ep; &#125; file = anon_inode_getfile("[eventpoll]", &amp;eventpoll_fops, ep, O_RDWR | (flags &amp; O_CLOEXEC)); if (IS_ERR(file)) &#123; error = PTR_ERR(file); goto out_free_fd; &#125; ep-&gt;file = file; // 绑定fd和file fd_install(fd, file); // 这个fd就是epfd句柄,返回给用户进程的 return fd;out_free_fd: put_unused_fd(fd);out_free_ep: ep_free(ep); return error;&#125;// fs/eventpoll.c// 形参是一个二级指针,该接口就是简单的分配一个struct eventpoll,然后执行初始化工作static int ep_alloc(struct eventpoll **pep)&#123; int error; struct user_struct *user; struct eventpoll *ep; user = get_current_user(); error = -ENOMEM; ep = kzalloc(sizeof(*ep), GFP_KERNEL); if (unlikely(!ep)) goto free_uid; mutex_init(&amp;ep-&gt;mtx); rwlock_init(&amp;ep-&gt;lock); init_waitqueue_head(&amp;ep-&gt;wq); init_waitqueue_head(&amp;ep-&gt;poll_wait); INIT_LIST_HEAD(&amp;ep-&gt;rdllist); ep-&gt;rbr = RB_ROOT_CACHED; ep-&gt;ovflist = EP_UNACTIVE_PTR; ep-&gt;user = user; *pep = ep; return 0;free_uid: free_uid(user); return error;&#125; anon_inode_getfile/alloc_file_pseudo/alloc_file123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121/** * fs/anon_inodes.c * anon_inode_getfile - creates a new file instance by hooking it up to an * anonymous inode, and a dentry that describe the "class" * of the file * * @name: [in] name of the "class" of the new file * @fops: [in] file operations for the new file * @priv: [in] private data for the new file (will be file's private_data) * @flags: [in] flags * * Creates a new file by hooking it on a single inode. This is useful for files * that do not need to have a full-fledged inode in order to operate correctly. * All the files created with anon_inode_getfile() will share a single inode, * hence saving memory and avoiding code duplication for the file/inode/dentry * setup. Returns the newly created file* or an error pointer. * 在一个inode上挂接一个新文件,这对于不需要完整inode才能正确操作的文件非常有用。 * 使用anon_inode_getfile()创建的所有文件都将共享一个inode， * 因此可以节省内存并避免文件/inode/dentry设置的代码重复。 * 返回新创建的文件*或错误指针。 */struct file *anon_inode_getfile(const char *name,const struct file_operations *fops,void *priv, int flags)&#123; struct file *file; if (IS_ERR(anon_inode_inode)) return ERR_PTR(-ENODEV); if (fops-&gt;owner &amp;&amp; !try_module_get(fops-&gt;owner)) return ERR_PTR(-ENOENT); /* * We know the anon_inode inode count is always greater than zero, * so ihold() is safe. */ ihold(anon_inode_inode); // 创建一个名字为“[eventpoll]”的eventpollfs文件描述符 file = alloc_file_pseudo(anon_inode_inode, anon_inode_mnt, name, flags &amp; (O_ACCMODE | O_NONBLOCK), fops); if (IS_ERR(file)) goto err; file-&gt;f_mapping = anon_inode_inode-&gt;i_mapping; // file-&gt;private_data指向传进来的priv( = struct eventpoll *ep) file-&gt;private_data = priv; return file;err: iput(anon_inode_inode); module_put(fops-&gt;owner); return file;&#125;EXPORT_SYMBOL_GPL(anon_inode_getfile);// fs/file_table.cstruct file *alloc_file_pseudo(struct inode *inode, struct vfsmount *mnt, const char *name, int flags,const struct file_operations *fops)&#123; static const struct dentry_operations anon_ops = &#123; .d_dname = simple_dname &#125;; struct qstr this = QSTR_INIT(name, strlen(name)); struct path path; struct file *file; // 挂载名为“[eventpoll]”的eventpollfs文件系统 path.dentry = d_alloc_pseudo(mnt-&gt;mnt_sb, &amp;this); if (!path.dentry) return ERR_PTR(-ENOMEM); if (!mnt-&gt;mnt_sb-&gt;s_d_op) d_set_d_op(path.dentry, &amp;anon_ops); path.mnt = mntget(mnt); d_instantiate(path.dentry, inode); // inode和file绑定，返回绑定后的file结构 file = alloc_file(&amp;path, flags, fops); if (IS_ERR(file)) &#123; ihold(inode); path_put(&amp;path); &#125; return file;&#125;EXPORT_SYMBOL(alloc_file_pseudo);/** * fs/file_table.c * alloc_file - allocate and initialize a 'struct file' * * @path: the (dentry, vfsmount) pair for the new file * @flags: O_... flags with which the new file will be opened * @fop: the 'struct file_operations' for the new file */static struct file *alloc_file(const struct path *path, int flags, const struct file_operations *fop)&#123; struct file *file; // 申请一个空的file结构 file = alloc_empty_file(flags, current_cred()); if (IS_ERR(file)) return file; file-&gt;f_path = *path; file-&gt;f_inode = path-&gt;dentry-&gt;d_inode; file-&gt;f_mapping = path-&gt;dentry-&gt;d_inode-&gt;i_mapping; file-&gt;f_wb_err = filemap_sample_wb_err(file-&gt;f_mapping); if ((file-&gt;f_mode &amp; FMODE_READ) &amp;&amp; likely(fop-&gt;read || fop-&gt;read_iter)) file-&gt;f_mode |= FMODE_CAN_READ; if ((file-&gt;f_mode &amp; FMODE_WRITE) &amp;&amp; likely(fop-&gt;write || fop-&gt;write_iter)) file-&gt;f_mode |= FMODE_CAN_WRITE; file-&gt;f_mode |= FMODE_OPENED; file-&gt;f_op = fop; if ((file-&gt;f_mode &amp; (FMODE_READ | FMODE_WRITE)) == FMODE_READ) i_readcount_inc(path-&gt;dentry-&gt;d_inode); return file;&#125; epoll_ctl用户进程调用int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)，op可填EPOLL_CTL_ADD(注册fd到epfd)、EPOLL_CTL_MOD(修改已注册fd监听的事件)和EPOLL_CTL_DEL(从epfd中删除fd)。 epoll_ctl陷入内核123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171/* * fs/eventpoll.c * The following function implements the controller interface for * the eventpoll file that enables the insertion/removal/change of * file descriptors inside the interest set. */SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event)&#123; int error; int full_check = 0; struct fd f, tf; struct eventpoll *ep; struct epitem *epi; struct epoll_event epds; struct eventpoll *tep = NULL; error = -EFAULT; // copy_from_user将用户空间关注的event事件拷贝到内核空间 if (ep_op_has_event(op) &amp;&amp; copy_from_user(&amp;epds, event, sizeof(struct epoll_event))) goto error_return; error = -EBADF; f = fdget(epfd); if (!f.file) goto error_return; /* Get the "struct file *" for the target file */ tf = fdget(fd); if (!tf.file) goto error_fput; /* The target file descriptor must support poll */ error = -EPERM; if (!file_can_poll(tf.file)) goto error_tgt_fput; /* 如果系统设置了自动休眠模式（通过/sys/power/autosleep）， * 当唤醒设备的事件发生时，设备驱动会保持唤醒状态，直到事件进入排队状态。 * 为了保持设备唤醒直到事件处理完成，必须使用epoll EPOLLWAKEUP 标记。 * 一旦给structe poll_event中的events字段设置了EPOLLWAKEUP标记，系统会在事件排队时就保持唤醒， * 从epoll_wait调用开始，持续要下一次epoll_wait调用。 */ /* Check if EPOLLWAKEUP is allowed */ if (ep_op_has_event(op)) ep_take_care_of_epollwakeup(&amp;epds); /* * We have to check that the file structure underneath the file descriptor * the user passed to us _is_ an eventpoll file. And also we do not permit * adding an epoll file descriptor inside itself. */ error = -EINVAL; if (f.file == tf.file || !is_file_epoll(f.file)) goto error_tgt_fput; /* * epoll adds to the wakeup queue at EPOLL_CTL_ADD time only, * so EPOLLEXCLUSIVE is not allowed for a EPOLL_CTL_MOD operation. * Also, we do not currently supported nested exclusive wakeups. */ if (ep_op_has_event(op) &amp;&amp; (epds.events &amp; EPOLLEXCLUSIVE)) &#123; if (op == EPOLL_CTL_MOD) goto error_tgt_fput; if (op == EPOLL_CTL_ADD &amp;&amp; (is_file_epoll(tf.file) || (epds.events &amp; ~EPOLLEXCLUSIVE_OK_BITS))) goto error_tgt_fput; &#125; /* * At this point it is safe to assume that the "private_data" contains * our own data structure. */ ep = f.file-&gt;private_data; /* * When we insert an epoll file descriptor, inside another epoll file * descriptor, there is the change of creating closed loops, which are * better be handled here, than in more critical paths. While we are * checking for loops we also determine the list of files reachable * and hang them on the tfile_check_list, so we can check that we * haven't created too many possible wakeup paths. * * We do not need to take the global 'epumutex' on EPOLL_CTL_ADD when * the epoll file descriptor is attaching directly to a wakeup source, * unless the epoll file descriptor is nested. The purpose of taking the * 'epmutex' on add is to prevent complex toplogies such as loops and * deep wakeup paths from forming in parallel through multiple * EPOLL_CTL_ADD operations. */ mutex_lock_nested(&amp;ep-&gt;mtx, 0); if (op == EPOLL_CTL_ADD) &#123; if (!list_empty(&amp;f.file-&gt;f_ep_links) || is_file_epoll(tf.file)) &#123; full_check = 1; mutex_unlock(&amp;ep-&gt;mtx); mutex_lock(&amp;epmutex); if (is_file_epoll(tf.file)) &#123; error = -ELOOP; if (ep_loop_check(ep, tf.file) != 0) &#123; clear_tfile_check_list(); goto error_tgt_fput; &#125; &#125; else list_add(&amp;tf.file-&gt;f_tfile_llink, &amp;tfile_check_list); mutex_lock_nested(&amp;ep-&gt;mtx, 0); if (is_file_epoll(tf.file)) &#123; tep = tf.file-&gt;private_data; mutex_lock_nested(&amp;tep-&gt;mtx, 1); &#125; &#125; &#125; /* * Try to lookup the file inside our RB tree, Since we grabbed "mtx" * above, we can be sure to be able to use the item looked up by * ep_find() till we release the mutex. * 从红黑树中寻找添加的fd是否存在，存在则返回到ep中，否则返回NULL */ epi = ep_find(ep, tf.file, fd); error = -EINVAL; switch (op) &#123; case EPOLL_CTL_ADD: // 若ep为空说明红黑树中不存在,执行ep_insert添加到红黑树中 if (!epi) &#123; epds.events |= EPOLLERR | EPOLLHUP; // 如果不存在则添加，已经存在不重复添加 error = ep_insert(ep, &amp;epds, tf.file, fd, full_check); &#125; else error = -EEXIST; if (full_check) clear_tfile_check_list(); break; // 删除fd调用ep_remove case EPOLL_CTL_DEL: if (epi) error = ep_remove(ep, epi); else error = -ENOENT; break; // 修改已注册fd所监听的事件,调用ep_modify case EPOLL_CTL_MOD: if (epi) &#123; if (!(epi-&gt;event.events &amp; EPOLLEXCLUSIVE)) &#123; epds.events |= EPOLLERR | EPOLLHUP; error = ep_modify(ep, epi, &amp;epds); &#125; &#125; else error = -ENOENT; break; &#125; if (tep != NULL) mutex_unlock(&amp;tep-&gt;mtx); mutex_unlock(&amp;ep-&gt;mtx);error_tgt_fput: if (full_check) mutex_unlock(&amp;epmutex); fdput(tf);error_fput: fdput(f);error_return: return error;&#125; ep_find12345678910111213141516171819202122232425262728293031/* * fs/eventpoll.c * Search the file inside the eventpoll tree. The RB tree operations * are protected by the "mtx" mutex, and ep_find() must be called with * "mtx" held. */static struct epitem *ep_find(struct eventpoll *ep, struct file *file, int fd)&#123; int kcmp; struct rb_node *rbp; struct epitem *epi, *epir = NULL; struct epoll_filefd ffd; ep_set_ffd(&amp;ffd, file, fd); // 从红黑树根节开始二分查找,判断左右子树 for (rbp = ep-&gt;rbr.rb_root.rb_node; rbp; ) &#123; epi = rb_entry(rbp, struct epitem, rbn); kcmp = ep_cmp_ffd(&amp;ffd, &amp;epi-&gt;ffd); if (kcmp &gt; 0) rbp = rbp-&gt;rb_right; else if (kcmp &lt; 0) rbp = rbp-&gt;rb_left; else &#123; epir = epi; break; &#125; &#125; return epir;&#125; ep_insert123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151/* * fs/eventpoll.c * Must be called with "mtx" held. */static int ep_insert(struct eventpoll *ep, const struct epoll_event *event, struct file *tfile, int fd, int full_check)&#123; int error, pwake = 0; __poll_t revents; long user_watches; struct epitem *epi; struct ep_pqueue epq; lockdep_assert_irqs_enabled(); user_watches = atomic_long_read(&amp;ep-&gt;user-&gt;epoll_watches); if (unlikely(user_watches &gt;= max_user_watches)) return -ENOSPC; // epi_cache内存池在epoll模块初始化时已经分配,这里根据slab直接取一个epitem if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL))) return -ENOMEM; // 初始化 epitem /* Item initialization follow here ... */ INIT_LIST_HEAD(&amp;epi-&gt;rdllink); INIT_LIST_HEAD(&amp;epi-&gt;fllink); INIT_LIST_HEAD(&amp;epi-&gt;pwqlist); epi-&gt;ep = ep; ep_set_ffd(&amp;epi-&gt;ffd, tfile, fd); epi-&gt;event = *event; epi-&gt;nwait = 0; epi-&gt;next = EP_UNACTIVE_PTR; if (epi-&gt;event.events &amp; EPOLLWAKEUP) &#123; error = ep_create_wakeup_source(epi); if (error) goto error_create_wakeup_source; &#125; else &#123; RCU_INIT_POINTER(epi-&gt;ws, NULL); &#125; // 创建一个struct ep_pqueue epq, 并与epitem(epi)关联 /* Initialize the poll table using the queue callback */ epq.epi = epi; /* 设置epq的回调函数为ep_ptable_queue_proc,当调用poll_wait时会调用该回调函数， * 而函数体ep_ptable_queue_proc内部所做的主要工作, * 就是把epitem对应fd的事件到来时的回调函数设置为ep_poll_callback。 * ep_poll_callback所做的主要工作就是把就绪的fd放到就绪链表rdllist上, * 然后唤醒epoll_wait的调用者, 被唤醒的进程再把rdllist上就绪的fd的events拷贝给用户进程, * 完成一个闭环。 */ init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc); /* * Attach the item to the poll hooks and get current event bits. * We can safely use the file* here because its usage count has * been increased by the caller of this function. Note that after * this operation completes, the poll callback can start hitting * the new item. * 判断当前插入的event是否刚好发生，返回就绪事件的掩码赋给revents, * 如果发生，那么做一个ready动作， * 后面的if语句将epitem加入到rdlist中，并对epoll上的wait队列调用wakeup */ revents = ep_item_poll(epi, &amp;epq.pt, 1); /* * We have to check if something went wrong during the poll wait queue * install process. Namely an allocation for a wait queue failed due * high memory pressure. */ error = -ENOMEM; if (epi-&gt;nwait &lt; 0) goto error_unregister; /* Add the current item to the list of active epoll hook for this file */ spin_lock(&amp;tfile-&gt;f_lock); // 每个文件会将所有监听自己的epitem链起来 list_add_tail_rcu(&amp;epi-&gt;fllink, &amp;tfile-&gt;f_ep_links); spin_unlock(&amp;tfile-&gt;f_lock); /* * Add the current item to the RB tree. All RB tree operations are * protected by "mtx", and ep_insert() is called with "mtx" held. * 将epitem插入到对应的eventpoll红黑树中去,红黑树用一个互斥锁进行保护 */ ep_rbtree_insert(ep, epi); /* now check if we've created too many backpaths */ error = -EINVAL; if (full_check &amp;&amp; reverse_path_check()) goto error_remove_epi; /* We have to drop the new item inside our item list to keep track of it */ write_lock_irq(&amp;ep-&gt;lock); /* record NAPI ID of new item if present */ ep_set_busy_poll_napi_id(epi); /* If the file is already "ready" we drop it inside the ready list */ if (revents &amp;&amp; !ep_is_linked(epi)) &#123; list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); /* Notify waiting tasks that events are available */ if (waitqueue_active(&amp;ep-&gt;wq)) wake_up(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125; write_unlock_irq(&amp;ep-&gt;lock); atomic_long_inc(&amp;ep-&gt;user-&gt;epoll_watches); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); return 0;error_remove_epi: spin_lock(&amp;tfile-&gt;f_lock); list_del_rcu(&amp;epi-&gt;fllink); spin_unlock(&amp;tfile-&gt;f_lock); rb_erase_cached(&amp;epi-&gt;rbn, &amp;ep-&gt;rbr);error_unregister: ep_unregister_pollwait(ep, epi); /* * We need to do this because an event could have been arrived on some * allocated wait queue. Note that we don't care about the ep-&gt;ovflist * list, since that is used/cleaned only inside a section bound by "mtx". * And ep_insert() is called with "mtx" held. */ write_lock_irq(&amp;ep-&gt;lock); if (ep_is_linked(epi)) list_del_init(&amp;epi-&gt;rdllink); write_unlock_irq(&amp;ep-&gt;lock); wakeup_source_unregister(ep_wakeup_source(epi));error_create_wakeup_source: kmem_cache_free(epi_cache, epi); return error;&#125; kmem_cache_alloc1234567891011121314151617181920212223/** * slab算法从内存池cachep中分配一个实例返回 * mm/slab.c * kmem_cache_alloc - Allocate an object * @cachep: The cache to allocate from. * @flags: See kmalloc(). * * Allocate an object from this cache. The flags are only relevant * if the cache has no available objects. * * Return: pointer to the new object or %NULL in case of error */void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)&#123; void *ret = slab_alloc(cachep, flags, _RET_IP_); trace_kmem_cache_alloc(_RET_IP_, ret, cachep-&gt;object_size, cachep-&gt;size, flags); return ret;&#125;EXPORT_SYMBOL(kmem_cache_alloc); init_poll_funcptr/ep_ptable_queue_proc/ep_poll_callback/init_waitqueue_func_entryinit_poll_funcptr：设置epq的回调函数为ep_ptable_queue_proc，当调用poll_wait时会调用该回调函数；ep_ptable_queue_proc：该函数内部所做的主要工作，就是把epitem对应fd的事件到来时的回调函数设置为ep_poll_callback。ep_poll_callback：主要工作就是把就绪的fd放到就绪链表rdllist上，然后唤醒epoll_wait的调用者，被唤醒的进程再把rdllist上就绪的fd的events拷贝给用户进程，完成一个闭环。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183/* * 设置回调 * include/linux/poll.h */static inline void init_poll_funcptr(poll_table *pt, poll_queue_proc qproc)&#123; pt-&gt;_qproc = qproc; pt-&gt;_key = ~(__poll_t)0; /* all events enabled */&#125;/* * This is the callback that is used to add our wait queue to the * target file wakeup lists. * struct file *file（目标文件）= epi-&gt;ffd.file, * wait_queue_head_t *whead（目标文件的waitlist）= eventpoll-&gt;poll_wait, * poll_table *pt（前面生成的poll_table） */static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,poll_table *pt)&#123; struct epitem *epi = ep_item_from_epqueue(pt); // 创建一个struct eppoll_entry,与对应的epitem关联上 struct eppoll_entry *pwq; // 从pwq_cache内存池中取一个struct eppoll_entry if (epi-&gt;nwait &gt;= 0 &amp;&amp; (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) &#123; // 把每个epitem对应的回调函数设置为ep_poll_callback, // 当epitem关注的事件中断到来时会执行回调函数ep_poll_callback init_waitqueue_func_entry(&amp;pwq-&gt;wait, ep_poll_callback); pwq-&gt;whead = whead; // 关联上epitem pwq-&gt;base = epi; // 通过add_wait_queue将epoll_entry挂载到目标文件的waitlist。 // 完成这个动作后，epoll_entry已经被挂载到waitlist if (epi-&gt;event.events &amp; EPOLLEXCLUSIVE) add_wait_queue_exclusive(whead, &amp;pwq-&gt;wait); else add_wait_queue(whead, &amp;pwq-&gt;wait); // eppoll_entry-&gt;llink执行epitem-&gt;pwqlist list_add_tail(&amp;pwq-&gt;llink, &amp;epi-&gt;pwqlist); epi-&gt;nwait++; &#125; else &#123; /* We have to signal that an error occurred */ epi-&gt;nwait = -1; &#125;&#125;// include/linux/wait.hstatic inline void init_waitqueue_func_entry(struct wait_queue_entry *wq_entry, wait_queue_func_t func)&#123; wq_entry-&gt;flags = 0; wq_entry-&gt;private = NULL; wq_entry-&gt;func = func;&#125;/* * fs/eventpoll.c * This is the callback that is passed to the wait queue wakeup * mechanism. It is called by the stored file descriptors when they * have events to report. * * This callback takes a read lock in order not to content with concurrent * events from another file descriptors, thus all modifications to -&gt;rdllist * or -&gt;ovflist are lockless. Read lock is paired with the write lock from * ep_scan_ready_list(), which stops all list modifications and guarantees * that lists state is seen correctly. * * Another thing worth to mention is that ep_poll_callback() can be called * concurrently for the same @epi from different CPUs if poll table was inited * with several wait queues entries. Plural wakeup from different CPUs of a * single wait queue is serialized by wq.lock, but the case when multiple wait * queues are used should be detected accordingly. This is detected using * cmpxchg() operation. */static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)&#123; int pwake = 0; struct epitem *epi = ep_item_from_wait(wait); struct eventpoll *ep = epi-&gt;ep; __poll_t pollflags = key_to_poll(key); unsigned long flags; int ewake = 0; read_lock_irqsave(&amp;ep-&gt;lock, flags); ep_set_busy_poll_napi_id(epi); /* * If the event mask does not contain any poll(2) event, we consider the * descriptor to be disabled. This condition is likely the effect of the * EPOLLONESHOT bit that disables the descriptor when an event is received, * until the next EPOLL_CTL_MOD will be issued. */ if (!(epi-&gt;event.events &amp; ~EP_PRIVATE_BITS)) goto out_unlock; /* * Check the events coming with the callback. At this stage, not * every device reports the events in the "key" parameter of the * callback. We need to be able to handle both cases here, hence the * test for "key" != NULL before the event match test. */ if (pollflags &amp;&amp; !(pollflags &amp; epi-&gt;event.events)) goto out_unlock; /* * If we are transferring events to userspace, we can hold no locks * (because we're accessing user memory, and because of linux f_op-&gt;poll() * semantics). All the events that happen during that period of time are * chained in ep-&gt;ovflist and requeued later on. */ if (READ_ONCE(ep-&gt;ovflist) != EP_UNACTIVE_PTR) &#123; // epi-&gt;next == EP_UNACTIVE_PTR说明rdllist当前被其他进程持有, // 因此调用chain_epi_lockless把epitem放入vovflist上 if (epi-&gt;next == EP_UNACTIVE_PTR &amp;&amp; chain_epi_lockless(epi)) ep_pm_stay_awake_rcu(epi); goto out_unlock; &#125; // rdllist抢占成功,调用list_add_tail_lockless把epitem挂入rdllist上 /* If this file is already in the ready list we exit soon */ if (!ep_is_linked(epi) &amp;&amp; list_add_tail_lockless(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist)) &#123; ep_pm_stay_awake_rcu(epi); &#125; /* * Wake up ( if active ) both the eventpoll wait list and the -&gt;poll() * wait list. */ if (waitqueue_active(&amp;ep-&gt;wq)) &#123; if ((epi-&gt;event.events &amp; EPOLLEXCLUSIVE) &amp;&amp; !(pollflags &amp; POLLFREE)) &#123; switch (pollflags &amp; EPOLLINOUT_BITS) &#123; case EPOLLIN: if (epi-&gt;event.events &amp; EPOLLIN) ewake = 1; break; case EPOLLOUT: if (epi-&gt;event.events &amp; EPOLLOUT) ewake = 1; break; case 0: ewake = 1; break; &#125; &#125; // 同时唤醒eventpoll的wq等待队列,也就是唤醒poll_wait的调用者 wake_up(&amp;ep-&gt;wq); &#125; if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; out_unlock: read_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); if (!(epi-&gt;event.events &amp; EPOLLEXCLUSIVE)) ewake = 1; if (pollflags &amp; POLLFREE) &#123; /* * If we race with ep_remove_wait_queue() it can miss * -&gt;whead = NULL and do another remove_wait_queue() after * us, so we can't use __remove_wait_queue(). */ list_del_init(&amp;wait-&gt;entry); /* * -&gt;whead != NULL protects us from the race with ep_free() * or ep_remove(), ep_remove_wait_queue() takes whead-&gt;lock * held by the caller. Once we nullify it, nothing protects * ep/epi or even wait. */ smp_store_release(&amp;ep_pwq_from_wait(wait)-&gt;whead, NULL); &#125; return ewake;&#125; ep_item_poll/poll_wait/ep_scan_ready_list123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152/* * Differs from ep_eventpoll_poll() in that internal callers already have * the ep-&gt;mtx so we need to start from depth=1, such that mutex_lock_nested() * is correctly annotated. */static __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt,int depth)&#123; struct eventpoll *ep; bool locked; pt-&gt;_key = epi-&gt;event.events; if (!is_file_epoll(epi-&gt;ffd.file)) return vfs_poll(epi-&gt;ffd.file, pt) &amp; epi-&gt;event.events; // 拿到eventpoll,回头过去看UML数据结构,private_data是指向eventpoll的 ep = epi-&gt;ffd.file-&gt;private_data; // 这里面会执行前面设置的ep_ptable_queue_proc回调体 // ep_ptable_queue_proc函数体的工作在前面已经介绍过 poll_wait(epi-&gt;ffd.file, &amp;ep-&gt;poll_wait, pt); locked = pt &amp;&amp; (pt-&gt;_qproc == ep_ptable_queue_proc); // 把就绪链表rdllist拷贝到用户空间 return ep_scan_ready_list(epi-&gt;ffd.file-&gt;private_data, ep_read_events_proc, &amp;depth, depth, locked) &amp; epi-&gt;event.events;&#125;// include/linux/poll.hstatic inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)&#123; if (p &amp;&amp; p-&gt;_qproc &amp;&amp; wait_address) p-&gt;_qproc(filp, wait_address, p);&#125;/** * ep_scan_ready_list - Scans the ready list in a way that makes possible for * the scan code, to call f_op-&gt;poll(). Also allows for * O(NumReady) performance. * * @ep: Pointer to the epoll private data structure. * @sproc: Pointer to the scan callback. * @priv: Private opaque data passed to the @sproc callback. * @depth: The current depth of recursive f_op-&gt;poll calls. * @ep_locked: caller already holds ep-&gt;mtx * * Returns: The same integer error code returned by the @sproc callback. */static __poll_t ep_scan_ready_list(struct eventpoll *ep, __poll_t (*sproc)(struct eventpoll *, struct list_head *, void *),void *priv, int depth, bool ep_locked)&#123; __poll_t res; int pwake = 0; struct epitem *epi, *nepi; LIST_HEAD(txlist); lockdep_assert_irqs_enabled(); /* * We need to lock this because we could be hit by * eventpoll_release_file() and epoll_ctl(). */ if (!ep_locked) mutex_lock_nested(&amp;ep-&gt;mtx, depth); /* * Steal the ready list, and re-init the original one to the * empty list. Also, set ep-&gt;ovflist to NULL so that events * happening while looping w/out locks, are not lost. We cannot * have the poll callback to queue directly on ep-&gt;rdllist, * because we want the "sproc" callback to be able to do it * in a lockless way. */ write_lock_irq(&amp;ep-&gt;lock); // 把就绪链表rdllist赋给临时的txlist,执行该操作后rdllist会被清空, // 因为rdllist需要腾出来给其他进程继续往上放内容, // 从而把txlist内epitem对应fd的就绪events复制到用户空间 list_splice_init(&amp;ep-&gt;rdllist, &amp;txlist); WRITE_ONCE(ep-&gt;ovflist, NULL); write_unlock_irq(&amp;ep-&gt;lock); /* * sproc就是前面设置好的ep_poll_callback,事件到来了执行该回调体, * sproc会把就绪的epitem放入rdllist或ovflist上 * Now call the callback function. */ res = (*sproc)(ep, &amp;txlist, priv); write_lock_irq(&amp;ep-&gt;lock); /* * During the time we spent inside the "sproc" callback, some * other events might have been queued by the poll callback. * We re-insert them inside the main ready-list here. */ for (nepi = READ_ONCE(ep-&gt;ovflist); (epi = nepi) != NULL; nepi = epi-&gt;next, epi-&gt;next = EP_UNACTIVE_PTR) &#123; /* * We need to check if the item is already in the list. * During the "sproc" callback execution time, items are * queued into -&gt;ovflist but the "txlist" might already * contain them, and the list_splice() below takes care of them. */ if (!ep_is_linked(epi)) &#123; /* * -&gt;ovflist is LIFO, so we have to reverse it in order * to keep in FIFO. */ list_add(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); &#125; &#125; /* * We need to set back ep-&gt;ovflist to EP_UNACTIVE_PTR, so that after * releasing the lock, events will be queued in the normal way inside * ep-&gt;rdllist. */ WRITE_ONCE(ep-&gt;ovflist, EP_UNACTIVE_PTR); /* * 把水平触发EPOLLLT属性的epitem依旧挂回到rdllist, * 因为我们希望即使没有新的数据到来,只要数据还没被用户空间读完,就继续上报 * Quickly re-inject items left on "txlist". */ list_splice(&amp;txlist, &amp;ep-&gt;rdllist); __pm_relax(ep-&gt;ws); if (!list_empty(&amp;ep-&gt;rdllist)) &#123; /* * Wake up (if active) both the eventpoll wait list and * the -&gt;poll() wait list (delayed after we release the lock). * wake_up唤醒epoll_wait的调用者 */ if (waitqueue_active(&amp;ep-&gt;wq)) wake_up(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125; write_unlock_irq(&amp;ep-&gt;lock); if (!ep_locked) mutex_unlock(&amp;ep-&gt;mtx); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); return res;&#125; 到此，epoll_ctl的分析就已经完了，这里只描述的EPOLL_CTL_ADD调用。EPOLL_CTL_MOD/EPOLL_CTL_DEL相对就简单很多，这三个操作差异主要体现在fs/eventpoll.c文件内接口SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd,struct epoll_event __user*, event)的switch语句部分，EPOLL_CTL_MOD和EPOLL_CTL_DEL分别对应ep_modify和ep_remove，这两个函数就是从红黑树中去找到对应的节点进行修改和删除操作，因此这里没有贴代码。 epoll_waitepoll_wait陷入内核123456// fs/eventpoll.cSYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events,int, maxevents, int, timeout)&#123; return do_epoll_wait(epfd, events, maxevents, timeout);&#125; do_epoll_wait/ep_poll/ep_send_events/ep_send_events_proc123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296/* * Implement the event wait interface for the eventpoll file. It is the kernel * part of the user space epoll_wait(2). */static int do_epoll_wait(int epfd, struct epoll_event __user *events, int maxevents, int timeout)&#123; int error; // struct fd结构在数据结构部分代码已经列出 struct fd f; struct eventpoll *ep; /* The maximum number of event must be greater than zero */ if (maxevents &lt;= 0 || maxevents &gt; EP_MAX_EVENTS) return -EINVAL; /* Verify that the area passed by the user is writeable */ if (!access_ok(events, maxevents * sizeof(struct epoll_event))) return -EFAULT; /* Get the "struct file *" for the eventpoll file */ f = fdget(epfd); if (!f.file) return -EBADF; /* * We have to check that the file structure underneath the fd * the user passed to us _is_ an eventpoll file. */ error = -EINVAL; if (!is_file_epoll(f.file)) goto error_fput; /* * At this point it is safe to assume that the "private_data" contains * our own data structure. * 直接拿到eventpoll对象 */ ep = f.file-&gt;private_data; // ep_poll时主循环体,当rdllist为空时调用者根据设置的超时参数, // 决定是等待还是返回 /* Time to fish for events ... */ error = ep_poll(ep, events, maxevents, timeout);error_fput: fdput(f); return error;&#125;/** * ep_poll - Retrieves ready events, and delivers them to the caller supplied * event buffer. * * @ep: Pointer to the eventpoll context. * @events: Pointer to the userspace buffer where the ready events should be * stored. * @maxevents: Size (in terms of number of events) of the caller event buffer. * @timeout: Maximum timeout for the ready events fetch operation, in * milliseconds. If the @timeout is zero, the function will not block, * while if the @timeout is less than zero, the function will block * until at least one event has been retrieved (or an error * occurred). * * Returns: Returns the number of ready events which have been fetched, or an * error code, in case of error. */static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout)&#123; int res = 0, eavail, timed_out = 0; u64 slack = 0; bool waiter = false; wait_queue_entry_t wait; ktime_t expires, *to = NULL; lockdep_assert_irqs_enabled(); // 超时设置 if (timeout &gt; 0) &#123; struct timespec64 end_time = ep_set_mstimeout(timeout); slack = select_estimate_accuracy(&amp;end_time); to = &amp;expires; *to = timespec64_to_ktime(end_time); &#125; else if (timeout == 0) &#123; // 立即返回 /* * Avoid the unnecessary trip to the wait queue loop, if the * caller specified a non blocking operation. We still need * lock because we could race and not see an epi being added * to the ready list while in irq callback. Thus incorrectly * returning 0 back to userspace. */ timed_out = 1; write_lock_irq(&amp;ep-&gt;lock); eavail = ep_events_available(ep); write_unlock_irq(&amp;ep-&gt;lock); goto send_events; &#125;// 否则是永久等待,直到有新的事件到来fetch_events: if (!ep_events_available(ep)) ep_busy_loop(ep, timed_out); eavail = ep_events_available(ep); if (eavail) goto send_events; /* * Busy poll timed out. Drop NAPI ID for now, we can add * it back in when we have moved a socket with a valid NAPI * ID onto the ready list. */ ep_reset_busy_poll_napi_id(ep); /* * We don't have any available event to return to the caller. We need * to sleep here, and we will be woken by ep_poll_callback() when events * become available. */ if (!waiter) &#123; waiter = true; // ep-&gt;rdllist存放的是已就绪(read)的fd，为空时说明当前没有就绪的fd, // 创建一个等待队列,并使用当前进程（current）初始化 init_waitqueue_entry(&amp;wait, current); spin_lock_irq(&amp;ep-&gt;wq.lock); // 将当前进程添加到等待队列 __add_wait_queue_exclusive(&amp;ep-&gt;wq, &amp;wait); spin_unlock_irq(&amp;ep-&gt;wq.lock); &#125; for (;;) &#123; /* * We don't want to sleep if the ep_poll_callback() sends us * a wakeup in between. That's why we set the task state * to TASK_INTERRUPTIBLE before doing the checks. */ set_current_state(TASK_INTERRUPTIBLE); /* * Always short-circuit for fatal signals to allow * threads to make a timely exit without the chance of * finding more events available and fetching * repeatedly. */ if (fatal_signal_pending(current)) &#123; res = -EINTR; break; &#125; // ep_events_available内部会判断rdllist是否为空 eavail = ep_events_available(ep); if (eavail) break; // 循环体,如果rdllist不为空,则跳出循环体,进入send_events if (signal_pending(current)) &#123; res = -EINTR; break; &#125; if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) &#123; timed_out = 1; break; &#125; &#125; __set_current_state(TASK_RUNNING);send_events: /* * Try to transfer events to user space. In case we get 0 events and * there's still timeout left over, we go trying again in search of * more luck. * ep_send_events接口复制txlist内epitem对应fd的就绪events到用户空间 */ if (!res &amp;&amp; eavail &amp;&amp; !(res = ep_send_events(ep, events, maxevents)) &amp;&amp; !timed_out) goto fetch_events; if (waiter) &#123; spin_lock_irq(&amp;ep-&gt;wq.lock); // 将当前进程移出等待队列 __remove_wait_queue(&amp;ep-&gt;wq, &amp;wait); spin_unlock_irq(&amp;ep-&gt;wq.lock); &#125; return res;&#125;fs/eventpoll.cstatic int ep_send_events(struct eventpoll *ep, struct epoll_event __user *events, int maxevents)&#123; struct ep_send_events_data esed; esed.maxevents = maxevents; esed.events = events; // 传入ep_send_events_proc ep_scan_ready_list(ep, ep_send_events_proc, &amp;esed, 0, false); return esed.res;&#125;// 实际执行复制到用户空间的工作是由该函数体负责static __poll_t ep_send_events_proc(struct eventpoll *ep, struct list_head *head,void *priv)&#123; struct ep_send_events_data *esed = priv; __poll_t revents; struct epitem *epi, *tmp; struct epoll_event __user *uevent = esed-&gt;events; struct wakeup_source *ws; poll_table pt; init_poll_funcptr(&amp;pt, NULL); esed-&gt;res = 0; /* * We can loop without lock because we are passed a task private list. * Items cannot vanish during the loop because ep_scan_ready_list() is * holding "mtx" during this call. */ lockdep_assert_held(&amp;ep-&gt;mtx); // lambda表达式 list_for_each_entry_safe(epi, tmp, head, rdllink) &#123; if (esed-&gt;res &gt;= esed-&gt;maxevents) break; /* * Activate ep-&gt;ws before deactivating epi-&gt;ws to prevent * triggering auto-suspend here (in case we reactive epi-&gt;ws * below). * * This could be rearranged to delay the deactivation of epi-&gt;ws * instead, but then epi-&gt;ws would temporarily be out of sync * with ep_is_linked(). */ ws = ep_wakeup_source(epi); if (ws) &#123; if (ws-&gt;active) __pm_stay_awake(ep-&gt;ws); __pm_relax(ws); &#125; list_del_init(&amp;epi-&gt;rdllink); /* * If the event mask intersect the caller-requested one, * deliver the event to userspace. Again, ep_scan_ready_list() * is holding ep-&gt;mtx, so no operations coming from userspace * can change the item. */ revents = ep_item_poll(epi, &amp;pt, 1); if (!revents) continue; // 复制到用户空间 if (__put_user(revents, &amp;uevent-&gt;events) || __put_user(epi-&gt;event.data, &amp;uevent-&gt;data)) &#123; list_add(&amp;epi-&gt;rdllink, head); ep_pm_stay_awake(epi); if (!esed-&gt;res) esed-&gt;res = -EFAULT; return 0; &#125; esed-&gt;res++; uevent++; if (epi-&gt;event.events &amp; EPOLLONESHOT) epi-&gt;event.events &amp;= EP_PRIVATE_BITS; else if (!(epi-&gt;event.events &amp; EPOLLET)) &#123; /* * If this file has been added with Level * Trigger mode, we need to insert back inside * the ready list, so that the next call to * epoll_wait() will check again the events * availability. At this point, no one can insert * into ep-&gt;rdllist besides us. The epoll_ctl() * callers are locked out by * ep_scan_ready_list() holding "mtx" and the * poll callback will queue them in ep-&gt;ovflist. */ list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); &#125; &#125; return 0;&#125; 参考文献[1] epoll react[2] linux epoll源码分析[3] IO复用select/poll/epoll[4] IO复用epoll[5] linux epoll源码[6] linux poll/epoll实现[7] linux源码github仓库]]></content>
      <categories>
        <category>IO多路复用模型</category>
        <category>UML</category>
        <category>linux源码</category>
      </categories>
      <tags>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于线程池、消息队列和epoll模型实现并发服务器架构]]></title>
    <url>%2F2019%2F05%2F25%2Fcs-threadpool-message-queue%2F</url>
    <content type="text"><![CDATA[引言并发是什么？企业在进行产品开发过程中为什么需要考虑这个问题？想象一下天猫的双11和京东的618活动，一秒的点击量就有几十万甚至上百万，这么多请求一下子涌入到服务器，服务器需要对这么多的请求逐个进行消化掉，假如服务器一秒的处理能力就几万，那么剩下的不能及时得到处理的这些请求作何处理？总不能让用户界面一直等着，因此消息队列应运而生，所有的请求都统一放入消息队列，工作线程从消息队列不断的消费，消息队列相当于一个缓冲区，可达到解藕、异步和削峰的目的。 Kafka、ActiveMQ、RabbitMQ和RockerMQ都是消息队列的典型，每一种都有其自身的优势和劣势。本文我用自己编写的Buffer类模拟消息队列，如果是企业级需要上线的应用，一般都是基于业界已有的MQ框架上开发。 需求原型 N个Client从标准输入接收数据，然后连续不断的发送到Server端； Server端接收来自每个Client的数据，将数据中的小写字母全部转换成大写字母，其他字符保持不变，最后把转换结果发送给对应的Client。 需求分解 拿到需求，第一步要做的就是分析需求并选择合适的设计架构，考虑到Server需要和Client进行通信，Client来自四面八方，端对端通信自然选择TCP，因此Server端需要能够监听新的连接请求和已有连接的业务请求； 又由于Server需要响应多个Client的业务请求，我们希望把业务处理交给Server端的工作线程（消费者）来做； 同时还需要一个IO线程负责监听Socket描述符，当IO线程监听到已有连接的业务请求时，立即把请求内容封装成一个任务推入消息队列尾； IO线程与工作线程互斥访问消息队列，当然工作线程消费一个任务或者IO线程添加一个任务都需要通知对方，也就是同步； 工作线程处理完毕后，把处理结果交给IO线程，由IO线程负责把结果发送给对应的Client，也就是IO线程与工作线程的分离，这里工作线程通知IO线程的方式我用eventfd来实现； 我们希望引入Log4cpp记录服务端的日志，并能够保存到文件中； 分析完这些，一个整体架构和大体的样子在脑海中就已经形成了，接着就需要编写设计文档和画流程图、类图和时序图了。 详细设计文档 UML静态类图： UML动态时序图： 效果 如图，开了三个Client，运行结果正确： Server端通过Log4cpp把日志写到文件中： 源码获取https://github.com/icoty/cs_threadpool_epoll_mq 目录结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859.├── client // 客户端Demo│ ├── Client.cc│ ├── Client.exe│ ├── client.sh // 进入该目录下启动Client Demo: sh client.sh│ ├── Log4func.cc // 引入日志模块重新疯转│ ├── Log4func.h│ └── Makefile // 编译方式：make├── conf│ └── my.conf // IP,Port配置文件, 从这里进行修改├── include // 头文件│ ├── Configuration.hpp // 配置文件,单例类,my.conf的内存化│ ├── FileName.hpp // 全局定义,Configuration会用到│ ├── log // 日志模块头文件│ │ └── Log4func.hpp│ ├── net // 网络框架模块头文件│ │ ├── EpollPoller.hpp│ │ ├── InetAddress.hpp│ │ ├── Socket.hpp│ │ ├── SockIO.hpp│ │ ├── TcpConnection.hpp│ │ └── TcpServer.hpp│ ├── String2Upper.hpp // 工作线程转换成大写实际走的这里面的接口│ ├── String2UpperServer.hpp // Server端的整个工厂│ └── threadpool // 线程池、锁、条件变量和消息队列的封装│ ├── Buffer.hpp│ ├── Condition.hpp│ ├── MutexLock.hpp│ ├── Noncopyable.hpp│ ├── Pthread.hpp│ ├── Task.hpp│ └── Threadpool.hpp├── log // Server端的日志通过Log4cpp记录到这个文件中│ └── log4test.log├── Makefile // 编译方式：make├── README.md ├── server // server端Demo│ ├── server.exe│ └── server.sh // 进入该目录下启动Server Demo：sh server.sh└── src // 源文件 ├── Configuration.cpp ├── log │ └── Log4func.cpp ├── main.cpp ├── net │ ├── EpollPoller.cpp │ ├── InetAddress.cpp │ ├── Socket.cpp │ ├── SockIO.cpp │ ├── TcpConnection.cpp │ └── TcpServer.cpp ├── String2Upper.cpp ├── String2UpperServer.cpp └── threadpool ├── Buffer.cpp ├── Condition.cpp ├── MutexLock.cpp // MutexLockGuard封装 ├── Pthread.cpp └── Threadpool.cpp 参考文献[1] UNIX环境高级编程第3版[2] cpp reference[3] UML时序图[4] Log4cpp官网下载[5] Log4cpp安装]]></content>
      <categories>
        <category>IO多路复用模型</category>
        <category>同步机制</category>
        <category>并发服务器架构</category>
        <category>互斥机制</category>
        <category>TCP</category>
        <category>线程池</category>
        <category>UML</category>
        <category>C++</category>
        <category>IPC</category>
      </categories>
      <tags>
        <tag>eventfd</tag>
        <tag>STL</tag>
        <tag>线程池</tag>
        <tag>TCP/Socket编程</tag>
        <tag>消息队列</tag>
        <tag>条件变量</tag>
        <tag>锁</tag>
        <tag>面向对象编程</tag>
        <tag>单例模式</tag>
        <tag>epoll</tag>
        <tag>Log4cpp</tag>
        <tag>Makefile</tag>
        <tag>智能指针</tag>
        <tag>UML</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo引入Mermaid流程图和MathJax数学公式]]></title>
    <url>%2F2019%2F05%2F23%2Fmarkdown-mermaid-mathjax%2F</url>
    <content type="text"><![CDATA[近来用Markdown写文章，越来越不喜欢插入图片了，一切能用语法解决的问题坚决不放图，原因有二： 如果把流程图和数学公式都以图片方式放到文章内，当部署到Github上后，访问博客时图片加载实在太慢，有时一篇文章需要画10来个流程图，那你就得截图10来多次，还得给这些图片想一个合适的名字，同时插入图片的时候还要注意图片的插入位置和顺序； 如果你要把文章发布到其他博客平台，如CSDN、博客园，在每一个平台上你都要插入10来多次图片，作为程序员，这种笨拙又耗时的方法，我实在不能忍。 于是愤而搜索，Mermaid语法可实现流程图功能，MathJax语法可实现数学公式和特殊符号的功能，只需要遵循其语法规则即可，这也不由得让我想起：“苏乞儿打完降龙十八掌前17掌之后幡然领悟出第18掌的奥妙时说的那句话：我实在是太聪明了！”。下面都以next主题为例，我的主题是https://github.com/theme-next/hexo-theme-next Mermaid 如果你用的主题和我的主题仓库是同一个，你只需修改blog/themes/next/_config.yml内mermaid模块enable为true，其他的啥也不用做。 12345678910$cd blog/ # 走到博客根目录$yarn add hexo-filter-mermaid-diagrams # 安装mermaid插件# Mermaid tagmermaid: enable: true # Available themes: default | dark | forest | neutral theme: forest cdn: //cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js #cdn: //cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js 如果你的不是next主题或者你的next主题是github上旧版本仓库，你首先需要查看themes/next/_config.yml内是否有mermaid模块，如果有，按照前面的方法1，执行完方法1后，如果不奏效，不要改回去，接着下面的内容继续配置。如果没有mermaid模块，仍然着接下面内容继续配置。 编辑博客根目录下的blog/_config.yml，在最后添加如下内容： 123456# mermaid chartmermaid: ## mermaid url https://github.com/knsv/mermaid enable: true # default true version: "7.1.2" # default v7.1.2 options: # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js #startOnload: true // default true 编辑blog/themes/next/layout/_partials/footer.swig，在最后添加如下内容： 12345678&#123;% if theme.mermaid.enable %&#125; &lt;script src='https://unpkg.com/mermaid@&#123;&#123; theme.mermaid.version &#125;&#125;/dist/mermaid.min.js'&gt;&lt;/script&gt; &lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;&#123; JSON.stringify(theme.mermaid.options) &#125;&#125;); &#125; &lt;/script&gt;&#123;% endif %&#125; 如果你的主题下没有footer.swig文件，你需要在你的主题目录下搜索文件名为after-footer.ejs和after_footer.pug的文件，根据文件名的不同在其最后添加不同的内容，这点在github上的 hexo-filter-mermaid-diagrams 教程已经明确交代了。123456789101112131415161718# 若是after_footer.pug，在最后添加内容if theme.mermaid.enable == true script(type='text/javascript', id='maid-script' mermaidoptioins=theme.mermaid.options src='https://unpkg.com/mermaid@'+ theme.mermaid.version + '/dist/mermaid.min.js' + '?v=' + theme.version) script. if (window.mermaid) &#123; var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins')); mermaid.initialize(options); &#125;# 若是after-footer.ejs，在最后添加&lt;% if (theme.mermaid.enable) &#123; %&gt; &lt;script src='https://unpkg.com/mermaid@&lt;%= theme.mermaid.version %&gt;/dist/mermaid.min.js'&gt;&lt;/script&gt; &lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;theme: 'forest'&#125;); &#125; &lt;/script&gt;&lt;% &#125; %&gt; 最后，赶紧部署到github上观看效果吧，如果不奏效的话，把blog/_config.yml中的external_link设置为false和设置为true都试下，这点在github教程上也已经交代了，因为我的next版本不涉及这个问题，请君多试。1!!!Notice: if you want to use 'Class diagram', please edit your '_config.yml' file, set external_link: false. - hexo bug. 前两步做完后，如果都不奏效，这里还有一招绝杀技，那就是打开blog/public目录下你写的文章的index.html。 搜索“mermaid”，所有的流程图都应该是括在一个标签类的，如果你的流程图没有class = “mermaid”，那就是第一步安装的hexo-filter-mermaid-diagrams插件没有解析成功，可能是hexo，node，yarn版本问题所致。 12345# 流程图解析为：&lt;pre class="mermaid"&gt;流程图&lt;/pre&gt;&lt;pre class="mermaid"&gt;graph LRA[Bob&lt;br&gt;输入明文P] --&gt;|P|B["Bob的私钥PRbob&lt;br&gt;加密算法(如RSA)&lt;br&gt;C=E(PRbob,P)"];B --&gt;|传输数字签名C|C["Alice的公钥环&#123;PUbob,……&#125;&lt;br&gt;解密算法(如RSA)&lt;br&gt;P=D(PUbob,C)"];C --&gt;|P|D["Alice&lt;br&gt;输出明文P"];&lt;/pre&gt; 若流程图确实解析成功了，但是web仍然不显示流程图，说明js文件引入失败，继续在index.html中搜索“mermaid.min.js”，正常情况下需要有如下内容，如果没有，在文件最后的”body”之前添加上，之后再部署观看效果，到此理论上应该可以了，如果还是不行，仔细检查下有没有遗漏步骤，考验你解bug的时候到了。 123456&lt;script src="https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js"&gt;&lt;/script&gt;&lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;theme: 'forest'&#125;); &#125;&lt;/script&gt; MathJax我的主题只需修改blog/themes/next/_config.yml内math模块enable为true即可，不需要安装任何插件，修改之后，在文章的Front Matter栏添加”mathjax: true”才能解析，其他主题也可以试下该方法可行否，都大同小异。12345678910111213141516171819202122232425262728293031# Math Equations Render Supportmath: enable: true # 这里改为true # Default (true) will load mathjax / katex script on demand. # That is it only render those page which has `mathjax: true` in Front Matter. # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE. per_page: true engine: mathjax #engine: katex # hexo-rendering-pandoc (or hexo-renderer-kramed) needed to full MathJax support. mathjax: cdn: //cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML #cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML # See: https://mhchem.github.io/MathJax-mhchem/ #mhchem: //cdn.jsdelivr.net/npm/mathjax-mhchem@3 #mhchem: //cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0 # hexo-renderer-markdown-it-plus (or hexo-renderer-markdown-it with markdown-it-katex plugin) needed to full Katex support. katex: cdn: //cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css #cdn: //cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css copy_tex: # See: https://github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex enable: false copy_tex_js: //cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js copy_tex_css: //cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css 12345678910# 文章引入方式---title: 常用加密算法的应用date: 2019-05-20 13:58:36tags: [对称加密算法,非对称加密算法/公钥算法,Hash函数/散列函数/摘要函数,消息认证,流密码,数字签名/指纹/消息摘要]categories: - [密码学与信息安全]copyright: truemathjax: true # 添加这行，文章才会解析--- 参考文献MathJax语法规则Mermaid语法规则Mermaid官方教程Mermaid Github仓库MathJax Github仓库]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Mermaid</tag>
        <tag>MathJax</tag>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用加密算法的应用]]></title>
    <url>%2F2019%2F05%2F20%2Fcrypt%2F</url>
    <content type="text"><![CDATA[实际工作和开发过程中，网络通信过程中的数据传输和存储大多需要经过严格的加解密设计，比如用户的登陆与注册，敏感信息传输，支付网站和银行的交易信息，甚至为了防止被拖库，数据库的敏感信息存储也需要经过精心的设计。在进行安全设计过程中，或多或少涉及到密码学的一些概念，比如对称加密算法，非对称加密算法(也名公钥算法)，消息认证，Hash函数(也名散列函数或摘要算法)，数字签名(也名指纹或摘要)，流密码等。 一直以来，对于这些概念，你是否有一种模棱两可，似懂非懂的感觉？下面咱们一起揭开密码学这层神秘的面纱。 基本概念密码体制密码体制是满足以下5个条件的五元组(P, C, K, E, D)，满足条件: P(Plaintext)是可能明文的有限集(明文空间); C(Ciphertext)是可能密文的有限集(密文空间); K(Key)是一切可能密钥构成的有限集(密钥空间); E(Encrtption)和D(Decryption)是分别由密钥决定的所有加密算法和解密算法的集合; 存在： $k \in K$ ,有加密算法 $e_k:P \rightarrow C$ , $e_k \in E$；同时有由 $ {k_1} \in K$ 决定的解密算法 $d_{k_1} : C \rightarrow P，d_{k_1} \in D$；满足关系 $d_{k_1} (e_k(x)) = x, x \in P$。 密码破译密码破译根本目的在于破译出密钥或密文，假设破译者Oscar是在已知密码体制的前提下来破译Bob使用的密钥。这个假设被称为Kerckhoff准则，最常见的破解类型有如下5种，从1～5，Oscar的破译难度逐渐降低。 唯密文攻击：Oscar仅具有密文串c，Oscar只能通过统计特性分析密文串p的规律； 已知明文攻击：Oscar具有一些明文串p和相应的密文c，{p，c}可以是{P，C}的任意非空子集； 选择明文攻击：Oscar可获得对加密机的暂时访问，因此他能选择特定明文串p并构造出相应的密文串c； 选择密文攻击：Oscar可暂时接近解密机，因此他能选择特定密文串c并构造出相应的明文串p。 选择文本攻击：Oscar可以制造任意明文(p) / 密文(c)并得到对应的密文(c) / 明文(p)。 加密算法对称加密算法对称加密算法的加密密钥和解密密钥相同，常见的对称加密算法有：AES、DES、2DES、3DES、RC4、RC5、RC6，Blowfish和IDEA，目前使用最广泛的是DES、AES。 graph LR A[发送方Bob输入明文P] -->|P|B["发送方Bob与接收方Alice共用相同密钥K加密算法(如DES)C=E(K,P)"]; B -->|传输密文C|C["接收方Alice与发送方Bob共用相同密钥K解密算法(如DES)P=D(K,C)"]; C -->|P|D["Alice接收方输出明文P"]; 非对称加密算法(公钥算法)公钥算法的加密算法和解密算法使用不同的密钥，分别为公钥和私钥，这两个密钥中的任何一个都可以用来加密，而另一个用来解密。常见的公钥算法有：椭圆曲线(ECC)、RSA、Diffie-Hellman、El Gamal(安全性建立在基于求解离散对数是困难的)、DSA(适用于数字签名)。 公钥算法的应用 发送方Bob用接收方Alice的公钥对消息进行加密，接收方Alice用自己的私钥进行解密，可提供消息传输过程中的保密性。 graph LR A[Bob输入明文P] -->|P|B["Bob的公钥环{PUalice,……}加密算法C=E(PUalice,P)"]; B -->|传输密文C|C["Alice私钥PRalice解密算法P=D(PRalice,C)"]; C -->|P|D["Alice输出明文P"]; 发送方Bob采用自己的私钥对明文进行加密，虽然任何持有Bob公钥的人都能够解密，但是只有拥有Bob私钥的人才能产生密文C，而Bob的私钥只有自己知道，因此密文C也叫做数字签名，数字签名C可用于认证源和数据的完整性。 graph LR A[Bob输入明文P] -->|P|B["Bob的私钥PRbob加密算法(如RSA)C=E(PRbob,P)"]; B -->|传输数字签名C|C["Alice的公钥环{PUbob,……}解密算法(如RSA)P=D(PUbob,C)"]; C -->|P|D["Alice输出明文P"]; 发送方Bob首先采用自己的私钥对明文进行加密，然后使用接收方Alice的公钥再进行一次加密后传输，则既可提供认证功能，又可提供消息传输过程中的保密性。 graph LR A[Bob输入明文P] -->|P|B["Bob的私钥PRbob加密算法(如RSA)C=E(PRbob,P)"]; B -->|数字签名C|C["Bob的公钥环{PUalice,……}加密算法(如RSA)C1=E(PUalice,C)"]; C -->|传输密文C1|D["Alice的私钥PRalice解密算法(如RSA)C=D(PRalice,C1)"]; D -->|数字签名C|E["Alice的公钥环{PUbob,……}解密算法(如RSA)P=D(PUbob,C)"]; E -->|P|F["Alice输出明文P"]; 发送方Bob用接收方Alic的公钥对自己的私钥进行加密，然后发送给Alice，Alic用自己的私钥解密即可得到发送方Bob的私钥，从而实现密钥交换功能。 graph LR A[Bob的私钥PRbob] -->|Bob的私钥PRbob|B["Bob的公钥环{PUalice,……}加密算法(如RSA)C=E(PUalice,PRbob)"]; B -->|传输密文C|C["Alice的私钥PRalice解密算法(如RSA)PRbob=D(PRalice,C)"]; C -->|PRbob|D["AliceBob的私钥PRbob"]; 另外需要说明一下，Diffie-Hellman的密钥交换算法与此方法不同，如果你学过密码学，应该清楚其中的差异。并且并不是所有的公钥算法都支持加密/解密、数字签名和密钥交换功能，有的公钥算法只支持其中的一种或两种，下表列出部分公钥算法锁支持的应用。 算法 加密/解密 数字签名 密钥交换 RSA安全性建立在基于大素数分解是困难的 Y Y Y 椭圆曲线/ECC安全性建立在椭圆曲线对数问题之上(即由kP和P确定k是困难的) Y Y Y Diff-Hellman安全性建立在计算离散对数是很困难的 N Y Y DSS N Y N Hash函数(散列函数或摘要函数)Hash函数将可变长度的消息映射为固定长度的Hash值或消息摘要，常见的Hash算法有：MD2、MD4、MD5、SHA-1、SHA-224、SHA-256、SHA-384、SHA-512、HAVAL、HMAC、HMAC-MD5、HMAC-SHA1。对于给定的密码学Hash函数y=Hash(x)，要求如下两种情况再计算上不可行： 对给定的y，找到对应的x； 找到两个不同的x1和x2，使得Hash(x1)=Hash(x2)，具有抗碰撞性的特点。 Hash函数的应用 消息认证是用来验证消息完整性的一种机制或服务，消息认证确认收到的数据确实和发送时的一样(即防篡改)，并且还要确保发送方的身份是真实有效的的(即防冒充)。下图以对称加密算法为例，因为对称密钥K只有Bob和Alice才有，保证了发送方的合法有效性，同时比较C3与C是否相等，可以确定传输过程中是否被篡改过。 graph LR A[Bob输入明文P] -->|P|B["BobHash函数(如sha256)C=Hash(P)"]; B -->|C|C["BobC1=P||C"]; A -->|P|C; C -->|"C1=P||C"|D["Bob和Alice公用的密钥K对称加密算法(如DES)C2=E(K,C1)"]; D -->|传输密文C2|E["Alice和Bob公用的密钥K对称解密算法(如DES)C1=D(K,C2)"]; E -->|"C1=P||C"|F["Alice1.Hash函数(如sha256)C3=Hash(P)2.比较C3与C是否相等"]; 数字签名(也名指纹或摘要)是一种认证机制，它使得消息的产生者可以添加一个起签名作用的码字，通过计算消息的Hash值并用产生者的私钥加密Hash值来生成签名，签名保证了消息和来源和完整性。下图最后一步比较C3与C如果不相等，认证失败，该图没有提供保密性，因为传输过程中只是将P和C1简单的连接在一起，并没有对C2进行加密，如果需要提供保密性，可以使用Alic的私钥对C2加密后再传输。 graph LR A[Bob输入明文P] -->|P|B["BobHash函数(如sha256)C=Hash(P)"]; B -->|C|C["Bob的私钥PRbob加密算法(如RSA)C1=E(PRbob,C)"]; C -->|C1|D["BobC2=P||C1"]; A -->|P|D; D -->|传输C2|E["Alice的公钥环{PUbob,……}1.解密算法(如RSA)C=D(PUbob,C1)2.Hash函数(如sha256)C3=Hash(P)3.比较C3与C是否相等"]; 用于产生单向口令文件，比如操作系统存储的都是口令的Hah值而不是口令本身，当用户输入口令时，计算其Hash值和之前存储的口令比对，这样即使操作系统被黑之后，也能保证用户口令的安全性。同样适用于入侵检测和病毒检测，如将你需要保护的文件的Hash值存储到安全系统中(比如只读设备中，不可修改也不可删除)，这样病毒入侵后只能修改文件而不能修改Hash值，于是可以通过重新计算文件的Hash值和之前保存的Hash值比对。 加密方式流密码典型的流密码是每次加密一个字节的密文，加密长度可以按需求设计，比如每次只加密一位或者大于一个字节的单元都行。实质上$Ci=Pi \oplus K1i，Pi=Ci \oplus K2i$，就是简单的异或，加密异或一次，解密再异或一次，即可恢复明文字节流。 graph LR A["Bob明文字节流P1~Pn"] -->|"P1~Pn"|C["Bob加密函数Ci=E(K1i,Pi)"]; B["Bob 由密钥K1控制的密钥流发生器K11～K1n其中K1i=K2i"] -->|"K11～K1n"|C; C -->|"传输密文C1～Cn"|E["Alice解密函数Pi=D(K2i,Ci)"]; D["Alice 由密钥K2控制的密钥流发生器K21～K2n其中K1i=K2i"] -->|"K21～K2n"|E; E -->|"明文字节流P1～Pn"|F["Alice明文字节流P1~Pn"]; 参考文献MathJax语法规则Mermaid语法规则Mermaid官方教程Mermaid Github仓库MathJax Github仓库常用加密算法概述HTTPS建立过程]]></content>
      <categories>
        <category>密码学与信息安全</category>
      </categories>
      <tags>
        <tag>对称加密算法</tag>
        <tag>非对称加密算法/公钥算法</tag>
        <tag>Hash函数/散列函数/摘要函数</tag>
        <tag>消息认证</tag>
        <tag>流密码</tag>
        <tag>数字签名/指纹/消息摘要</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nachos-Lab3-同步与互斥机制模块实现]]></title>
    <url>%2F2019%2F05%2F14%2Fnachos-3-4-Lab3%2F</url>
    <content type="text"><![CDATA[源码获取https://github.com/icoty/nachos-3.4-Lab 内容一：总体概述本实习希望通过修改Nachos系统平台的底层源代码，达到“扩展同步机制，实现同步互斥实例”的目标。 内容二：任务完成情况任务完成列表（Y/N） Exercise1 Exercise2 Exercise3 Exercise4 Challenge1 Challenge2 Challenge3 第一部分 Y Y Y Y Y Y N 具体Exercise的完成情况Exercise1 调研调研Linux或Windows中采用的进程/线程调度算法。具体内容见课堂要求。 同步是指用于实现控制多个进程按照一定的规则或顺序访问某些系统资源的机制，进程间的同步方式有共享内存，套接字，管道，信号量，消息队列，条件变量；线程间的同步有套接字，消息队列，全局变量，条件变量，信号量。 互斥是指用于实现控制某些系统资源在任意时刻只能允许一个进程访问的机制。互斥是同步机制中的一种特殊情况。进程间的互斥方式有锁，信号量，条件变量；线程间的互斥方式有信号量，锁，条件变量。此外，通过硬件也能实现同步与互斥。 linux内核中提供的同步机制 原子操作 自旋锁 读写自旋锁 信号量 读写信号量 互斥量 完成变量 大内核锁 顺序锁 禁止抢占 顺序和屏障 Exercise2 源代码阅读code/threads/synch.h和code/threads/synch.cc：Condition和Lock仅仅声明了未定义；Semaphore既声明又定义了。 Semaphore有一个初值和一个等待队列，提供P、V操作： P操作：当value等于0时，将当前运行线程放入线程等待队列，当前进程进入睡眠状态，并切换到其他线程运行；当value大于0时，value–。 V操作：如果线程等待队列中有等待该信号量的线程，取出其中一个将其设置成就绪态，准备运行，value++。 Lock：Nachos中没有给出锁机制的实现，接口有获得锁(Acquire)和释放锁(Release)，他们都是原子操作。 Acquire：当锁处于BUSY态，进入睡眠状态。当锁处于FREE态，当前进程获得该锁，继续运行。 Release：释放锁（只能由拥有锁的线程才能释放锁），将锁的状态设置为FREE态，如果有其他线程等待该锁，将其中的一个唤醒，进入就绪态。 Condition：条件变量同信号量、锁机制不一样，条件变量没值。当一个线程需要的某种条件没有得到满足时，可以将自己作为一个等待条件变量的线程插入所有等待该条件变量的队列，只要条件一旦得到满足，该线程就会被唤醒继续运行。条件变量总是和锁机制一起使。主要接口Wait、Signal、BroadCast，这三个操作必须在当前线程获得一个锁的前提下，而且所有对一个条件变量进行的操作必须建立在同一个锁的前提下。 Wait(Lock *conditionLock)：线程等待在条件变量上，把线程放入条件变量的等待队列上。 Signal(Lock *conditionLock)：从条件变量的等待队列中唤醒一个等待该条件变量的线程。 BroadCast(Lock *conditionLock)：唤醒所有等待该条件变量的线程。 code/threads/synchlist.h和code/threads/synchlist.cc：利用锁、条件变量实现的一个消息队列，使多线程达到互斥访问和同步通信的目的，类内有一个Lock和List成员变量。提供了对List的Append()，Remove()和Mapcar()操作。每个操作都要先获得该锁，然后才能对List进行相应的操作。 Exercise3 实现锁和条件变量可以使用sleep和wakeup两个原语操作（注意屏蔽系统中断），也可以使用Semaphore作为唯一同步原语（不必自己编写开关中断的代码）。 这里选择用1值信号量实现锁功能，Lock添加成员变量lock和owner，请求锁和释放锁都必须关中断，Condition添加一个成员变量queue，用于存放所有等待在该条件变量上的线程。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899// synch.h Lock声明部分class Lock &#123;……private: char* name; // for debugging // add by yangyu Semaphore *lock; Thread* owner;&#125;;class Condition &#123;……private: char* name; // add by yangyu List* queue;&#125;;// synch.cc Lock定义部分Lock::Lock(char* debugName) :lock(new Semaphore("lock", 1)),name(debugName),owner(NULL)&#123;&#125;Lock::~Lock() &#123; delete lock;&#125;bool Lock::isHeldByCurrentThread()&#123; return currentThread == owner;&#125;void Lock::Acquire() &#123; IntStatus prev = interrupt-&gt;SetLevel(IntOff); lock-&gt;P(); owner = currentThread; (void)interrupt-&gt;SetLevel(prev);&#125;void Lock::Release() &#123; IntStatus prev = interrupt-&gt;SetLevel(IntOff); ASSERT(currentThread == owner); lock-&gt;V(); owner = NULL; (void)interrupt-&gt;SetLevel(prev);&#125;// synch.cc Condition定义部分Condition::Condition(char* debugName):name(debugName),queue(new List)&#123; &#125;Condition::~Condition()&#123; &#125;void Condition::Wait(Lock* conditionLock) &#123; //ASSERT(FALSE); // 关中断 IntStatus prev = interrupt-&gt;SetLevel(IntOff); // 锁和信号量不同，谁加锁必须由谁解锁，因此做下判断 ASSERT(conditionLock-&gt;isHeldByCurrentThread()); // 进入睡眠前把锁的权限释放掉，然后放到等待队列，直到被唤醒时重新征用锁 conditionLock-&gt;Release(); queue-&gt;Append(currentThread); currentThread-&gt;Sleep(); conditionLock-&gt;Acquire(); (void)interrupt-&gt;SetLevel(prev);&#125;void Condition::Signal(Lock* conditionLock) &#123; IntStatus prev = interrupt-&gt;SetLevel(IntOff); ASSERT(conditionLock-&gt;isHeldByCurrentThread()); if(!queue-&gt;IsEmpty()) &#123; // 唤醒一个等待的线程，挂入倒就绪队列中 Thread* next = (Thread*)queue-&gt;Remove(); scheduler-&gt;ReadyToRun(next); &#125; (void)interrupt-&gt;SetLevel(prev);&#125;void Condition::Broadcast(Lock* conditionLock) &#123; IntStatus prev = interrupt-&gt;SetLevel(IntOff); ASSERT(conditionLock-&gt;isHeldByCurrentThread()); // 唤醒等待在该条件变量上的所有线程 while(!queue-&gt;IsEmpty()) &#123; Signal(conditionLock); &#125; (void)interrupt-&gt;SetLevel(prev);&#125; Exercise4 实现同步互斥实例基于Nachos中的信号量、锁和条件变量，采用两种方式实现同步和互斥机制应用（其中使用条件变量实现同步互斥机制为必选题目）。具体可选择“生产者-消费者问题”、“读者-写者问题”、“哲学家就餐问题”、“睡眠理发师问题”等。（也可选择其他经典的同步互斥问题）。 生产者-消费者问题(Condition实现)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121// threadtest.cc// 条件变量实现生产者消费者问题Condition* condc = new Condition("ConsumerCondition");Condition* condp = new Condition("ProducerCondition");Lock* pcLock = new Lock("producerConsumerLock");int shareNum = 0; // 共享内容，生产+1，消费-1，互斥访问// lab3 条件变量实现生产者消费者问题void Producer1(int val)&#123; while(1)&#123; pcLock-&gt;Acquire(); // 缓冲区已满则等待在条件变量上，停止生产，等待消费后再生产 while(shareNum &gt;= N)&#123; printf("Product alread full:[%d],threadId:[%d],wait consumer.\n",shareNum,currentThread-&gt;getThreadId()); condp-&gt;Wait(pcLock); &#125; printf("name:[%s],threadId:[%d],before:[%d],after:[%d]\n",currentThread-&gt;getName(),currentThread-&gt;getThreadId(),shareNum,shareNum+1); ++shareNum; // 生产一个通知可消费，唤醒一个等待在condc上的消费者 condc-&gt;Signal(pcLock); pcLock-&gt;Release(); sleep(val); &#125;&#125;void Customer1(int val)&#123; while(1)&#123; pcLock-&gt;Acquire(); // 为零表示已经消费完毕,等待在条件变量上，等待生产后再消费 while(shareNum &lt;= 0)&#123; printf("--&gt;Product alread empty:[%d],threadId:[%d],wait producer.\n",shareNum,currentThread-&gt;getThreadId()); condc-&gt;Wait(pcLock); &#125; printf("--&gt;name:[%s],threadId:[%d],before:[%d],after:[%d]\n",currentThread-&gt;getName(),currentThread-&gt;getThreadId(),shareNum,shareNum-1); --shareNum; // 消费一个后通知生产者缓冲区不为满，可以生产 condp-&gt;Signal(pcLock); pcLock-&gt;Release(); //sleep(val); &#125;&#125;void ThreadProducerConsumerTest1()&#123; DEBUG('t', "Entering ThreadProducerConsumerTest1"); // 两个生产者循环生产 Thread* p1 = new Thread("Producer1"); Thread* p2 = new Thread("Producer2"); p1-&gt;Fork(Producer1, 1); p2-&gt;Fork(Producer1, 3); // 两个消费者循环消费 Thread* c1 = new Thread("Consumer1"); Thread* c2 = new Thread("Consumer2"); c1-&gt;Fork(Customer1, 1); c2-&gt;Fork(Customer1, 2);&#125;void ThreadTest()&#123; switch (testnum) &#123; case 1: ThreadTest1(); break; case 2: ThreadCountLimitTest(); break; case 3: ThreadPriorityTest(); break; case 4: ThreadProducerConsumerTest(); break; case 5: ThreadProducerConsumerTest1(); break; case 6: barrierThreadTest(); break; case 7: readWriteThreadTest(); break; default: printf("No test specified.\n"); break; &#125;&#125;// 运行结果，需要-rs，否则可能没有中断发生，永远是一个线程在运行// 通过结果可以明确看出生产前和生产后，消费前和消费后的数值变化// 可以通过修改Producer1和Consumer1内的sleep(val)来调整不同的速度// 当生产满了会停止生产，消费完了也会停止消费root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -rs -q 5name:[Producer1],threadId:[1],before:[0],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer1],threadId:[1],before:[1],after:[2]--&gt;name:[Consumer2],threadId:[4],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer1],threadId:[1],before:[1],after:[2]--&gt;name:[Consumer2],threadId:[4],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]--&gt;name:[Consumer2],threadId:[4],before:[1],after:[0]name:[Producer1],threadId:[1],before:[0],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]name:[Producer1],threadId:[1],before:[2],after:[3]--&gt;name:[Consumer2],threadId:[4],before:[3],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]name:[Producer1],threadId:[1],before:[2],after:[3]--&gt;name:[Consumer2],threadId:[4],before:[3],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]^CCleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# 生产者-消费者问题(Semaphore实现)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485// threadtest.cc// 信号量解决生产者消费者问题#define N 1024 // 缓冲区大小Semaphore* empty = new Semaphore("emptyBuffer", N);Semaphore* mutex = new Semaphore("lockSemaphore", 1);Semaphore* full = new Semaphore("fullBuffer", 0);int msgQueue = 0;void Producer(int val)&#123; while(1) &#123; empty-&gt;P(); mutex-&gt;P(); if(msgQueue &gt;= N)&#123; // 已经满了则停止生产 printf("--&gt;Product alread full:[%d],wait consumer.",msgQueue); &#125;else&#123; printf("--&gt;name:[%s],threadId:[%d],before:[%d],after:[%d]\n",\ currentThread-&gt;getName(),currentThread-&gt;getThreadId(),msgQueue,msgQueue+1); ++msgQueue; &#125; mutex-&gt;V(); full-&gt;V(); sleep(val); // 休息下再生产 &#125;&#125;void Customer(int val)&#123; while(1) &#123; full-&gt;P(); mutex-&gt;P(); if(msgQueue &lt;= 0)&#123; printf("Product alread empty:[%d],wait Producer.",msgQueue); &#125;else&#123; printf("name:[%s] threadId:[%d],before:[%d],after:[%d]\n",\ currentThread-&gt;getName(),currentThread-&gt;getThreadId(),msgQueue,msgQueue-1); --msgQueue; &#125; mutex-&gt;V(); empty-&gt;V(); sleep(val); // 休息下再消费 &#125;&#125;void ThreadProducerConsumerTest()&#123; DEBUG('t', "Entering ThreadProducerConsumerTest"); // 两个生产者 Thread* p1 = new Thread("Producer1"); Thread* p2 = new Thread("Producer2"); p1-&gt;Fork(Producer, 1); p2-&gt;Fork(Producer, 3); // 两个消费者，可以关掉一个消费者，查看生产速率和消费速率的变化 Thread* c1 = new Thread("Consumer1"); //Thread* c2 = new Thread("Consumer2"); c1-&gt;Fork(Customer, 1); //c2-&gt;Fork(Customer, 2);&#125;// 通过结果可以明确看出生产前和生产后，消费前和消费后的数值变化// 可以通过修改Producer和Consumer内的sleep(val)来调整不同的速度// 当生产满了会停止生产，消费完了也会停止消费root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -rs -q 4--&gt;name:[Producer1],threadId:[1],before:[0],after:[1]--&gt;name:[Producer2],threadId:[2],before:[1],after:[2]name:[Consumer1] threadId:[3],before:[2],after:[1]--&gt;name:[Producer1],threadId:[1],before:[1],after:[2]--&gt;name:[Producer2],threadId:[2],before:[2],after:[3]--&gt;name:[Producer1],threadId:[1],before:[3],after:[4]name:[Consumer1] threadId:[3],before:[4],after:[3]--&gt;name:[Producer2],threadId:[2],before:[3],after:[4]name:[Consumer1] threadId:[3],before:[4],after:[3]--&gt;name:[Producer1],threadId:[1],before:[3],after:[4]--&gt;name:[Producer2],threadId:[2],before:[4],after:[5]name:[Consumer1] threadId:[3],before:[5],after:[4]--&gt;name:[Producer1],threadId:[1],before:[4],after:[5]--&gt;name:[Producer2],threadId:[2],before:[5],after:[6]--&gt;name:[Producer1],threadId:[1],before:[6],after:[7]--&gt;name:[Producer2],threadId:[2],before:[7],after:[8]name:[Consumer1] threadId:[3],before:[8],after:[7]^CCleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# Challenge1 实现barrier(至少选做一个Challenge)可以使用Nachos 提供的同步互斥机制（如条件变量）来实现barrier，使得当且仅当若干个线程同时到达某一点时方可继续执行。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// threadtest.cc// 条件变量实现barrierCondition* barrCond = new Condition("BarrierCond");Lock* barrLock = new Lock("BarrierLock");int barrierCnt = 0;// 当且仅当barrierThreadNum个线程同时到达时才能往下运行const int barrierThreadNum = 5; void barrierFun(int num)&#123; /*while(1)*/ &#123; barrLock-&gt;Acquire(); ++barrierCnt; if(barrierCnt == barrierThreadNum)&#123; // 最后一个线程到达后判断，条件满足则发送一个广播信号 // 唤醒等待在该条件变量上的所有线程 printf("threadName:[%s%d],barrierCnt:[%d],needCnt:[%d],Broadcast.\n",\ currentThread-&gt;getName(),num,barrierCnt,barrierThreadNum); barrCond-&gt;Broadcast(barrLock); barrLock-&gt;Release(); &#125;else&#123; // 每一个线程都执行判断，若条件不满足，线程等待在条件变量上 printf("threadName:[%s%d],barrierCnt:[%d],needCnt:[%d],Wait.\n",\ currentThread-&gt;getName(),num,barrierCnt,barrierThreadNum); barrCond-&gt;Wait(barrLock); barrLock-&gt;Release(); &#125; printf("threadName:[%s%d],continue to run.\n", currentThread-&gt;getName(),num); &#125;&#125;void barrierThreadTest()&#123; DEBUG('t', "Entering barrierThreadTest"); for(int i = 0; i &lt; barrierThreadNum; ++i)&#123; Thread* t = new Thread("barrierThread"); t-&gt;Fork(barrierFun,i+1); &#125;&#125;// 运行结果，当第五个线程进入后判断条件满足，唤醒所有线程root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -rs -q 6threadName:[barrierThread1],barrierCnt:[1],needCnt:[5],Wait.threadName:[barrierThread2],barrierCnt:[2],needCnt:[5],Wait.threadName:[barrierThread3],barrierCnt:[3],needCnt:[5],Wait.threadName:[barrierThread4],barrierCnt:[4],needCnt:[5],Wait.threadName:[barrierThread5],barrierCnt:[5],needCnt:[5],Broadcast.threadName:[barrierThread5],continue to run.threadName:[barrierThread2],continue to run.threadName:[barrierThread1],continue to run.threadName:[barrierThread4],continue to run.threadName:[barrierThread3],continue to run.No threads ready or runnable, and no pending interrupts.Assuming the program completed.Machine halting!Ticks: total 814, idle 4, system 810, user 0Disk I/O: reads 0, writes 0Console I/O: reads 0, writes 0Paging: faults 0Network I/O: packets received 0, sent 0Cleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# Challenge2 实现read/write lock基于Nachos提供的lock(synch.h和synch.cc)，实现read/write lock。使得若干线程可以同时读取某共享数据区内的数据，但是在某一特定的时刻，只有一个线程可以向该共享数据区写入数据。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// threadtest.cc// Lab3 锁实现读者写者问题int rCnt = 0; // 记录读者数量Lock* rLock = new Lock("rlock");// 必须用信号量，不能用锁，因为锁只能由加锁的线程解锁Semaphore* wLock = new Semaphore("wlock",1); int bufSize = 0;// Lab3 锁实现读者写者问题void readFunc(int num)&#123; while(1) &#123; rLock-&gt;Acquire(); ++rCnt; // 如果是第一个读者进入，需要竞争1值信号量wLock，竞争成功才能进入临界区 // 一旦竞争到wLock，由最后一个读者出临界区后释放，保证了读者优先 if(rCnt == 1)&#123; wLock-&gt;P(); &#125; rLock-&gt;Release(); if(0 == bufSize)&#123; // 没有数据可读 printf("threadName:[%s],bufSize:[%d],current not data.\n",currentThread-&gt;getName(),bufSize); &#125;else&#123; // 读取数据 printf("threadName:[%s],bufSize:[%d],exec read operation.\n",currentThread-&gt;getName(),bufSize); &#125; rLock-&gt;Acquire(); --rCnt; // 最后一个读者释放wLock if(rCnt == 0)&#123; wLock-&gt;V(); &#125; rLock-&gt;Release(); currentThread-&gt;Yield(); sleep(num); &#125;&#125;void writeFunc(int num)&#123; while(1) &#123; wLock-&gt;P(); ++bufSize; printf("writerThread:[%s],before:[%d],after:[%d]\n", currentThread-&gt;getName(), bufSize, bufSize+1); wLock-&gt;V(); currentThread-&gt;Yield(); sleep(num); &#125;&#125;void readWriteThreadTest()&#123; DEBUG('t', "Entering readWriteThreadTest"); Thread * r1 = new Thread("read1"); Thread * r2 = new Thread("read2"); Thread * r3 = new Thread("read3"); Thread * w1 = new Thread("write1"); Thread * w2 = new Thread("write2"); // 3个读者2个写者 r1-&gt;Fork(readFunc,1); w1-&gt;Fork(writeFunc,1); r2-&gt;Fork(readFunc,1); w2-&gt;Fork(writeFunc,1); r3-&gt;Fork(readFunc,1);&#125;// 运行结果，第一个读者进入无数据可读// 可以发现读操作比写操作多// 一旦开始读，就要等所有线程读取完毕后，写线程才进入root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -rs -q 7threadName:[read1],Val:[0],current not data.writerThread:[write1],before:[0],after:[1]writerThread:[write2],before:[1],after:[2]writerThread:[write1],before:[2],after:[3]writerThread:[write2],before:[3],after:[4]threadName:[read2],readVal:[4],exec read operation.threadName:[read1],readVal:[4],exec read operation.threadName:[read3],readVal:[4],exec read operation.writerThread:[write1],before:[4],after:[5]threadName:[read2],readVal:[5],exec read operation.threadName:[read3],readVal:[5],exec read operation.threadName:[read2],readVal:[5],exec read operation.threadName:[read3],readVal:[5],exec read operation.threadName:[read2],readVal:[5],exec read operation.threadName:[read3],readVal:[5],exec read operation.threadName:[read1],readVal:[5],exec read operation.writerThread:[write2],before:[5],after:[6]writerThread:[write1],before:[6],after:[7]^CCleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# 内容三：遇到的困难以及解决方法困难1刚开始没有加-rs参数，导致永远都只有一个线程在运行，原因是没有中断发生，运行的线程永远在执行循环体，加-rs参数，会在一个固定的时间短内发一个时钟中断，然后调度其他线程运行。 内容四：收获及感想可以说实际操作后，对信号量，条件变量的应用更加清晰了。 内容五：对课程的意见和建议暂无。 内容六：参考文献https://blog.csdn.net/FreeeLinux/article/details/54267446]]></content>
      <categories>
        <category>同步机制</category>
        <category>互斥机制</category>
      </categories>
      <tags>
        <tag>条件变量</tag>
        <tag>锁</tag>
        <tag>信号量</tag>
        <tag>Nachos-3.4</tag>
        <tag>生产者消费者问题</tag>
        <tag>读者写者问题</tag>
        <tag>Barrier</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nachos-Lab2-线程调度模块实现]]></title>
    <url>%2F2019%2F05%2F14%2Fnachos-3-4-Lab2%2F</url>
    <content type="text"><![CDATA[源码获取https://github.com/icoty/nachos-3.4-Lab 内容一：总体概述本实习希望通过修改Nachos系统平台的底层源代码，达到“扩展调度算法”的目标。本次实验主要是要理解Timer、Scheduler和Interrupt之间的关系，从而理解线程之间是如何进行调度的。 内容二：任务完成情况任务完成列表（Y/N） Exercise1 Exercise2 Exercise3 Challenge1 第一部分 Y Y Y Y 具体Exercise的完成情况Exercise1 调研调研Linux或Windows中采用的进程/线程调度算法。具体内容见课堂要求。 linux-4.19.23进程调度策略：SCHED_OTHER分时调度策略，SCHED_FIFO实时调度策略（先到先服务），SCHED_RR实时调度策略（时间片轮转）。 RR调度和FIFO调度的进程属于实时进程，以分时调度的进程是非实时进程。 当实时进程准备就绪后，如果当前cpu正在运行非实时进程，则实时进程立即抢占非实时进程。 RR进程和FIFO进程都采用实时优先级做为调度的权值标准，RR是FIFO的一个延伸。FIFO时，如果两个进程的优先级一样，则这两个优先级一样的进程具体执行哪一个是由其在队列中的位置决定的，这样导致一些不公正性(优先级是一样的，为什么要让你一直运行?)，如果将两个优先级一样的任务的调度策略都设为RR，则保证了这两个任务可以循环执行，保证了公平。 内核代码：内核为每个cpu维护一个进程就绪队列，cpu只调度由其维护的队列上的进程： vi linux-4.19.23/kernel/sched/core.c：123456……#define CREATE_TRACE_POINTS#include &lt;trace/events/sched.h&gt;DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);…… ​ vi linux-4.19.23/kernel/sched/sched.h：123456789101112131415161718192021222324252627282930313233/* * This is the main, per-CPU runqueue data structure. * * Locking rule: those places that want to lock multiple runqueues * (such as the load balancing or the thread migration code), lock * acquire operations must be ordered by ascending &amp;runqueue. */struct rq &#123; /* runqueue lock: */ raw_spinlock_t lock; // 锁保证互斥访问runqueue …… struct cfs_rq cfs; // 所有普通进程的集合，采用cfs调度策略 struct rt_rq rt; // 所有实时进程的集合，采用实时调度策略 struct dl_rq dl; // struct dl_rq空闲进程集合 ……&#125;;// cfs_rq就绪队列是一棵红黑树。/* CFS-related fields in a runqueue */struct cfs_rq &#123; …… struct rb_root_cached tasks_timeline; // 红黑树的树根 /* * 'curr' points to currently running entity on this cfs_rq. * It is set to NULL otherwise (i.e when none are currently running). */ struct sched_entity *curr; // 指向当前正运行的进程 struct sched_entity *next; // 指向将被唤醒的进程 struct sched_entity *last; // 指向唤醒next进程的进程 struct sched_entity *skip; ……&#125;; ​ vi linux-4.19.23/include/linux/sched.h：实时进程调度实体struct sched_rt_entity，双向链表组织形式；空闲进程调度实体struct sched_dl_entity，红黑树组织形式；普通进程的调度实体sched_entity，每个进程描述符中均包含一个该结构体变量，该结构体有两个作用： 包含有进程调度的信息（比如进程的运行时间，睡眠时间等等，调度程序参考这些信息决定是否调度进程）； 使用该结构体来组织进程，struct rb_node类型结构体变量run_node是红黑树节点，struct sched_entity调度实体将被组织成红黑树的形式，同时意味着普通进程也被组织成红黑树的形式。parent指向了当前实体的上一级实体，cfs_rq指向了该调度实体所在的就绪队列。my_q指向了本实体拥有的就绪队列（调度组），该调度组（包括组员实体）属于下一个级别，和本实体不在同一个级别，该调度组中所有成员实体的parent域指向了本实体，depth代表了此队列（调度组）的深度，每个调度组都比其parent调度组深度大1。内核依赖my_q域实现组调度。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546……// 普通进程的调度实体sched_entity，使用红黑树组织struct sched_entity &#123; /* For load-balancing: */ struct load_weight load; unsigned long runnable_weight; struct rb_node run_node; // 红黑树节点 struct list_head group_node; unsigned int on_rq; ……#ifdef CONFIG_FAIR_GROUP_SCHED int depth; struct sched_entity *parent; // 当前节点的父节点 /* rq on which this entity is (to be) queued: */ struct cfs_rq *cfs_rq; // 当前节点所在的就绪队列 /* rq "owned" by this entity/group: */ struct cfs_rq *my_q;#endif ……&#125;;// 实时进程调度实体，采用双向链表组织struct sched_rt_entity &#123; struct list_head run_list; // 链表组织 unsigned long timeout; unsigned long watchdog_stamp; unsigned int time_slice; unsigned short on_rq; unsigned short on_list; struct sched_rt_entity *back;#ifdef CONFIG_RT_GROUP_SCHED struct sched_rt_entity *parent; /* rq on which this entity is (to be) queued: */ struct rt_rq *rt_rq; // 当前节点所在的就绪队列 /* rq "owned" by this entity/group: */ struct rt_rq *my_q;#endif&#125; __randomize_layout;// 空闲进程调度实体，采用红黑树组织struct sched_dl_entity &#123; struct rb_node rb_node; ……&#125;;…… ​ vi linux-4.19.23/kernel/sched/sched.h：内核声明了一个调度类sched_class的结构体类型，用来实现不同的调度策略，可以看到该结构体成员都是函数指针，这些指针指向的函数就是调度策略的具体实现，所有和进程调度有关的函数都直接或者间接调用了这些成员函数，来实现进程调度。此外，每个进程描述符中都包含一个指向该结构体类型的指针sched_class，指向了所采用的调度类。 12345678910111213……struct sched_class &#123; const struct sched_class *next; void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags); void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags); void (*yield_task) (struct rq *rq); bool (*yield_to_task)(struct rq *rq, struct task_struct *p, bool preempt); void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags); ……&#125;;…… Exercise2 源代码阅读code/threads/scheduler.h和code/threads/scheduler.cc：scheduler类是nachos中的进程调度器，维护了一个挂起的中断队列，通过FIFO进行调度。 void ReadyToRun(Thread* thread)；设置线程状态为READY，并放入就绪队列readyList。 Thread* FindNextToRun(int source); 从就绪队列中取出下一个上CPU的线程，实现基于优先级的抢占式调度和FIFO调度。 void Run(Thread* nextThread); 把下CPU的线程的寄存器和堆栈信息从CPU保存到线程本身的寄存器数据结构中， 执行线程切换，把上CPU的线程的寄存器和堆栈信息从线程本身的寄存器中拷贝到CPU的寄存器中，运行新线程。 code/threads/switch.s：switch.s模拟内容是汇编代码，负责CPU上进程的切换。切换过程中，首先保存当前进程的状态，然后恢复新运行进程的状态，之后切换到新进程的栈空间，开始运行新进程。 code/machine/timer.h和code/machine/timer.cc：Timer类用以模拟硬件的时间中断。在TimerExired中，会调用TimeOfNextInterrupt，计算出下次时间中断的时间，并将中断插入中断队列中。初始化时会调用TimerExired，然后每次中断处理函数中都会调用一次TimerExired，从而时间系统时间一步步向前走。需要说明的是，在运行nachos时加入-rs选项，会初始化一个随机中断的Timer。当然你也可以自己声明一个非随机的Timer，每隔固定的时间片执行中断。时间片大小的定义位于ststs.h中，每次开关中断会调用OneTick()，当Ticks数目达到时间片大小时，会出发一次时钟中断。 Exercise3 线程调度算法扩展扩展线程调度算法，实现基于优先级的抢占式调度算法。 思路：更改Thread类，加入priority成员变量，同时更改初始化函数对其初始化，并完成对应的set和get函数。scheduler中的FindNextToRun负责找到下一个运行的进程，默认是FIFO，找到队列最开始时的线程返回。我们现在要实现的是根据优先级来返回，仅需将插入readyList队列的方法按照优先级从高到低顺序插入SortedInsert，那么插入时会维护队列中的Thread按照优先级排序，每次依旧从头取出第一个，即为优先级最高的队列。抢占式调度则需要在每次中断发生时尝试进行进程切换，如果有优先级更高的进程，则运行高优先级进程。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125// 基于优先级的可抢占式调度策略和FIFO调度策略Thread * Scheduler::FindNextToRun (bool bySleep)&#123; // called by threadsleep，直接调度，不用判断时间片 if(bySleep)&#123; lastSwitchTick = stats-&gt;systemTicks; return (Thread *)readyList-&gt;SortedRemove(NULL); // 与Remove()等价，都是从队头取 &#125;else&#123; int ticks = stats-&gt;systemTicks - lastSwitchTick; // 这里设置了运行的最短时间TimerSlice，防止频繁切换消耗CPU资源 // 测试优先级抢占调度时需要屏蔽这句，因为调用Yield()的线程运行时间很短 // 会直接返回NULL /*if(ticks &lt; TimerSlice)&#123; // 不用切换 return NULL; &#125;else*/&#123; if(readyList-&gt;IsEmpty())&#123; return NULL; &#125; Thread * next = (Thread *)readyList-&gt;SortedRemove(NULL);// 基于优先级可抢占调度策略,自己添加的宏，Makefile编译添加： -DSCHED_PRIORITY#ifdef SCHED_PRIORITY // nextThread优先级高于当前线程则切换，否则不切换 if(next-&gt;getPriority() &lt; currentThread-&gt;getPriority())&#123; lastSwitchTick = stats-&gt;systemTicks; return next; &#125;else&#123; readyList-&gt;SortedInsert(next, next-&gt;getPriority()); return NULL; &#125;#else // FIFO策略需要取消Makefile编译选项：-DSCHED_PRIORITY lastSwitchTick = stats-&gt;systemTicks; return next;#endif &#125; &#125;&#125;// 线程主动让出cpu,在FIFO调度策略下能够看到多个线程按顺序运行void SimpleThread(int which)&#123; for (int num = 0; num &lt; 5; num++) &#123; int ticks = stats-&gt;systemTicks - scheduler-&gt;getLastSwitchTick(); printf("userId=%d,threadId=%d,prio=%d,loop:%d,lastSwitchTick=%d,systemTicks=%d,usedTicks=%d,TimerSlice=%d\n",currentThread-&gt;getUserId(),currentThread-&gt;getThreadId(),currentThread-&gt;getPriority(),num,scheduler-&gt;getLastSwitchTick(),stats-&gt;systemTicks,ticks,TimerSlice); // 时间片轮转算法，判断时间片是否用完， // 如果用完主动让出cpu，针对nachos内核线程算法 /*if(ticks &gt;= TimerSlice)&#123; //printf("threadId=%d Yield\n",currentThread-&gt;getThreadId()); currentThread-&gt;Yield(); &#125;*/ // 非抢占模式下，多个线程同时执行该接口的话，会交替执行，交替让出cpu // 基于优先级抢占模式下，优先级高的线程运行结束后才调度低优先级线程 currentThread-&gt;Yield(); &#125;&#125;threadtest.cc:// 创建四个线程，加上主线程共五个，优先值越小优先级越高void ThreadPriorityTest()&#123; Thread* t1 = new Thread("forkThread1", 1); printf("--&gt;name=%s,threadId=%d\n",t1-&gt;getName(),t1-&gt;getThreadId()); t1-&gt;Fork(SimpleThread, (void*)1); Thread* t2 = new Thread("forkThread2", 2); printf("--&gt;name=%s,threadId=%d\n",t2-&gt;getName(),t2-&gt;getThreadId()); t2-&gt;Fork(SimpleThread, (void*)2); Thread* t3 = new Thread("forkThread3", 3); printf("--&gt;name=%s,threadId=%d\n",t3-&gt;getName(),t3-&gt;getThreadId()); t3-&gt;Fork(SimpleThread, (void*)3); Thread* t4 = new Thread("forkThread4", 4); printf("--&gt;name=%s,threadId=%d\n",t4-&gt;getName(),t4-&gt;getThreadId()); t4-&gt;Fork(SimpleThread, (void*)4); currentThread-&gt;Yield(); SimpleThread(0);&#125;// 运行结果，优先级1最高，最先执行完，其次是优先为2的线程，直到所有线程结束root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -q 3--&gt;name=forkThread1,threadId=1--&gt;name=forkThread2,threadId=2--&gt;name=forkThread3,threadId=3--&gt;name=forkThread4,threadId=4userId=0,threadId=1,prio=1,loop:0,lastSwitchTick=50,systemTicks=60,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=1,loop:1,lastSwitchTick=50,systemTicks=70,usedTicks=20,TimerSlice=30userId=0,threadId=1,prio=1,loop:2,lastSwitchTick=50,systemTicks=80,usedTicks=30,TimerSlice=30userId=0,threadId=1,prio=1,loop:3,lastSwitchTick=50,systemTicks=90,usedTicks=40,TimerSlice=30userId=0,threadId=1,prio=1,loop:4,lastSwitchTick=50,systemTicks=100,usedTicks=50,TimerSlice=30userId=0,threadId=2,prio=2,loop:0,lastSwitchTick=110,systemTicks=120,usedTicks=10,TimerSlice=30userId=0,threadId=2,prio=2,loop:1,lastSwitchTick=110,systemTicks=130,usedTicks=20,TimerSlice=30userId=0,threadId=2,prio=2,loop:2,lastSwitchTick=110,systemTicks=140,usedTicks=30,TimerSlice=30userId=0,threadId=2,prio=2,loop:3,lastSwitchTick=110,systemTicks=150,usedTicks=40,TimerSlice=30userId=0,threadId=2,prio=2,loop:4,lastSwitchTick=110,systemTicks=160,usedTicks=50,TimerSlice=30userId=0,threadId=3,prio=3,loop:0,lastSwitchTick=170,systemTicks=180,usedTicks=10,TimerSlice=30userId=0,threadId=3,prio=3,loop:1,lastSwitchTick=170,systemTicks=190,usedTicks=20,TimerSlice=30userId=0,threadId=3,prio=3,loop:2,lastSwitchTick=170,systemTicks=200,usedTicks=30,TimerSlice=30userId=0,threadId=3,prio=3,loop:3,lastSwitchTick=170,systemTicks=210,usedTicks=40,TimerSlice=30userId=0,threadId=3,prio=3,loop:4,lastSwitchTick=170,systemTicks=220,usedTicks=50,TimerSlice=30userId=0,threadId=4,prio=4,loop:0,lastSwitchTick=230,systemTicks=240,usedTicks=10,TimerSlice=30userId=0,threadId=4,prio=4,loop:1,lastSwitchTick=230,systemTicks=250,usedTicks=20,TimerSlice=30userId=0,threadId=4,prio=4,loop:2,lastSwitchTick=230,systemTicks=260,usedTicks=30,TimerSlice=30userId=0,threadId=4,prio=4,loop:3,lastSwitchTick=230,systemTicks=270,usedTicks=40,TimerSlice=30userId=0,threadId=4,prio=4,loop:4,lastSwitchTick=230,systemTicks=280,usedTicks=50,TimerSlice=30userId=0,threadId=0,prio=6,loop:0,lastSwitchTick=290,systemTicks=300,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=6,loop:1,lastSwitchTick=290,systemTicks=310,usedTicks=20,TimerSlice=30userId=0,threadId=0,prio=6,loop:2,lastSwitchTick=290,systemTicks=320,usedTicks=30,TimerSlice=30userId=0,threadId=0,prio=6,loop:3,lastSwitchTick=290,systemTicks=330,usedTicks=40,TimerSlice=30userId=0,threadId=0,prio=6,loop:4,lastSwitchTick=290,systemTicks=340,usedTicks=50,TimerSlice=30No threads ready or runnable, and no pending interrupts.Assuming the program completed.Machine halting!Ticks: total 350, idle 0, system 350, user 0Disk I/O: reads 0, writes 0Console I/O: reads 0, writes 0Paging: faults 0Network I/O: packets received 0, sent 0Cleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# Challenge 线程调度算法扩展（至少实现一种算法）可实现“时间片轮转算法”、“多级队列反馈调度算法”，或将Linux或Windows采用的调度算法应用到Nachos上。 思路：nachos启动时在system.cc中会new一个timer类，每隔一个TimerTicks大小触发时钟中断，从而让时钟向前走，时间片的大下定义在stats.h中。同时在stats.h中定义一个时间片大小变量TimerSlice，每个线程运行时间只要大于等于TimerSlice，立即放弃CPU。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135stats.h：……// nachos执行每条用户指令的时间为1Tick#define UserTick 1// 系统态无法进行指令计算，// 所以nachos系统态的一次中断调用或其他需要进行时间计算的单位设置为10Tick#define SystemTick 10// 磁头寻找超过一个扇区的时间#define RotationTime 500// 磁头寻找超过一个磁道的时间#define SeekTime 500#define ConsoleTime 100 // time to read or write one character#define NetworkTime 100 // time to send or receive one packet// 时钟中断间隔#define TimerTicks 5 // (average) time between timer interrupts// 时间片轮转算法一个时间片大小#define TimerSlice 10 ……threadtest.cc:void SimpleThread(int which)&#123; for (int num = 0; num &lt; 5; num++) &#123; int ticks = stats-&gt;systemTicks - scheduler-&gt;getLastSwitchTick(); printf("userId=%d,threadId=%d,prio=%d,loop:%d,lastSwitchTick=%d,systemTicks=%d,usedTicks=%d,TimerSlice=%d\n",currentThread-&gt;getUserId(),currentThread-&gt;getThreadId(),currentThread-&gt;getPriority(),num,scheduler-&gt;getLastSwitchTick(),stats-&gt;systemTicks,ticks,TimerSlice); // 时间片轮转算法，判断时间片是否用完 // 如果用完主动让出cpu，针对nachos内核线程算法 if(ticks &gt;= TimerSlice)&#123; printf("threadId=%d Yield\n",currentThread-&gt;getThreadId()); currentThread-&gt;Yield(); &#125; // 非抢占模式下，多个线程同时执行该接口的话，会交替执行，交替让出cpu // currentThread-&gt;Yield(); &#125;&#125;threadtest.cc:// 创建四个线程，加上主线程共五个，时间片轮转调度策略，不可抢占void ThreadPriorityTest()&#123; Thread* t1 = new Thread("forkThread1", 1); printf("--&gt;name=%s,threadId=%d\n",t1-&gt;getName(),t1-&gt;getThreadId()); t1-&gt;Fork(SimpleThread, (void*)1); Thread* t2 = new Thread("forkThread2", 2); printf("--&gt;name=%s,threadId=%d\n",t2-&gt;getName(),t2-&gt;getThreadId()); t2-&gt;Fork(SimpleThread, (void*)2); Thread* t3 = new Thread("forkThread3", 3); printf("--&gt;name=%s,threadId=%d\n",t3-&gt;getName(),t3-&gt;getThreadId()); t3-&gt;Fork(SimpleThread, (void*)3); Thread* t4 = new Thread("forkThread4", 4); printf("--&gt;name=%s,threadId=%d\n",t4-&gt;getName(),t4-&gt;getThreadId()); t4-&gt;Fork(SimpleThread, (void*)4); currentThread-&gt;Yield(); SimpleThread(0);&#125;// 运行结果，可看到usedTicks &gt;= TimserSlice时都让出cpu// 并且线程执行顺序为1 2 3 4 0，直到结束root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -q 3--&gt;name=forkThread1,threadId=1--&gt;name=forkThread2,threadId=2--&gt;name=forkThread3,threadId=3--&gt;name=forkThread4,threadId=4userId=0,threadId=1,prio=1,loop:0,lastSwitchTick=50,systemTicks=60,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:0,lastSwitchTick=60,systemTicks=70,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:0,lastSwitchTick=70,systemTicks=80,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:0,lastSwitchTick=80,systemTicks=90,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:0,lastSwitchTick=90,systemTicks=100,usedTicks=10,TimerSlice=10threadId=0 YielduserId=0,threadId=1,prio=1,loop:1,lastSwitchTick=100,systemTicks=110,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:1,lastSwitchTick=110,systemTicks=120,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:1,lastSwitchTick=120,systemTicks=130,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:1,lastSwitchTick=130,systemTicks=140,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:1,lastSwitchTick=140,systemTicks=150,usedTicks=10,TimerSlice=10threadId=0 YielduserId=0,threadId=1,prio=1,loop:2,lastSwitchTick=150,systemTicks=160,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:2,lastSwitchTick=160,systemTicks=170,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:2,lastSwitchTick=170,systemTicks=180,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:2,lastSwitchTick=180,systemTicks=190,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:2,lastSwitchTick=190,systemTicks=200,usedTicks=10,TimerSlice=10threadId=0 YielduserId=0,threadId=1,prio=1,loop:3,lastSwitchTick=200,systemTicks=210,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:3,lastSwitchTick=210,systemTicks=220,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:3,lastSwitchTick=220,systemTicks=230,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:3,lastSwitchTick=230,systemTicks=240,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:3,lastSwitchTick=240,systemTicks=250,usedTicks=10,TimerSlice=10threadId=0 YielduserId=0,threadId=1,prio=1,loop:4,lastSwitchTick=250,systemTicks=260,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:4,lastSwitchTick=260,systemTicks=270,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:4,lastSwitchTick=270,systemTicks=280,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:4,lastSwitchTick=280,systemTicks=290,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:4,lastSwitchTick=290,systemTicks=300,usedTicks=10,TimerSlice=10threadId=0 YieldNo threads ready or runnable, and no pending interrupts.Assuming the program completed.Machine halting!Ticks: total 350, idle 0, system 350, user 0Disk I/O: reads 0, writes 0Console I/O: reads 0, writes 0Paging: faults 0Network I/O: packets received 0, sent 0Cleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# 内容三：遇到的困难以及解决方法困难1切换线程过程中，产生段错误，通过定位，误把销毁的线程挂入就绪对了所致。 内容四：收获及感想自己动手实现后，发现时间片轮转算法，线程调度，FIFO，时钟中断等其实并不陌生。一切只要你不懒和肯付出实际行动的难题都是纸老虎。 内容五：对课程的意见和建议暂无。 内容六：参考文献暂无。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>时间片轮转调度策略</tag>
        <tag>FIFO线程调度策略</tag>
        <tag>基于优先级的可抢占式调度策略</tag>
        <tag>Nachos-3.4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nachos-Lab1-完善线程机制]]></title>
    <url>%2F2019%2F05%2F13%2Fnachos-3-4-Lab1%2F</url>
    <content type="text"><![CDATA[Nachos是什么Nachos (Not Another Completely Heuristic Operating System)，是一个教学用操作系统，提供了操作系统框架： 线程 中断 虚拟内存（位图管理所有物理页，虚拟地址与物理地址之间的转换等） 同步与互斥机制（锁、条件变量、信号量），读者写者问题，生产者消费者问题，BARRIER问题等 线程调度（基于优先级可抢占式调度，时间片轮转算法，FIFO调度） 文件系统 系统调用 机器指令、汇编指令、寄存器……Nachos模拟了一个MIPS模拟器，运行用户程序。 目录结构12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576.├── COPYRIGHT├── gnu-decstation-ultrix // 交叉编译工具链├── nachos-3.4.zip // 未经任何修改的源码和交叉编译工具，实验就是修改源码完善各个模块的功能├── README└── nachos-3.4 // 实验过程中完善的代码 ├── test // 该目录下编写用户自己的程序，需要修改Makfile添加自己的文件 ├── bin // 用户自己的程序需要利用coff2noff转换，才能在nachos下跑起来 ├── filesys // 文件系统管理 │ ├── directory.cc // 目录文件，由目录项组成，目录项里记录了文件头所在扇区号 │ ├── directory.h │ ├── filehdr.cc │ ├── filehdr.h // 文件头数据结构，通过索引记录了文件内容实际存储的所有扇区号 │ ├── filesys.cc // 文件系统数据结构，创建/删除/读/写/修改/重命名/打开/关别等接口 │ ├── filesys.h │ ├── fstest.cc │ ├── Makefile │ ├── openfile.cc // 管理所有打开的文件句柄 │ ├── openfile.h │ ├── synchdisk.cc // 同步磁盘类，加锁保证互斥和文件系统的一致性 │ ├── synchdisk.h │ └── test ├── machine // 机器硬件模拟 │ ├── console.cc // 终端 │ ├── console.h │ ├── disk.cc // 磁盘 │ ├── disk.h │ ├── interrupt.cc // 中断处理器，利用FIFO维护一个中断队列 │ ├── interrupt.h │ ├── timer.cc // 模拟硬件时钟，用于时钟中断 │ ├── timer.h │ ├── translate.cc // 用户程序空间虚拟地址和物理之间的转换类 │ └── translate.h ├── network // 网络系统管理 │ ├── Makefile │ ├── nettest.cc │ ├── post.cc │ ├── post.h │ └── README ├── threads // 内核线程管理 │ ├── list.cc // 工具模块 定义了链表结构及其操作 │ ├── list.h │ ├── main.cc // main入口，可以传入argv参数 │ ├── Makefile │ ├── scheduler.cc // 调度器，维护一个就绪的线程队列，时间片轮转/FIFO/优先级抢占 │ ├── scheduler.h │ ├── stdarg.h │ ├── switch.c // 线程启动和调度模块 │ ├── switch.h │ ├── switch-old.s │ ├── switch.s // 线程切换 │ ├── synch.cc // 同步与互斥，锁/信号量/条件变量 │ ├── synch.dis │ ├── synch.h │ ├── synchlist.cc // 类似于一个消息队列 │ ├── synchlist.h │ ├── system.cc // 主控模块 │ ├── system.h │ ├── thread.cc // 线程数据结构 │ ├── thread.h │ ├── threadtest.cc │ ├── utility.cc │ └── utility.h ├── userprog // 用户进程管理 │ ├── addrspace.cc // 为noff文件的代码段/数据段分配空间，虚拟地址空间 │ ├── addrspace.h │ ├── bitmap.cc // 位图，用于管理扇区的分配和物理地址的分配 │ ├── bitmap.h │ ├── exception.cc // 异常处理 │ ├── Makefile │ ├── progtest.cc // 测试nachos是否可执行用户程序 │ └── syscall.h // 系统调用 └── vm // 虚拟内存管理 └── Makefile // 多线程编译: make -j4 └── Makefile.common // 各个模块公共的Makefile内容存放到这里面 └── Makefile.dep // 依赖 环境选择Linux或Unix系统，安装32位GCC开发环境，安装32的ubuntu。 源码获取https://github.com/icoty/nachos-3.4-Lab 内容一：总体概述本次Lab针对的内容是实现线程机制最基本的数据结构——进程控制块（PCB）。当一个进程创建时必然会生成一个相应的进程控制块，记录一些该线程特征，如进程ID、进程状态、进程优先级，进程开始运行时间，在cpu上已经运行了多少时间，程序计数器，SP指针，根目录和当前目录指针，文件描述符表，用户ID，组ID，指向代码段、数据段和栈段的指针等（当然，Nachos简化了进程控制块的内容）。实验的主要内容是修改和扩充PCB，主要难点在于发现修改PCB影响到的文件并进行修改。PCB是系统感知进程存在的唯一标志，且进程与PCB一一对应。可将PCB内部信息划分为：进程描述信息，进程控制信息，进程占有的资源和使用情况，进程的cpu现场。扩展字段如下： 内容二：任务完成情况任务完成列表（Y/N） Exercise1 Exercise2 Exercise3 Exercise4 第一部分 Y Y Y Y 具体Exercise的完成情况Exercise1 调研调研Linux或Windows中进程控制块（PCB）的基本实现方式，理解与Nachos的异同。 linux-4.19.23调研：Linux中的每一个进程由一个task_struct数据结构来描述。task_struct也就是PCB的数据结构。task_struct容纳了一个进程的所有信息，linux内核代码中的task_struct在linux-4.19.23/include/linux/sched.h内。 Linux内核进程状态：如下可分为运行态，可中断和不可中断态，暂停态，终止态，僵死状态，挂起状态等。 Linux内核进程调度：sched_info数据结构，包括被调度次数，等待时间，最后一次调度时间。vi linux-4.19.23/include/linux/sched.h： 123456789101112131415161718192021222324252627282930313233343536373839404142……/* Used in tsk-&gt;state: */#define TASK_RUNNING 0x0000 // 运行态#define TASK_INTERRUPTIBLE 0x0001 // 可中断#define TASK_UNINTERRUPTIBLE 0x0002 // 不可中断#define __TASK_STOPPED 0x0004 #define __TASK_TRACED 0x0008/* Used in tsk-&gt;exit_state: */#define EXIT_DEAD 0x0010#define EXIT_ZOMBIE 0x0020 // 僵死态#define EXIT_TRACE (EXIT_ZOMBIE | EXIT_DEAD)/* Used in tsk-&gt;state again: */#define TASK_PARKED 0x0040#define TASK_DEAD 0x0080#define TASK_WAKEKILL 0x0100#define TASK_WAKING 0x0200#define TASK_NOLOAD 0x0400#define TASK_NEW 0x0800#define TASK_STATE_MAX 0x1000…………struct sched_info &#123;#ifdef CONFIG_SCHED_INFO /* Cumulative counters: */ /* # of times we have run on this CPU: */ unsigned long pcount; /* Time spent waiting on a runqueue: */ unsigned long long run_delay; /* Timestamps: */ /* When did we last run on a CPU? */ unsigned long long last_arrival; /* When were we last queued to run? */ unsigned long long last_queued;#endif /* CONFIG_SCHED_INFO */&#125;;…… 时钟与锁：内核需要记录进程在其生存期内使用CPU的时间以便于统计、计费等有关操作。进程耗费CPU的时间由两部分组成：一是在用户态下耗费的时间，一是在系统态下耗费的时间。这类信息还包括进程剩余的时间片和定时器信息等，以控制相应事件的触发。 文件系统信息：进程可以打开或关闭文件，文件属于系统资源，Linux内核要对进程使用文件的情况进行记录。 虚拟内存信息：除了内核线程，每个进程都拥有自己的地址空间，Linux内核中用mm_struct结构来描述。 物理页管理信息：当物理内存不足时，Linux内存管理子系统需要把内存中部分页面交换到外存，并将产生PageFault的地址所在的页面调入内存，交换以页为单位。这部分结构记录了交换所用到的信息。 多处理器信息：与多处理器相关的几个域，每个处理器都维护了自己的一个进程调度队列，Linux内核中没有线程的概念，统一视为进程。 处理器上下文信息：当进程因等待某种资源而被挂起或停止运行时，处理机的状态必须保存在进程的task_struct，目的就是保存进程的当前上下文。当进程被调度重新运行时再从进程的task_struct中把上下文信息读入CPU（实际是恢复这些寄存器和堆栈的值），然后开始执行。 与Nachos的异同：Nachos相对于Linux系统的线程部分来讲，要简单许多。它的PCB仅有几个必须的变量，并且定义了一些最基本的对线程操作的函数。Nachos线程的总数目没有限制，线程的调度比较简单，而且没有实现线程的父子关系。很多地方需要完善。 Exercise2 源代码阅读code/threads/main.cc：main.cc是整个nachos操作系统启动的入口，通过它可以直接调用操作系统的方法。通过程序中的main函数，配以不同的参数，可以调用Nachos操作系统不同部分的各个方法。 code/threads/threadtest.cc：nachos内核线程测试部分，Fork两个线程，交替调用Yield()主动放弃CPU，执行循环体，会发现线程0和线程1刚好是交替执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566int main(int argc, char **argv)&#123; int argCount; // the number of arguments DEBUG('t', "Entering main"); (void) Initialize(argc, argv);#ifdef THREADS for (argc--, argv++; argc &gt; 0; argc -= argCount, argv += argCount) &#123; argCount = 1; switch (argv[0][1]) &#123; case 'q': testnum = atoi(argv[1]); argCount++; break; case 'T': if(argv[0][2] == 'S') testnum = 3; break; default: testnum = 1; break; &#125; &#125; ThreadTest();#endif ……&#125;threadtest.cc// 线程主动让出cpu,在FIFO调度策略下能够看到多个线程按顺序运行void SimpleThread(int which)&#123; for (int num = 0; num &lt; 5; num++) &#123; int ticks = stats-&gt;systemTicks - scheduler-&gt;getLastSwitchTick(); // 针对nachos内核线程的时间片轮转算法，判断时间片是否用完，如果用完主动让出cpu if(ticks &gt;= TimerSlice)&#123; currentThread-&gt;Yield(); &#125; // 多个线程同时执行该接口的话，会交替执行，交替让出cpu // currentThread-&gt;Yield(); &#125;&#125;root@yangyu-ubuntu-32:/mnt/nachos-3.4/code/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4/code/threads# ./nachos -q 1userId=0,threadId=0,prio=5,loop:0,lastSwitchTick=0,systemTicks=20,usedTicks=20,TimerSlice=30userId=0,threadId=1,prio=5,loop:0,lastSwitchTick=20,systemTicks=30,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=5,loop:1,lastSwitchTick=30,systemTicks=40,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=5,loop:1,lastSwitchTick=40,systemTicks=50,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=5,loop:2,lastSwitchTick=50,systemTicks=60,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=5,loop:2,lastSwitchTick=60,systemTicks=70,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=5,loop:3,lastSwitchTick=70,systemTicks=80,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=5,loop:3,lastSwitchTick=80,systemTicks=90,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=5,loop:4,lastSwitchTick=90,systemTicks=100,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=5,loop:4,lastSwitchTick=100,systemTicks=110,usedTicks=10,TimerSlice=30No threads ready or runnable, and no pending interrupts.Assuming the program completed.Machine halting!Ticks: total 130, idle 0, system 130, user 0Disk I/O: reads 0, writes 0Console I/O: reads 0, writes 0Paging: faults 0Network I/O: packets received 0, sent 0Cleaning up... code/threads/thread.h：这部分定义了管理Thread的数据结构，即Nachos中线程的上下文环境。主要包括当前线程栈顶指针，所有寄存器的状态，栈底，线程状态，线程名。当前栈指针和机器状态的定义必须必须放作为线程成员变量的前两个，因为Nachos执行线程切换时，会按照这个顺序找到线程的起始位置，然后操作线程上下文内存和寄存器。在Thread类中还声明了一些基本的方法，如Fork()、Yield()、Sleep()等等，由于这些方法的作用根据名字已经显而易见了，在此不再赘述。 code/threads/thread.cc： Thread.cc中主要是管理Thread的一些事务。主要接口如下： Fork(VoidFunctionPtr func,int arg)：func是新线程运行的函数，arg是func函数的入参，Fork的实现包括分为几步：分配一个堆栈，初始化堆栈，将线程放入就绪队列。 Finish()：不是直接收回线程的数据结构和堆栈，因为当前仍在这个堆栈上运行这个线程。先将threadToBeDestroyed的值设为当前线程，在Scheduler的Run()内切换到新的线程时在销毁threadToBeDestroyed。Yield()、Sleep()。这里实现的方法大多是都是原子操作，在方法的一开始保存中断层次关闭中断，并在最后恢复原状态。 Yield()：当前线程放入就绪队列，从scheduler就绪队列中的找到下一个线程上cpu，以达到放弃CPU的效果。 Exercise3 扩展线程的数据结构Exercise4 增加全局线程管理机制这里我把Exercise3和Exercise4放在一起完成。 在Thread类中添加私有成员userId和threadId，添加公有接口getUserId()和getThreadId()，userId直接沿用Linux个getuid()接口。 system.h内部添加全局变量maxThreadsCount=128，全局数组threads[maxThreadsCount]，每创建一个线程判断并分配threadId。 -TS模仿Linux的PS命令打印所有线程信息，仔细阅读list.cc代码和scheduler.cc的代码，就会发现可以直接用scheduler.cc::Print()接口，不用我们重新造轮子。 在system.cc中的void Initialize(int argc, char argv)函数体对全局数组初始化。如下我用root用户执行分配的userId为0，切换到其他用户userId会发生变化，线程id分别为0和1。当线程数超过128个线程时，ASSERT断言报错。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455threadtest.cc：void ThreadTest()&#123; switch (testnum) &#123; case 1: ThreadTest1(); break; case 2: ThreadCountLimitTest(); break; case 3: ThreadPriorityTest(); break; case 4: ThreadProducerConsumerTest(); break; case 5: ThreadProducerConsumerTest1(); break; case 6: barrierThreadTest(); break; case 7: readWriteThreadTest(); break; default: printf("No test specified.\n"); break; &#125;&#125;// 线程最多128个，超过128个终止运行void ThreadCountLimitTest()&#123; for (int i = 0; i &lt;= maxThreadsCount; ++i) &#123; Thread* t = new Thread("fork thread"); printf("thread name = %s, userId = %d, threadId = %d\n", t-&gt;getName(), t-&gt;getUserId(), t-&gt;getThreadId()); &#125;&#125;root@yangyu-ubuntu-32:/mnt/nachos-3.4/code/threads# ./nachos -TSthread name = fork thread, userId = 0, threadId = 1thread name = fork thread, userId = 0, threadId = 2thread name = fork thread, userId = 0, threadId = 3……thread name = fork thread, userId = 0, threadId = 122thread name = fork thread, userId = 0, threadId = 123thread name = fork thread, userId = 0, threadId = 124thread name = fork thread, userId = 0, threadId = 125thread name = fork thread, userId = 0, threadId = 126thread name = fork thread, userId = 0, threadId = 127allocatedThreadID fail, maxThreadsCount:[128]Assertion failed: line 73, file "../threads/thread.cc"Aborted (core dumped)root@yangyu-ubuntu-32:/mnt/nachos-3.4/code/threads# 内容三：遇到的困难以及解决方法困难1开始make编译出错，通过定位到具体行，复制出来手动执行，发现是gcc交叉编译工具链路径不对。 困难2刚开始修改代码验证效果，重定义错误，外部文件全局变量使用方式不对导致。 内容四：收获及感想动手实践很重要，不管你是做什么事、什么项目、什么作业，一定要落实到代码和跑到程序上面来。绝知此事要躬行，学习来不得半点虚假。 内容五：对课程的意见和建议暂无。 内容六：参考文献暂无。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Nachos-3.4</tag>
        <tag>PCB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[另类P、V操作问题-详细图解]]></title>
    <url>%2F2019%2F04%2F25%2Fstack-pv%2F</url>
    <content type="text"><![CDATA[问题模型有一个系统，定义如下P、V操作：123456789P(s)： s.count--; if s&lt;0 then 将本进程插入相应队列末尾等待; V(s): s.count++; if s&lt;=0 then 从相应等待队列队尾唤醒一个进程，将其插入就绪队列; 思考并回答:a. 这样定义P、V操作是否有问题?b. 试用这样的P、V操作实现N个进程竞争使用某一共享变量的互斥机制。c. 对于b的解法，有无效率更高的方法。如有，试问降低了多少复杂性? 分析a. 当然有问题，假设s=2，现有进程p1、p2按顺序来请求共享资源A，p1和p2直接获取A，假设p1和p2都还未释放A的时候，p3、p4、p5按顺序也来请求A，这时s的等待队列L为：(尾)p5p4p3(头)，然后p1释放A，执行V(s)操作从L队尾唤醒p5，L变为：(尾)p4p3(头)。这时A被p2和p5持有，且p2和p5都未释放A的时候，假设这时p1又来请求A，p1被挂起，L变为：(尾)p1p4p3(头)。然后p2释放A执行V(s)操作从L队尾唤醒p1，你会发现p1又竞争到了A，而p3和p4还一次都未竞争到，这会导致越靠近L队首的p3和p4越容易饿死，出现饥饿现象。问题的根源就在于这样定义的P、V操作，由于在信号量的等待队列上是先进后出导致的，这属于栈P、V。 b. 解决方案这里以N个进程为例进行一般化分析，定义信号量数组S[N-1]，共有N-1个信号量，下标从0～N-2，其中S[i] = N-i-1，表示第i+1个信号量S[i]的初值为N-i-1，初值为何取这个看后面分析，下为伪码。 123456789Semaphore S[N-1]; // S[i] = N-i-1void func()&#123; for(int i=0 ; i&lt;n-1 ; i++) P(S[i]); // 临界区 Critical Section for(int i=n-2 ; i&gt;=0 ; i--) V(S[i]);&#125; 一定要注意P(S[i])操作中的i是从0～N-2，而V(S[i])的i是反过来的从N-2～0，这个很重要，这个就是多级队列的精髓，顺序不能换。下面的分析，假设t1时刻p1进入临界区还没出来之前，t2～tN时刻p2～pN按顺序来请求进入临界区，那么p2～pN都执行for循环，分别被挂起在信号量N-2～0的等待队列上，并且每个信号量的等待队列上有且只有一个进程被挂起。在tN+1时刻p1出临界区，由于V(S[i])是从N-2～0，因此等待在LN-2上的P2最先被唤醒，然后L2进入临界区。之后按顺序p3～pN依次被唤醒并依次挂入就绪队列等待被调度，而处理器从就绪队列进行调度是FIFO，与请求临界区的顺序一致，饥饿现象得以解决。 该方法的资源复杂度为O(N-1)，需要N-1个信号量。 c. 优化方法除了前面的办法，已经可以确定存在更优方案能把资源复杂度降为O(logN)。]]></content>
      <categories>
        <category>同步机制</category>
        <category>互斥机制</category>
      </categories>
      <tags>
        <tag>信号量</tag>
        <tag>临界区</tag>
        <tag>P/V操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker最简教程]]></title>
    <url>%2F2019%2F04%2F22%2Fdocker%2F</url>
    <content type="text"><![CDATA[本文旨在让你用最短的时间弄懂Docker命令操作，什么虚拟化都太泛泛了，不讲大道理，实践出真知，让你从此的日常开发和工作中在面对Docker时不再茫然失措而是得心应手。本文也不谈安装，我实在认为作为程序员，要是我在这里教你如何安装Docker，既是在浪费你的时间也是在浪费我的时间，请参考Docker安装； Docker Hub是Docker官方维护的一个公共仓库，其中已经包括了数量超过15 000 的镜像，开发者可以注册自己的账号，并自定义自己的镜像进行存储，需要的时候可以直接拿来用，同时也能够分享，有点类似于Github，如想注册可移步 Docker Hub，注册与否不影响接下来的操作。 实践出真知我认为只要你不是专门研究这个的，那么你只需学会如何使用Docker的一些基本命令，使自己的日常开发和工作不受阻碍，弄清Docker和容器之间的区别，为什么现在很多企业流行Docker，这个东西解决了啥问题，有啥优势就够了。关于Docker是什么有何优势，这里挑了一个简洁的博客链接。 接下来从零开始，首先从docker hub仓库上拉取centos镜像，带你走进docker日常命令，学会这些命令，足以应对你的日常开发和工作中关于docker的问题。 search: 从docker hub仓库搜索带centos的所有镜像。1$docker seach centos images: 查看本地所有镜像，pull前左侧REPOSITORY栏无centos。REPOSITORY表示镜像被归入到本地的仓库，比如icoty1/lamp:v1.0.0表示本地icoty1仓库下有一个镜像名为lamp,其TAG(版本)为v1.0.0，每个镜像有一个IMAGE ID唯一标识该镜像，SIZE为镜像大小。1$docker images pull: 从docker hub远程仓库把centos镜像拉到本地，pull后再次执行images，会发现centos已经被拉到本地。1$docker pull centos ps: 只列出正在运行的容器。1$docker ps ps -a: 列出所有容器, 每一个容器有一个CONTAINER ID唯一标识该容器；IMAGE表示该容器是基于哪个镜像生成的，COMMAND是容器启动时执行的命令，可以传入也可以不传入；STATUS是容器当前的状态，Exit是已停止，Up是正在运行。1$docker ps -a run: 从镜像衍生一个新的容器并运行；-d后台模式运行容器，-i交互模式运行容器；-p把主机80关口映射到容器的80端口，因为容器具有封闭性，容器外部不能直接访问容器内部的端口，通过映射后，主机80端口收到数据后会转发到容器内部的80端口，不过在容器内是可以直接访问容器外的主机的；-v把主机的/Users/yangyu/ide/LeetCode/目录映射到容器的/LeetCode/，容器内若无/LeetCode/目录会自动创建，用于实现主机和容器之间的目录共享，在两个目录下操作文件是对等的；centos:latest是镜像名称，可以换成IMAGE ID，二者等价；/bin/bash是容器启动时执行的命令，还可以带参数，这个不懂的可以搜索下。执行docker run后再次执行ps命令，能够看到运行中的容器多了一个。1$docker run -d -ti -p 80:80 -v /Users/yangyu/ide/LeetCode/:/LeetCode/ centos:latest /bin/bash cp: 拷贝主机/Users/yangyu/ide/LeetCode/Database/目录到容器eaf43b370eb7根目录。1$docker cp /Users/yangyu/ide/LeetCode/Database/ eaf43b370eb7:/ exec: 执行该命令进入容器eaf43b370eb7内，进入容器后在容器内/LeetCode/目录下新建readme.py2，在主机/Users/yangyu/ide/LeetCode/目录下能够看到该文件。12$docker exec -it eaf43b370eb7 /bin/bash$cp /LeetCode/readme.py /LeetCode/readme.py2 cp: 从容器eaf43b370eb7根目录下拷贝目录/Database到主机的/Users/yangyu/ide/LeetCode/Database/目录下。1$docker cp eaf43b370eb7:/Database/ /Users/yangyu/ide/LeetCode/Database/ stop/restart: 停止容器eaf43b370eb7然后查看其状态变为Exited；然后通过restart命令重启，容器又处于运行态。12$docker stop eaf43b370eb7$docker restart eaf43b370eb7 tag: 把centos镜像归入icoty1仓库下名称为centos，TAG为7，TAG随你定。1$docker tag centos icoty1/centos:7 commit: 最初pull下来的centos镜像是最简版本，里面没有安装mysql vim等；最初pull下来后基于其运行一个容器，你在容器内部可以安装你需要的环境，比如mysql，apache，nginx，hexo博客等，安装好后通过commit命令把容器提交为一个新的镜像，以后凡是从这个新的镜像运行的容器都带有你安装的内容。-m是提交说明；-p是执行commit命令时暂停该容器；eaf43b370eb7是容器ID。1$docker commit -m "提交我的自定义镜像，centos7内安装mysql，版本号v1.0.0" -p eaf43b370eb7 icoty1/centos7-mysql:v1.0.0 save: 把镜像03cb190015bf打包成主机目录/Users/yangyu/ide/LeetCode/下的centos7-mysql.tar，然后你可以通过U盘拷贝到其他机器上，在其他机器上通过load命令可以把centos7-mysql.tar加载成一个镜像。1$docker save 03cb190015bf &gt; /Users/yangyu/ide/LeetCode/centos7-mysql.tar load: 把centos7-mysql.tar加载为镜像，因为这个包是从我主机上的镜像03cb190015bf打出来的，所以执行load的时候直接返回镜像03cb190015bf，如果在其他机器上执行会生成一个新的镜像ID。1$docker load &lt; /Users/yangyu/ide/LeetCode/centos7-mysql.tar push: 把本地icoty1仓库下TAG为v1.0.0的镜像icoty1/centos7-mysql推到远程仓库docker hub上的icoty1仓库下保存，执行push前需要在本地icoty1已经登陆。push成功之后，其他人就可以通过pull命令拉取你的镜像使用了，相当于git clone操作。12$docker push icoty1/centos7-mysql:v1.0.0$docker pull icoty1/centos7-mysql:v1.0.0 rm: 删除容器eaf43b370eb7，运行中的容器无法删除。1$docker rm eaf43b370eb7 rmi: 删除镜像03cb190015bf，在这之前必须删除由该镜像衍生出来的所有容器删除，否则会删除失败，执行该命令后通过images发现镜像已经没有了。1$docker rmi 03cb190015bf build: 如下以我搭建hexo博客的Dockerfile举例说明。12345678910111213141516171819202122232425# 基础镜像，icoty1/ubuntu-hexo-blog:latest在本地仓库必须已经存在FROM icoty1/ubuntu-hexo-blog:latest# 维护人员信息，可写可不写MAINTAINER icoty1 "https://icoty.github.io" # 暴露容器的4000端口，这样主机就可以映射端口到4000了EXPOSE 4000/*自动安装所需环境，可替换成安装mysql vim等你需要的命令 *hexo部分插件安装，使支持rss，图片，字数统计等功能 */RUN npm install -g hexo-cli \&amp;&amp; npm install hexo-server --save \&amp;&amp; hexo init blog &amp;&amp; cd /blog \&amp;&amp; npm install \&amp;&amp; npm install hexo-deployer-git --save \&amp;&amp; npm install hexo-migrator-rss --save \ &amp;&amp; npm install hexo-asset-image --save \&amp;&amp; npm install hexo-wordcount --save \&amp;&amp; npm install hexo-generator-sitemap --save \ &amp;&amp; npm install hexo-generator-baidu-sitemap --save \ &amp;&amp; npm install hexo-helper-live2d --save \&amp;&amp; git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia \&amp;&amp; sed "s/theme: landscape/theme: yilia/g" -i /blog/_config.yml 12$ docker build -t icoty1/ubuntu-hexo . # icoty1/ubuntu-hexo是新的镜像的名字$ docker images # build后会多出icoty1/ubuntu-hexo镜像 镜像与容器为了便于理解，你可以把镜像理解成一个初始模版A，通过这个模板A你可以复制出模板B、模板C等，模板B和模板C在这里就相当于容器，突然某一天你发现模板A现有的内容已经不能满足你的需求了（比如模板A没有安装Mysql，而你需要安装Mysql），这时你就只能自定义新的模板(相当于自定义新的符合你的要求的镜像)，而自定义方式则可以从模板B或模板C中安装Mysql，安装成功之后，通过docker commit命令将模板B或模板C提交成一个新的初始模板A1（也就是新的镜像），以后所有从模板A1运行的容器就都有Mysql了，然后你就有模板A和模板A1了（就是两个镜像）。 建议实际操作部分，对各个命令有疑问的，相信我，直接执行一遍才是解决你心中疑虑的不二之法，如果你的命令参数不正确，顶多就是报错和执行不成功，不会让你的主机崩溃，最坏也就不过重新执行一遍，IT这个职业，其本身就是一个不断试错、犯错和总结经验的过程，如果你学到了，请我喝奶茶吧，小生会一直奋斗在原创的路上。 参考文献Docker命令Docker中文社区]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
        <tag>镜像</tag>
        <tag>容器</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github博客最简教程-Dockerfile自动搭建]]></title>
    <url>%2F2019%2F04%2F18%2Fdocker-hexo-blog%2F</url>
    <content type="text"><![CDATA[闲谈拿出你的气质，打开你的电脑，借你半小时搭建好属于你的hexo博客，小生用dockerfile自动帮你搭建好；你只需要在你的mac或linux或windows上提前把docker安装好，如何安装不是我的重点，请参考Docker安装；作为程序员，博客就像你的影子，我都已经忘了内心曾经多少次告诫自己，一定要搭建一个属于自己的技术博客，奈何日复一日过去了，近来终于落实到行动上来，所谓明日复明日，明日何其多，早晚要做的事，劝君晚做不如早做。 搭建Hexo获取基础镜像Docker安装成功之后方能进行接下来的操作，如果对Docker基本命令不熟悉又想真懂的可以看下我的另一篇文章Docker最简教程。首先从我的Docker Hub仓库上获取基础镜像： 12$ docker pull icoty1/ubuntu-hexo-blog # 从Docker hub上的icoty1用户下获取基础镜像$ docker images # 查看本地所有镜像，会发现icoty1/ubuntu-hexo-blog已经被pull下来了 生成Dockerfile进入一个空的目录下新建空文件Dockerfile，复制如下内容： 123456789101112131415161718192021# 基础镜像FROM icoty1/ubuntu-hexo-blog:latestMAINTAINER icoty1 "https://icoty.github.io" EXPOSE 4000# hexo部分插件安装，使支持rss，图片，字数统计等功能RUN npm install -g hexo-cli \&amp;&amp; npm install hexo-server --save \&amp;&amp; hexo init blog &amp;&amp; cd /blog \&amp;&amp; npm install \&amp;&amp; npm install hexo-deployer-git --save \&amp;&amp; npm install hexo-migrator-rss --save \&amp;&amp; npm install hexo-asset-image --save \&amp;&amp; npm install hexo-wordcount --save \&amp;&amp; npm install hexo-generator-sitemap --save \&amp;&amp; npm install hexo-generator-baidu-sitemap --save \&amp;&amp; npm install hexo-helper-live2d --save \&amp;&amp; git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia \&amp;&amp; sed "s/theme: landscape/theme: yilia/g" -i /blog/_config.yml 更换主题Dockerfile中的最后两行内容表示的含义是从github上把hexo-theme-yilia克隆下来并重命名成yilia，然后放到容器的/blog/themes/目录下，其中hexo-theme-yilia是hexo的主题，hexo有很多种主题，用每一种主题搭建出来的hexo博客界面美观和布局都不尽相同，你可以通过hexo官网上浏览每一种主题长啥样子，通过github获取主题的源码仓库，选择一个你喜欢的主题，并相应的修改这两行。假如你从github选择的主题仓库地址是https://github.com/yscoder/hexo-theme-indigo.git ， 那么你需要按照如下方式进行修改，如果你就想用yilia，那么你不需要做任何修改，我用的主题是https://github.com/theme-next/hexo-theme-next.git 12&amp;&amp; git clone https://github.com/yscoder/hexo-theme-indigo.git themes/indigo \&amp;&amp; sed "s/theme: landscape/theme: indigo/g" -i /blog/_config.yml 构建Hexo镜像在Dockerfile的同级目录执行： 12$ docker build -t icoty1/ubuntu-hexo . # 把icoty1/ubuntu-hexo替换成你取的名字$ docker images # 能够看到多出一条记录icoty1/ubuntu-hexo，并能看到该镜像的[IMAGE ID] 启动容器123456789/* 把[IMAGE ID]替换成上一步构建出来的镜像的ID，该句执行成功会多出来一个容器并有一个[CONTAINER ID] * -v /home/yangyu/blog/：/blog/是把本机的/home/yangyu/blog/目录映射到容器的/blog/目录 * 通过目录映射，你只需要在本机编辑/home/yangyu/blog/目录下的文件，而不用每次都进入容器/blog/目录下编辑文件 * -p 4000:4000 将主机的4000端口映射到容器的4000端口 *\$ docker run -d -ti -p 4000:4000 -v /home/yangyu/blog/：/blog/ [IMAGE ID] /bin/bash $ docker ps -a # 执行该句列出当前所有的容器$ docker exec -it [CONTAINER ID] /bin/bash # 根据前一步的容器ID进入该容器内部$ cd /blog/ &amp;&amp; hexo s # 进入容器内部的/blog/目录下，启动hexo 浏览器测试浏览器访问http://localhost:4000 ，出现下图说明已经成功，以后你的博客配置，文章撰写和发布等，都在/home/yangyu/blog/目录下进行，这和在容器内部/blog/目录下操作完全对等。 Hexo部署到Github注册Github账户，如果已经注册，跳过此步；在github上仓库“用户名.github.io”，比如我的用户名为icoty，仓库名则为：icoty.github.io； 执行如下命令生成ssh key，执行完后复制~/.sshid_rsa.pub文件内的全部内容，按照图示添加ssh keys，并粘贴保存到Key栏中，Title栏随便取。 1234$ cd ~/.ssh$ ssh-keygen -t rsa -C "youremail@example.com" # 全程回车$ git config --global user.name "你用github用户名"$ git config --global user.email "你的github邮箱地址" 配置Hexo主题编辑/blog/_config.yml文件，编辑标题、描述信息、Github信息，下图参见我的： 1234567891011121314# Sitetitle: 阳光沥肩头 仿佛自由人 # 标题subtitle: # 子标题description: Linux C++服务端 # 描述信息keywords: author: icotylanguage: zh-CN # 语言timezone: # 时区deploy: - type: git repository: git@github.com:icoty/icoty.github.io.git # 设置repository对应的链接 branch: master # 设置提交到的分支 message: Site updated at &#123;&#123; now("YYYY-MM-DD HH:mm:ss") &#125;&#125; # 设置我们提交的信息 执行如下命令发布到github上，通过“https://你的github用户名.github.io”访问，我的是https://icoty.github.io 12$hexo generate$hexo deploy # 部署到GitHub 编辑/blog/themes/yilia/_config.yml文件，自定义其他配置，如友链、评论、分享、头像等，这些配置并不是一定要做，做不做都行，只是配置的完善些，你的Hexo博客界面看起来美观些，如何配置在此不一一赘述，请自行查看对应主题的官方文档和Github说明。如果你能操作这里，说明我这个教程还是有效的，感谢你的坚持！ Hexo命令Hexo搭建好后，你可以写博客发布到GitHub 上，别人通过“https://你的github用户名.github.io”就能访问你的博客和看到你写的文章，而这个章节就是教你怎么在本地写你的博客，写博客用的MarkDown语法，推荐你安装MarkDown编辑器Typora。下面列出写博客过程中常用的命令，这些命令都需要走到/blog/目录下执行。 123456789$hexo new "my-hexo" #新建my-hexo文章，在/blog/source/_post/目录下生成my-hexo.md，在这个文件里面写你的文章$hexo generate # 文章写好后保存，然后执行这条命令，生成静态页面至public目录$hexo s # 然后开启预览访问端口（默认端口4000，'ctrl+c'关闭server，‘ctrl+z’放到后台运行），通过http://localhost:4000 查看效果，如果满意就执行下一条命令发布到github$hexo deploy # 发布到github，通过https://你用github用户名.github.io 访问$hexo clean # 有时你写文章和配置其他内容后，老是不生效，就执行下该命令清除缓存文件 (db.json) 和已生成的静态文件 (public)，不是删除，你的文章仍然在的$nohup hexo s &amp; # 启动hexo以后台方式运行$hexo new page "About" #新建About页面，这个是配置Hexo界面多出来一个About布局$hexo help # 查看帮助$hexo version #查看Hexo的版本 MarkDown语法这个比较基础，网上教程也一大堆，MarkDown很容易学，放心比九九表容易多了，只要你用markdown实际动手写过一篇博文后就上手了，因此没啥可说的。 Next主题配置接下来的内容是针对next主题的配置，因为我选择的是next，不同主题可能有差异，特此说明。 修改文章内链接文本样式打开themes/next/source/css/_common/components/post/post.styl文件，在文件最后且在@import之前添加如下代码：1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; //原始链接颜色 border-bottom: none; border-bottom: 1px solid #0593d3; //底部分割线颜色 &amp;:hover &#123; color: #fc6423; //鼠标经过颜色 border-bottom: none; border-bottom: 1px solid #fc6423; //底部分割线颜色 &#125;&#125; 文章末尾添加“文章结束”标记在themes/next/layout/_macro/目录下新建passage-end-tag.swig，填入如下内容：12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt;-------------本文结束&lt;i class="fa fa-paw"&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 然后编辑themes/next/layout/_macro/post.swig，按照下图添加代码块：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 最后编辑themes/next/_config.yml，添加如下内容：123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 添加网页加载进度条打开themes/next/_config.yml，搜索“pace:”，设置为true。1pace: true 设置文章的显示顺序编辑node_modules/hexo-generator-index/lib/generator.js，在return之前添加如下代码：12345678910111213posts.data = posts.data.sort(function(a, b) &#123;if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排&#125;else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1;&#125;else if(!a.top &amp;&amp; b.top) &#123; return 1;&#125;else return b.date - a.date; // 都没定义按照文章日期降序排&#125;) 然后在每篇文章的头部添加top字段，top值越大的文章显示越靠前。12345678---title: Hexo+Github博客最简教程-Dockerfile自动搭建date: 2019-04-18 15:23:05top: 6tags: [Hexo, Dockerfile, Linux, Github]categories: [IDE]copyright: ture--- 添加底部的小图标打开themes/next/layout/_partials/footer.swig搜索with-love，修改为如下代码。从fontawesom选择你喜欢的图标名称，我这里选择的是heart。123&lt;span class="with-love" id="animate"&gt; &lt;i class="fa fa-heart" aria-hidden = "true"&gt;&lt;/i&gt;&lt;/span&gt; 文章底部添加版权信息在themes/next/layout/_macro/下新建 my-copyright.swig，填入如下内容：123456789101112131415161718192021222324252627282930&#123;% if page.copyright %&#125;&lt;div class="my_post_copyright"&gt; &lt;script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"&gt;&lt;/script&gt; &lt;script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"&gt;&lt;/script&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href="&#123;&#123; url_for(page.path) &#125;&#125;"&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href="/" title="访问 &#123;&#123; theme.author &#125;&#125; 的个人博客"&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format("YYYY年MM月DD日 - HH:MM") &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format("YYYY年MM月DD日 - HH:MM") &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href="&#123;&#123; url_for(page.path) &#125;&#125;" title="&#123;&#123; page.title &#125;&#125;"&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class="copy-path" title="点击复制文章链接"&gt;&lt;i class="fa fa-clipboard" data-clipboard-text="&#123;&#123; page.permalink &#125;&#125;" aria-label="复制成功！"&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class="fa fa-creative-commons"&gt;&lt;/i&gt; &lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)"&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard('.fa-clipboard'); $(".fa-clipboard").click(function()&#123; clipboard.on('success', function()&#123; swal(&#123; title: "", text: '复制成功', icon: "success", showConfirmButton: true &#125;); &#125;); &#125;); &lt;/script&gt;&#123;% endif %&#125; 然后在themes/next/source/css/_common/components/post/下新建my-post-copyright.styl，填入如下内容：123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 接着编辑themes/next/layout/_macro/post.swig文件，按照下图位置添加如下代码：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'my-copyright.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 接着在themes/next/source/css/_common/components/post/post.styl文件最后添加如下代码：1@import "my-post-copyright" 然后，还需要在文章的头部添加copyright字段：12345678---title: Hexo+Github博客最简教程-Dockerfile自动搭建date: 2019-04-18 15:23:05top: 6tags: [Hexo, Dockerfile, Linux, Github]categories: [IDE]copyright: ture--- 最后，编辑根目录下的_config.yml文件，把url换成你的主页：123456# URL## If your site is put in a subdirectoryurl: https://icoty.github.io # 这里换成你的主页root: /permalink: :year/:month/:day/:title/permalink_defaults: 添加网易云音乐外链登陆网易云音乐网页版；点击个人头像“我的主页”；然后能够看到“我创建的歌单”，如果没有则创建一个歌单；选中一个歌单点进去，能够看到“歌曲列表”，点击“歌曲列表”右边的“生成外链播放器”；然后点击右下角的“复制代码”，粘贴到themes/next/layout/_macro/sidebar.swig文件中指定的位置即可，我的是放在侧栏中”友链”下面的。 设置文章缩略显示编辑themes/next/_config.yml，搜索auto_excerpt，设置为true：123456# Automatically Excerpt (Not recommend).# 设置文章不显示全部 点进去再显示全部# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 150 自定义代码块样式打开themes\next\source\css_custom\custom.styl，添加如下内容：123456789101112131415// Custom styles.code &#123; color: #ff7600; background: #fbf7f8; margin: 2px;&#125;// 大代码块的自定义样式.highlight, pre &#123; margin: 5px 0; padding: 5px; border-radius: 3px;&#125;.highlight, code, pre &#123; border: 1px solid #d6d6d6;&#125; 把一篇文章归为多类如下会把该文章归为Linux/IPC类。123categories: - Linux - IPC 如下会把该文章归为Linux/IPC和TCP两类。123categories: - [Linux, ICP] - TCP 参考文献https://www.jianshu.com/p/9f0e90cc32c2https://www.jianshu.com/p/bff1b1845ac9]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
        <tag>Hexo</tag>
        <tag>镜像</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于共享内存、信号、命名管道和Select模型实现聊天窗口]]></title>
    <url>%2F2019%2F04%2F18%2Fipc-chat%2F</url>
    <content type="text"><![CDATA[问题模型 A、B两个进程通过管道通信，A 进程每次接收到的数据通过共享内存传递给A1进程显示，同理，B进程每次接收到的数据通过共享内存传递给B1进程显示； 对于A、B 进程，采用ctrl+c（实际为SIGINT信号）方式退出，A、B进程通过捕捉SIGINT信号注册信号处理函数进行资源清理，A1、B1进程手动关闭即可。 特别注意 A、B通过管道通信，如果首先通过ctrl+c退出A进程，那么B进程的fifo1管道的写端会收到SIGPIPE信号而终止B进程，因此必须在B进程终止前清理掉被B占用的共享内存2，将共享内存2的引用计数减一，否则，当B1进程退出并清理共享内存2后，共享内存2的引用计数不为0，会导致共享内存2得不到释放； 为了解决前一个问题，A、B进程在启动后立即将各自的进程id通过管道发送给对方，并在各自的进程退出时向对方进程id发送SIGINT信号，触发对方进程进入信号处理接口执行资源回收工作； A和A1通过共享内存1通信，会从A进程和A1进程的虚拟地址空间分配一段连续的页映射到同一块连续的物理内存页上，这样A、A1两个进程都可以间接访问物理内存页，从而达到通信的目的，一般共享内存需要进行保护，读写不能同时进行，也不能同时进行写操作，共享内存省去了从内核缓冲区到用户缓冲区的拷贝，因此效率高。 编码与效果图 func.h:12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;strings.h&gt;#include &lt;string.h&gt;#include &lt;sys/select.h&gt;#include &lt;sys/time.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/wait.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt;#include &lt;netdb.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/select.h&gt;#include &lt;sys/time.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/uio.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt; processA.cpp：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include "func.h"int shmid;int pidB; // 存放对端进程B的进程id号char *p; // 共享内存指针// 回收共享内存资源前先杀死对端进程，否则回收失败void handle(int num)&#123; kill(pidB, SIGINT); shmdt(p); int ret; if(-1 == (ret=shmctl(shmid, IPC_RMID, NULL))) &#123; perror("shmctl"); return (void)-1; &#125; exit(0);&#125;int main(int argc, char **argv)&#123; signal(SIGINT, handle); if(-1 == (shmid=shmget(1234, 4096, IPC_CREAT|0666))) &#123; perror("shmget"); return -1; &#125; if((char*)-1 == (p=(char*)shmat(shmid, NULL, 0))) &#123; perror("shmat"); return -1; &#125; // 管道文件为单工通信方式，因此需要建立两条管道 // A进程通过管道文件fifo1的读端fdr读取B进程发送的数据 // A进程通过管道文件fifo2的写端fdw向B进程发送数据 int fdr, fdw; if(-1 == (fdr=open("fifo1", O_RDONLY)) || -1 == (fdw=open("fifo2", O_WRONLY))) &#123; perror("open fifo1 or open fifo2"); return -1; &#125; // 通信之前先通过管道互相告知对方自己的进程id char s1[10] = &#123;0&#125;; char s2[10] = &#123;0&#125;; sprintf(s1, "%d\n", getpid()); write(fdw, s1, strlen(s1) - 1); read(fdr, s2, strlen(s1) - 1); pidB = atoi(s2); printf("pipe connect success, A to A1 shmid:[%d], pidA:[%d], pidB:[%d]\n", shmid, getpid(), pidB); char buf[1024] = &#123;0&#125;; int ret; fd_set rdset; while(true) &#123; FD_ZERO(&amp;rdset); FD_SET(0, &amp;rdset); FD_SET(fdr, &amp;rdset); if((ret=select(fdr+1, &amp;rdset, NULL, NULL, NULL) &gt; 0)) &#123; // fdr可读,则接收数据之后通过共享内存传给A1 if(FD_ISSET(fdr, &amp;rdset)) &#123; bzero(buf, sizeof(buf)); if(read(fdr, buf, sizeof(buf)) &gt; 0) &#123; strncpy(p, buf, sizeof(buf)); &#125; else &#123; break; &#125; &#125; // 标准输入可读,读出来传递给B进程 if(FD_ISSET(0, &amp;rdset)) &#123; bzero(buf, sizeof(buf)); if(read(STDIN_FILENO, buf, sizeof(buf)) &gt; 0) &#123; write(fdw, buf, strlen(buf) - 1); &#125; else &#123; break; &#125; &#125; &#125; &#125; close(fdr); close(fdw); return 0;&#125; processB.cpp：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include "func.h"int shmid;int pidA; // 存放对端进程idchar *p; // 共享内存指针// 回收共享内存资源前先杀死对端进程，否则回收失败void handle(int num)&#123; kill(pidA, SIGINT); shmdt(p); int ret; if(-1 == (ret=shmctl(shmid, IPC_RMID, NULL))) &#123; perror("shmctl"); return (void)-1; &#125; exit(0);&#125;int main(int argc, char **argv)&#123; signal(SIGINT, handle); if(-1 == (shmid=shmget(1235, 4096, IPC_CREAT|0666))) &#123; perror("shmget"); return -1; &#125; if((char*)-1 == (p=(char*)shmat(shmid, NULL, 0))) &#123; perror("shmat"); return -1; &#125; // 管道文件为单工通信方式 // B进程通过管道文件fifo1的写端fdw向A进程发送数据 // B进程通过管道文件fifo2的读端fdr接收A进程的数据 int fdr, fdw; if(-1 == (fdw=open("fifo1", O_WRONLY)) || -1 == (fdr=open("fifo2", O_RDONLY))) &#123; perror("open fifo1 or open fifo2"); return -1; &#125; // 通信之前先通过管道互相告知对方自己的进程id char s1[10] = &#123;0&#125;; char s2[10] = &#123;0&#125;; sprintf(s1, "%d\n", getpid()); write(fdw, s1, strlen(s1) - 1); read(fdr, s2, strlen(s1) - 1); pidA = atoi(s2); printf("pipe connect success, B to B1 shmid:[%d], pidA:[%d], pidB:[%d]\n", shmid, pidA, getpid()); char buf[1024] = &#123;0&#125;; int ret; fd_set rdset; while(true) &#123; FD_ZERO(&amp;rdset); FD_SET(0, &amp;rdset); FD_SET(fdr, &amp;rdset); if((ret=select(fdr+1, &amp;rdset, NULL, NULL, NULL) &gt; 0)) &#123; // fdr可读,则接收数据之后通过共享内存传给B1 if(FD_ISSET(fdr, &amp;rdset)) &#123; bzero(buf, sizeof(buf)); if(read(fdr, buf, sizeof(buf)) &gt; 0) &#123; strncpy(p, buf, sizeof(buf)); &#125; else &#123; break; &#125; &#125; // 标注输入可读,读出来传递给A进程 if(FD_ISSET(0, &amp;rdset)) &#123; bzero(buf, sizeof(buf)); if(read(STDIN_FILENO, buf, sizeof(buf)) &gt; 0) &#123; write(fdw, buf, strlen(buf) - 1); &#125; else &#123; break; &#125; &#125; &#125; &#125; close(fdr); close(fdw); return 0;&#125; processA1.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142#include "func.h"int main(void)&#123; char buf[1024] = &#123;0&#125;; int shmid; if(-1 == (shmid=shmget(1234, 4096, IPC_CREAT|0666))) &#123; perror("shmget"); return -1; &#125; char *p; if((char*)-1 == (p=(char*)shmat(shmid, NULL, 0))) &#123; perror("shmat"); return -1; &#125; while(true) &#123; if(!(strcmp(buf, p))) &#123; continue; &#125; else &#123; // 共享内存有数据可读 bzero(buf, sizeof(buf)); strcpy(buf, p); printf("I am A1, recv from A:[%s]\n", buf); &#125; &#125; if(-1 ==(shmctl(shmid, IPC_RMID, 0))) &#123; perror("shmctl"); return -1; &#125; return 0;&#125; processB1.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142#include "func.h"int main(void)&#123; char buf[1024] = &#123;0&#125;; int shmid; if(-1 == (shmid=shmget(1235, 4096, IPC_CREAT|0666))) &#123; perror("shmget"); return -1; &#125; char *p; if((char*)-1 == (p=(char*)shmat(shmid, NULL, 0))) &#123; perror("shmat"); return -1; &#125; while(true) &#123; if(!(strcmp(buf, p))) &#123; continue; &#125; else &#123; // 共享内存有数据可读 bzero(buf, sizeof(buf)); strcpy(buf, p); printf("I am B1, recv from B:[%s]\n", buf); &#125; &#125; if(-1 ==(shmctl(shmid, IPC_RMID, 0))) &#123; perror("shmctl"); return -1; &#125; return 0;&#125; 回收资源 这里首先通过ctrl+c退出A进程，然后B进程收到SIGPIPE信号退出，A、B进程同时调用各自的信号处理函数回收资源，通过ipcs命令发现拥有者为root的共享内存资源的nattch都为1，分别被A1和B1占有。 然后手动关闭A1、B1进程，再次执行ipcs命令，发现拥有者为root的共享内存资源不存在，已经释放成功。1$ ipcs # 查看共性内存资源数量 源码获取本文所有源码链接]]></content>
      <categories>
        <category>IO多路复用模型</category>
        <category>同步机制</category>
        <category>IPC</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>共享内存</tag>
        <tag>命名管道</tag>
        <tag>信号</tag>
        <tag>Select</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下Docker快速部署LAMP]]></title>
    <url>%2F2019%2F04%2F16%2Fdocker-lamp%2F</url>
    <content type="text"><![CDATA[若你的mac或Linux环境上未安装Docker，请移步Docker安装，确认安装成功之后再进行下文内容。如果你不了解Docker如何操作，但是你又想彻底弄懂Docker命令，可以看我另一篇文章Docker最简教程。 拿来即用获取LAMPLAMP镜像我已经搭建好并且我已经测试过了，没有问题。你只需要直接拿去用，执行如下命令：123$docker pull icoty1/lamp:v1.1.0$docker images # 能够看到icoty1/lamp:v1.1.0已经被拉到你本地$docker run -d -ti -p 80:80 -p 3306:3306 -v /Users/yangyu/app/:/var/www/html/ icoty1/lamp:v1.1.0 /bin/bash start.sh # 运行一个容器，目录/Users/yangyu/app/是你本机PHP应用位置 /Users/yangyu/app/下存放的是public、thinkphp、vendor、runtime等内容。然后访问http://localhost 能够看到PHP应用目录下的内容，如下图，说明已经成功。 然后访问http://localhost/public/index.php ，这个是PHP的入口。如果浏览器打开提示权限不够，不要慌，检查下你无法访问的那个目录下是否存在.htaccess文件，如果有则删除就好了，如果没有则执行如下命令。 123$docker exec -it [CONTAINER ID] /bin/bash # 进入前面启动的容器中$chmod -R 0777 /var/www/html/ # 赋予最高权限$sh start.sh # start.sh在根目录下，是重启服务用的 访问phpadmin：http://localhost/phpmyadmin/index.php ，登陆的用户名和密码均为phpmyadmin，登陆后你能够在浏览器上一目了然的对所有数据表进行操作。 容器内根目录下有个start.sh文件，每次需要重启apache服务和mysql服务时只需要执行这个脚本就好了，命令如下： 1$sh start.sh LAMP版本Ubuntu 18.04.2，PHP 7.2.15，mysql 5.7.25，同时也安装了phpmyadmin。下面是查看版本的命令。mysql数据库的root账户密码是root，phpmyadmin账户密码是phpmyadmin ，你可以把密码修改成你的，mysql修改用户密码。 12345678910111213root@4f5e11ebccac:/# cat /etc/issueUbuntu 18.04.2 LTS \n \lroot@4f5e11ebccac:/# php -vPHP 7.2.15-0ubuntu0.18.04.2 (cli) (built: Mar 22 2019 17:05:14) ( NTS )Copyright (c) 1997-2018 The PHP GroupZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies with Zend OPcache v7.2.15-0ubuntu0.18.04.2, Copyright (c) 1999-2018, by Zend Technologiesroot@4f5e11ebccac:/# mysql -u root -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 9Server version: 5.7.25-0ubuntu0.18.04.2 (Ubuntu) 到这里你的目的就已经达到了，一个完整LAMP服务已经在你本机上跑起来并且能用了。下面的内容是我制作icoty1/lamp:v1.1.0的过程，如果你有兴趣，或者想知道我是怎么制作出来的，欢迎继续围观。 icoty1/lamp:v1.1.0制作过程获取ubuntu基础镜像1$ docker pull i icoty1/ubuntu:18.04.2-LTS # 从icoty1仓库拉取基础镜像并运行一个容器 安装依赖进入前面运行的容器中安装接下来的内容。 mysql12345$apt-get update$apt-get upgrade -y $apt-get dist-upgrade -y$apt-get install vim -y$apt-get install mysql-server mysql-client -y apache/php12345$apt-get install apache2 -y$vi /etc/apache2/apache2.conf # 添加 ServerName localhost:80$apt-get install php7.2 -y # 这个过程中需要选择国家和时区，如图。$apt-get install libapache2-mod-php7.2$apt-get install php7.2-mysql -y phpmyadmin123$apt-get install php-mbstring php7.0-mbstring php-gettext$service apache2 restart$apt-get install phpmyadmin # 这个过程中会自动创建mysql用户名phpmyadmin，需要手动输入密码，如图。 使apache解析php文件 vi /etc/apache2/apache2.conf，添加如下内容，让apache服务知道libphp7.2.so库在哪里，找不到这个动态库就无法解析php文件。1234# add by yangyu, current dictory is '/etc/apache2/', so '../../usr/lib/apache2/modules/libphp7.2.so' = '/usr/lib/apache2/modules/libphp7.2.so'LoadModule php7_module ../../usr/lib/apache2/modules/libphp7.2.soAddType application/x-httpd-php .phpDirectoryIndex index.php index.htm index.html 到此，这个容器内已经搭建好了LAMP服务，使用docker commit命令把这个容器提交为镜像icoty1/lamp:v1.1.0，然后push到我的docker hub仓库上，你所pull的正是我push上去的。 参考文献https://www.cnblogs.com/impy/p/8040684.html # lamphttps://linux.cn/article-7463-1.html # lamphttps://blog.csdn.net/longgeaisisi/article/details/78448525 # lamphttps://www.cnblogs.com/mmx8861/p/9062363.html # mysql密码修改]]></content>
      <categories>
        <category>Docker</category>
        <category>LAMP</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>LAMP</tag>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
</search>
